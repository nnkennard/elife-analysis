
<HTML>
<head>
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
</head>
<body>
<div class="container">

  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> The paper presents an analysis of differential privacy in machine learning, with a focus on neural networks trained via differentially private stochastic gradient descent (DPSGD).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> The main focus and the message in the paper is that the handcrafted features work better compared to learned features during training of NNs and having more training data results in better outcomes (i.e. a better privacy-utility trade-off).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> Starting with the latter, this is apparent from the noise formulation in DPSGD, where the noise is reduced via sampling probability, which decreases as the data size grows.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> Hence, I do not consider this as a new insight or a contribution.</td> 
  <td> some_polarity </td> 
  <td> I, this, a new insight or a contribution </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> Unless, I have misunderstood something, in which case, please do explain.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> For the former, as the final model used (Table 3 and Figure 1) is a linear classifier, it outperforming an end-to-end CNN based model is intuitive, as it has far fewer number of parameters (which improve the noise scale, due to smaller gradient norm).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> This is slightly touched upon in subsection "Training CNNs on handcrafted features", where the comparison is made using CNNs on the handcrafted features, however there are no detailed results presented in the paper, I would have liked to see a similar table and figures as earlier.</td> 
  <td> some_polarity </td> 
  <td> This, there, I, no detailed results presented in the paper, subsection " Training CNNs on handcrafted features " , where the comparison is made using CNNs on the handcrafted features, a similar table and figures </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> The presentation of results (Table 3) is a bit strange.</td> 
  <td> some_polarity </td> 
  <td> The presentation of results ( Table 3 ), a bit strange </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> I would have further liked to see the comparison of both models on the *same* set of hyperparameters.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> Also, instead of stating that hyperparameter search's privacy budget was not accounted for as in prior works, it would have been nice to see some analysis, such as the section D(Appendix) in Abadi et al.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> The paper shows that linear model on top of ScatterNet can outperform CNN for DPSGD training on a few generic image classification tasks.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> It analyzed the results, provides hypotheses to explain it, and concludes that more data / better feature is needed for DPSGD training.</td> 
  <td> some_polarity </td> 
  <td> It, the results, hypotheses, more data / better feature, it, DPSGD training </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> People have been searching for good models for DPSGD training, so it is nice to see a new baseline (ScatterNet + linear model) that is simple but performs better.</td> 
  <td> some_polarity </td> 
  <td> People, it, good models for DPSGD training, a new baseline ( ScatterNet + linear model ) that is simple </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> This can be pretty valuable for researchers and practitioners in the field.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> The paper also did some quite interesting experiments to explain the advantage of ScatterNet + linear model and to suggest directions to improve DPSGD training.</td> 
  <td> some_polarity </td> 
  <td> The paper, some quite interesting experiments to explain the advantage of ScatterNet + linear model and to suggest directions to improve DPSGD training </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> The experiments and discussions on the learning rate are quite interesting and inspiring to me.</td> 
  <td> some_polarity </td> 
  <td> The experiments and discussions on the learning rate, me </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> However, I feel like the results for having more data / transfer learning is not so surprising, though the experiments with models different from previous work are valuable.</td> 
  <td> some_polarity </td> 
  <td> I, the results for having more data / transfer learning, the experiments with models different from previous work </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> More detailed comments:
- In Sec 4 "smaller models are not easier to train privately", you mentioned that the CNN is smaller than the linear model, so dimensionality is not an explanation for ScatterNet + linear's better performance.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> But I guess convex model (or maybe shallow model) might have some fundamental difference from nonconvex (or maybe deeper model).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> Maybe you could try something deeper than linear but shallower than the CNN to see if there is a sweet spot in between.</td> 
  <td> some_polarity </td> 
  <td> you, something deeper than linear but shallower than the CNN, there, a sweet spot in between </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> - In Sec 4 "Models with handcrafted features converge faster without privacy", I guess the results can be explained by the fact that simpler model (linear) has a lower capacity than more complicated model (CNN) so requires less training time even with lower learning rate.</td> 
  <td> some_polarity </td> 
  <td> I, Sec 4, Models with handcrafted features, the results, privacy, the fact that simpler model ( linear ) has a lower capacity than more complicated model ( CNN ) so requires less training time even with lower learning rate </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> So maybe again it would worth trying something in between linear and CNN.</td> 
  <td> some_polarity </td> 
  <td> it, something, linear and CNN </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> - In Sec 5.2, you showed results that are much better than previous results with transfer learning.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> It seems like the main difference is the model architecture.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> Is that the case?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> Do you have any comment on that?</td> 
  <td> some_polarity </td> 
  <td> you, any comment on that </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> The presentation is clear in general.</td> 
  <td> some_polarity </td> 
  <td> The presentation </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> I would love to see more details about ScatterNet as that is the important component of the proposed method.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> This article is about a topical issue: performance degradation of of deep learning models trained with differential privacy (DP).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> Clipping of the gradients and addition of the noise, required to obtain DP guarantees, blur the models such that for moderate privacy guarantees (eps~7.0) CIFAR-10 test accuracy baseline is currently ~66% (Papernot et al., 2020b).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> Lower bounds for this degradation have been shown theoretically (e.g. Bassily et al., 2014), and there has recently been also work on circumventing this issue, see e.g.</td> 
  <td> some_polarity </td> 
  <td> Lower bounds for this degradation, there, work on circumventing this issue, e.g. Bassily et al. , 2014, e.g </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> Kairouz, P., Ribero, M., Rush, K. and Thakurta, A., 2020.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> Dimension Independence in Unconstrained Private ERM via Adaptive Preconditioning.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> arXiv preprint arXiv:2008.06570,
Yingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> Bypassing the ambient dimension: Private sgd with gradient subspace identification.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> arXiv preprint arXiv:2007.03813, 2020
(these references are not included in the paper).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> Although this paper does not introduce fundamentally anything new for DP learning (DP-SGD + Rényi DP accountant for obtaining eps,delta-guarantees are used), it does clearly beat the state-of-the-art for small epsilon values (eps up to 3.0) for MNIST, Fashion-MNIST and CIFAR-10.</td> 
  <td> some_polarity </td> 
  <td> it, this paper, the state - of - the - art, small epsilon values ( eps up to 3.0 ) for MNIST , Fashion - MNIST and CIFAR - 10, fundamentally anything new, DP learning ( DP - SGD + Rényi DP accountant for obtaining eps , delta- guarantees are used ) </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> This is obtained by using so called Scattering Networks (Oyallon and Mallat, 2015), which have the property of converging very fast without privacy.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> This phenomenon is transferred to DP learning and thus high accuracies for shorter DP-SGD runs (i.e. smaller epsilons) are obtained.</td> 
  <td> some_polarity </td> 
  <td> This phenomenon, DP learning and thus high accuracies for shorter DP - SGD runs ( i.e. smaller epsilons ) </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> As expected, these 'handcrafted data-independent feature extractors' of Scatter Networks cannot beat CNN+DP-SGD when more private data is available, or when features can be extracted from public image data.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> All in all, although I think the gist of the paper is simply combining these handcrafted feature extractors (ScatterNets) and DP-SGD, it does improve the baseline for DP CIFAR-10 for small / moderate eps-values (up to 3.0) and does provide new ideas / questions on how to improve DP learning (e.g. by accelerated convergence) also outside of image domain (handcrafted feature extraction for non-vision tasks).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> The paper is very well written.</td> 
  <td> some_polarity </td> 
  <td> The paper </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> A tiny remark:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> You write "Gaussian noise of variance sigma^2 C^2 is added to the mean gradient."</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> Notice that sigma^2 C^2 - noise is added to the summed gradients, and sigma^2 C^2 / B^2 to the mean.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> The paper considers ways of improving private versions of SGD in the context of image classification.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> The main finding is that providing "hand crafted" features can significantly improve the privacy/accuracy trade-off.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> In some cases, even a linear model built on top of such features (like those produced by ScatterNet), can improve over differentially private SGD.</td> 
  <td> some_polarity </td> 
  <td> even a linear model built on top of such features ( like those produced by ScatterNet ), some cases, differentially private SGD </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> A plausible explanation for this phenomenon is that extra features can reduce the number of iterations required in SGD, resulting in better privacy and/or less noise.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> (It is also argued that having much more data similarly improves the trade-off, but this is unsurprising and, it seems, has been observed before by McMahan et al.)</td> 
  <td> some_polarity </td> 
  <td> It, this, it, McMahan et al, much more data, the trade- off </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> The paper is quite well-written, and I found it easy to follow even though this is not my area of expertise.</td> 
  <td> some_polarity </td> 
  <td> The paper, I, it, this, my area of expertise </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> I also like that it presents a number of possible directions for further improving private SGD, including transfer learning from related, public data sets, and second-order optimization.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> A possible criticism is that in principle the "hand crafted" features may have been built based on empirical work on MNIST and CIFAR-10, and the same goes for the architecture choices, so in theory there could be some privacy leakage from these choices.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> It would have been more impressive to demonstrate effectiveness of a newer data set, not known when ScatterNet and the used CNN architectures were proposed.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> Two final comments:
- "Unlearned" usually means that you have (deliberately) forgotten something, so it is not the same as "not learned".</td> 
  <td> some_polarity </td> 
  <td> Two final comments : - " Unlearned ", it, you, the same as " not learned ", something </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> -</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> It would be interesting to consider the setting where just the image *label* is private.</td> 
  <td> some_polarity </td> 
  <td> It, the setting where just the image * label * is private </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> Has DP SGD been considered in that setting?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> Update: I really appreciate the authors' efforts to address my original concerns.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> I believe that this work is a nice application of transformers to image colorization.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> The paper is well-written and the performance of proposed transformer architecture is strong.</td> 
  <td> some_polarity </td> 
  <td> The paper, the performance of proposed transformer architecture </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> I think that this work is above the threshold of acceptance.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> **Strengths**</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> The motivation of the proposed architecture is reasonable.</td> 
  <td> some_polarity </td> 
  <td> The motivation of the proposed architecture </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> The paper is generally well-written.</td> 
  <td> some_polarity </td> 
  <td> The paper </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> **Major comments**</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> It’s better to include some discussion on regularization effects from Eq. (4).</td> 
  <td> some_polarity </td> 
  <td> It, some discussion on regularization effects from Eq. ( 4 ) </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> Eq. (4) seems to be helpful to capture the overall structure in an image, rather than capturing only local correlation from autoregressive formulation.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> For upsampling, do we really need to make use of an autoregressive model?</td> 
  <td> some_polarity </td> 
  <td> we, upsampling, use of an autoregressive model </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> A stack of transposed convolutions might be working well, because we only need to upsample input/color resolutions.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> I totally agree that autoregressive formulation does help achieve better results, but it may be possible to achieve similar performance by just using transposed convolutions.</td> 
  <td> some_polarity </td> 
  <td> I, it, autoregressive formulation, similar performance, better results, transposed convolutions </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> It’s better to include more details for reproducing results.</td> 
  <td> some_polarity </td> 
  <td> It, more details for reproducing results </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> (1) Even if different batch sizes used (224, 768 and 32), learning rates for all experiments are fixed 3e-4?</td> 
  <td> some_polarity </td> 
  <td> learning rates for all experiments, different batch sizes used ( 224 , 768 and 32 ), 3e - 4 </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> (2) How many epochs or steps are required for convergence?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> (3) Figure 6 shows that EMA is extremely important.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> How about using cosine annealing for a learning rate scheduler?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> It may help achieve more robust FID scores without EMA.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> (4) Compared to baselines, this approach is extremely slow due to the autoregressive sampling.</td> 
  <td> some_polarity </td> 
  <td> this approach, baselines, the autoregressive sampling </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> It’s better to report inference time.</td> 
  <td> some_polarity </td> 
  <td> It, inference time </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> I'm not sure that conditional layer normalization is indeed helpful.</td> 
  <td> some_polarity </td> 
  <td> I, conditional layer normalization </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> **Minor comments**</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 23 </td>
  <td> The x-axis title of figure 4c (“training steps”) seems to be wrong.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> Update:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> Thanks for the additional ablation studies.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> I would like to keep my original evaluation which is acceptance.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> --------------------</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> This paper proposes a transformer architecture for image colorization.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> It uses an axial transformer to process the low-resolution grayscale image, and uses a conditional version of the axial transformer to predict a low-resolution color image autoregressively conditioned on the gray image.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> It then uses an axial transformer to predict the final high-resolution output pixels.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> Pros:
+</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> The paper is well-written and easy to read.</td> 
  <td> some_polarity </td> 
  <td> The paper </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> The literature review is comprehensive.</td> 
  <td> some_polarity </td> 
  <td> The literature review </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> + Image colorization is an important problem in computer vision.</td> 
  <td> some_polarity </td> 
  <td> Image colorization, an important problem in computer vision </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> To my knowledge, this is the first paper that applies Transformer to colorization.</td> 
  <td> some_polarity </td> 
  <td> this, my knowledge, the first paper that applies Transformer to colorization </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> It could potentially be very impacted and inspire future work.</td> 
  <td> some_polarity </td> 
  <td> It, future work </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> + Both automatic metric (FID) and human evaluation are used to compare the method with existing approaches.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> The performance of the proposed method significantly outperforms the previous state of the art.</td> 
  <td> some_polarity </td> 
  <td> The performance of the proposed method, the previous state of the art </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> The qualitative examples are very impressive as well.</td> 
  <td> some_polarity </td> 
  <td> The qualitative examples </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> +</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> The paper performs extensive ablation studies (Figure 3) to verify the contribution of different components.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> Cons:
- The technical novelty of this paper is a bit limited.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> It basically applies existing conditioning techniques to the axial transformer and uses it for image colorization.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> - It seems that no cLN (Fig. 3 mid) is better than cLN with mean-pool only (Fig. 3 right), which is a bit counterintuitive.</td> 
  <td> some_polarity </td> 
  <td> It, no cLN ( Fig. 3 mid ), cLN, mean-pool only ( Fig. 3 right ) , which is a bit counterintuitive </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> Any possible explanation?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> Also, is there a reason to use the globally aggregated context for cLN but not for cMLP/cAtt?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 23 </td>
  <td> An ablation study on that would be helpful.</td> 
  <td> some_polarity </td> 
  <td> An ablation study on that </td> 
  </tr>  

  <tr>
  <td> 24 </td>
  <td> Besides, there is an ablation study on shift-only modulation but I am curious about how scale-only modulation performs.</td> 
  <td> some_polarity </td> 
  <td> there, I, an ablation study on shift-only modulation </td> 
  </tr>  

  <tr>
  <td> 25 </td>
  <td> -</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 26 </td>
  <td> It would be nice to show the number of parameters, training/inference speed of the proposed approach, and compare them to the baselines.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 27 </td>
  <td> - Please add references to all baseline methods compared in Table 2.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 28 </td>
  <td> I'm able to find the citation of PixColor in other parts of the paper, but cannot find most of the others'.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 29 </td>
  <td> Minor problems that do not affect my score:
- P1: determinisitic -> deterministic
-</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 30 </td>
  <td> The aggregated context is denoted as \hat{c} in Table 1 but as \bar{c} in section 4.3.
-</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 31 </td>
  <td> It would be better to use the vector format for Figure 3/4, and enlarge Figure 5 a bit.</td> 
  <td> some_polarity </td> 
  <td> It, the vector format, Figure 5, a bit, Figure 3/4 </td> 
  </tr>  

  <tr>
  <td> 32 </td>
  <td> Overall, I vote for acceptance.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 33 </td>
  <td> The novelty is not huge but I still think it would be a nice paper for ICLR and have impacts on the field given its strong empirical performance.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> Thank the authors for addressing reviewers' comments extensively.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> After rebuttal, I agree with the significance of the proposed method in terms of performance improvement in this particular task.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> However, the technical novelty is still limited.</td> 
  <td> some_polarity </td> 
  <td> the technical novelty </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> Thus, I increased my rating to 5.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> In this paper, the authors propose an autoregressive image colorization method based on self-attention.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> The proposed method first infers an initial low-resolution colorization in an autoregressive manner, then upsamples both spatial resolution and color depth.</td> 
  <td> some_polarity </td> 
  <td> The proposed method, an initial low-resolution colorization, both spatial resolution and color depth, an autoregressive manner </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> The authors adopt self-attention to encode contextual information of the scene.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> Experimental results show that each component of the proposed method is effective and the proposed method outperforms an existing autoregressive method.</td> 
  <td> some_polarity </td> 
  <td> Experimental results, each component of the proposed method, the proposed method, an existing autoregressive method </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> Overall, it is difficult to understand the contribution of this paper.</td> 
  <td> some_polarity </td> 
  <td> it, the contribution of this paper </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> I think it is because the writing in Sec. 1 and 2 is unclear.</td> 
  <td> some_polarity </td> 
  <td> I, it, the writing in Sec. 1 and 2 </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> Particularly, the writing of introduction needs a significant improvement as the authors reveal too much details of this paper instead of describing the high-level motivation of the proposed method and the technical contribution.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> The clarity also needs to be improved in the method and experiment sections.</td> 
  <td> some_polarity </td> 
  <td> The clarity, the method and experiment sections </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> (e.g. ColTran Core in Fig. 2 is confusing.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> It looks complicated, but the writing is too short.</td> 
  <td> some_polarity </td> 
  <td> It, the writing </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> what is the ground-truth in the objective?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> what about Table 2?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> each baseline is not explained and cited.)</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> Technical novelty is incremental.</td> 
  <td> some_polarity </td> 
  <td> Technical novelty </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> I could not understand the motivation of the proposed network due to the clarity issue, but this paper generally adopts existing methods such as an autoregressive model and self-attention blocks to apply them to an image colorization problem, which limits the novelty of this paper.</td> 
  <td> some_polarity </td> 
  <td> I, this paper, existing methods such as an autoregressive model and self-attention blocks to apply them to an image colorization problem , which limits the novelty of this paper, the motivation of the proposed network due to the clarity issue </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> Evaluation is weak.</td> 
  <td> some_polarity </td> 
  <td> Evaluation </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> PixColor is an old model (in 2017), so recent methods and state-of-the-art methods should be compared.</td> 
  <td> some_polarity </td> 
  <td> PixColor, an old model ( in 2017 ) , so recent methods and state - of - the - art methods should be compared </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> I could not find out what the baseline methods in Table 2 are, but they do not look like state-of-the-art models.</td> 
  <td> some_polarity </td> 
  <td> I, they, state - of - the - art models, the baseline methods in Table 2 </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> Performance gain over the previous autoregressive model using widely-used self-attention blocks is not enough for accepting this paper.</td> 
  <td> some_polarity </td> 
  <td> Performance gain over the previous autoregressive model using widely - used self-attention blocks, this paper </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> --- Update ---</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> The authors have addressed several concerns that I had regarding the work.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> While this is largely an application of a previous method, they have made some application-specific decisions in order to achieve the significant boost in performance on colorization that they saw.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> While the metrics for this task are much improved, there are still some things to be desired on the qualitative results (i.e. the diversity of results tends to be in blocks, as opposed to high within-image color variability).</td> 
  <td> some_polarity </td> 
  <td> there, some things to be desired on the qualitative results ( i.e. the diversity of results tends to be in blocks , as opposed to high within-image color variability ), the metrics for this task </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> Nevertheless, I think the improvement from this approach my guide future work in this area.</td> 
  <td> some_polarity </td> 
  <td> I, the improvement from this approach my guide future work in this area </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> Given the author's responses and changes made, I have amended my recommendation accordingly.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> __1.  Summary__</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> The authors propose a method for image colorization based on self-attention largely following the architecture of the Axial Transformer (Ho et al., 2019b).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> This approach outperforms several SOTA colorization models on FID and human evaluation.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> __2a. Strong Points__</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> The motivation for this work is clear.</td> 
  <td> some_polarity </td> 
  <td> The motivation for this work </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> Image colorization has many applications and while past approaches have significantly advanced in the past few years, there is certainly much left to be explored in this space.</td> 
  <td> some_polarity </td> 
  <td> Image colorization, there, many applications, much left to be explored in this space, past approaches, the past few years </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> The recap/explanation of the Axial Transformer is clear and concise.</td> 
  <td> some_polarity </td> 
  <td> The recap / explanation of the Axial Transformer </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> My concern (see below) is not with the articulation of this section, but more on the reliance of an approach that hasn’t been accepted via peer review.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> The performance of this method using both FID as well as using human evaluators is compelling.</td> 
  <td> some_polarity </td> 
  <td> The performance of this method using both FID as well as using human evaluators </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> Breaking the problem of colorization into two intermediate low resolution images is a nice approach for enabling larger models.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> One question would be how well a single model would perform if smaller images were all that was required.</td> 
  <td> some_polarity </td> 
  <td> One question, a single model, smaller images, all that was required </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> The ablation studies show how different components impact the performance.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> __2b. Weak Points__</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> All three modules of this approach are based on method of (Ho et al., 2019b), which is available on arxiv, but was rejected from ICLR 2020.</td> 
  <td> some_polarity </td> 
  <td> All three modules of this approach, method of ( Ho et al. , 2019 b ) , which is available on arxiv, ICLR 2020 </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> The current work is focused on the application of that method.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> This makes for a bit of a tricky situation.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> The description of the Axial Transformer is given in section 4, but it is only textual and refers the readers back to the pre-print for more detail.</td> 
  <td> some_polarity </td> 
  <td> The description of the Axial Transformer, it, the readers, section 4, more detail, the pre-print </td> 
  </tr>  

  <tr>
  <td> 23 </td>
  <td> Since this is the central method of the current work, at a minimum I think it requires more explanation/justification as opposed to pointing to a work that has not been accepted via peer review.</td> 
  <td> some_polarity </td> 
  <td> I, a minimum, this, the central method of the current work, it, more explanation / justification as opposed to pointing to a work that has not been accepted via peer review </td> 
  </tr>  

  <tr>
  <td> 24 </td>
  <td> While the language of the paper is fine, the overall flow of the paper is lacking a bit of narrative.</td> 
  <td> some_polarity </td> 
  <td> the overall flow of the paper, the language of the paper, a bit of narrative </td> 
  </tr>  

  <tr>
  <td> 25 </td>
  <td> Overall</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 26 </td>
  <td> I found myself having to jump around to find the definition and explanation of important things.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 27 </td>
  <td> Particularly within the description of the model, it would be good to add some language to help the sections flow- currently they feel very independent.</td> 
  <td> some_polarity </td> 
  <td> it, the description of the model, some language, the sections, they </td> 
  </tr>  

  <tr>
  <td> 28 </td>
  <td> Alternately, if maybe help if in the beginning part of the model, the different model components (fc, fs, etc.) are named there.</td> 
  <td> some_polarity </td> 
  <td> the different model components ( fc , fs , etc., the beginning part of the model </td> 
  </tr>  

  <tr>
  <td> 29 </td>
  <td> Related, the Architecture Section feels out of place after the Model description.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 30 </td>
  <td> There are references to the attention layers in the model description which are not explored until the Architecture section.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 31 </td>
  <td> Perhaps it makes sense to put the Architecture section first because it’s addressing layers/mechanisms that span all aspects of the model.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 32 </td>
  <td> Or perhaps combining the two sections?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 33 </td>
  <td> Right now it feels like there are two methods sections.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 34 </td>
  <td> Some of the text around Eqn(7) seems to be missing because the sentence structure doesn't make sense.</td> 
  <td> some_polarity </td> 
  <td> Some of the text around Eqn ( 7 ), the sentence structure, sense </td> 
  </tr>  

  <tr>
  <td> 35 </td>
  <td> It’s not clear what some of the labels in Figure 3 mean.</td> 
  <td> some_polarity </td> 
  <td> It, some of the labels in Figure 3 </td> 
  </tr>  

  <tr>
  <td> 36 </td>
  <td> You have to go into the text to find out what MLP 4x means, for example, and then when you find it in section 5.2, you have to go back to section 4.3 to actually understand what it means.</td> 
  <td> some_polarity </td> 
  <td> You, you, example, you, it, section 5.2, the text, section 4.3, MLP 4x, it </td> 
  </tr>  

  <tr>
  <td> 37 </td>
  <td> The ablation studies feel like they’re done in relative isolation.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 38 </td>
  <td> It would be useful to know, for example, how the lower performance of using the standard Axial Transformer vs.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 39 </td>
  <td> the conditional Axial transformer impacts the final results, not just that portion.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 40 </td>
  <td> The section “Conditioning Details” in 5.2 just feels like a results dump.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 41 </td>
  <td> It’s unclear what motivates those particular ablation choices and what those results tell the reader more generally about this approach.</td> 
  <td> some_polarity </td> 
  <td> It, those results, those particular ablation choices, the reader, this approach </td> 
  </tr>  

  <tr>
  <td> 42 </td>
  <td> Some kind of context or discussion would be useful.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 43 </td>
  <td> In general, this section feels like it’s being included just to show that ablation studies were performed without providing any greater understanding as to the approach (to potentially motivate future work or other examples, for instance).</td> 
  <td> some_polarity </td> 
  <td> this section, it, ablation studies, any greater understanding as to the approach ( to potentially motivate future work or other examples , for instance ) </td> 
  </tr>  

  <tr>
  <td> 44 </td>
  <td> The descriptions are also very terse.</td> 
  <td> some_polarity </td> 
  <td> The descriptions </td> 
  </tr>  

  <tr>
  <td> 45 </td>
  <td> If these experiments add meaningful insight to this approach, then they belong in the main text with additional explanation and discussion.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 46 </td>
  <td> If they are merely a justification that this approach works, then I would suggest moving most of this section to the appendix and using the space to give better explanation of the methods and results which are central to the application.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 47 </td>
  <td> Some of the models which the current method is compared to (Table 2) are not referenced to the best of my knowledge.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 48 </td>
  <td> What does "CNN" mean in this case?</td> 
  <td> some_polarity </td> 
  <td> " CNN, this case </td> 
  </tr>  

  <tr>
  <td> 49 </td>
  <td> Do all of these methods use a combined spatial and color upsampling method?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 50 </td>
  <td> If not, how were they implemented?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 51 </td>
  <td> This is actually a pretty significant issue as it limits the reproducibility of the comparative experiments.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 52 </td>
  <td> __3. Recommendation__
Reject.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 53 </td>
  <td> While the results are compelling, the work largely relies on a method which has not been accepted via peer review.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 54 </td>
  <td> That in and of itself does not warrant rejection, but I believe it contributed to some of the difficulties in explaining the approach, the motivation behind the approach, the results of the ablation studies, etc., which make the paper extremely difficult to follow, likely difficult to build upon, and potentially difficult to reproduce.</td> 
  <td> some_polarity </td> 
  <td> That in and of itself, I, rejection, it, some of the difficulties in explaining the approach , the motivation behind the approach , the results of the ablation studies , etc. , which make the paper extremely difficult to follow , likely difficult to build upon , and potentially difficult to reproduce </td> 
  </tr>  

  <tr>
  <td> 55 </td>
  <td> __4. Recommendation Explanation__</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 56 </td>
  <td> I would argue that the main goal of this paper is to show a novel application of the Axial Transformer approach of Ho et al 2019b and this is done by adapting that method to the task of Image Colorization.</td> 
  <td> some_polarity </td> 
  <td> I, the main goal of this paper, this, a novel application of the Axial Transformer approach of Ho et al 2019 b, that method, the task of Image Colorization </td> 
  </tr>  

  <tr>
  <td> 57 </td>
  <td> I would argue the focus is around applying that method, not exclusively doing better Image Colorization, because there is no discussion around how this advances our understanding of image colorization broadly.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 58 </td>
  <td> Nevertheless, that (showing the usefulness of an approach to a new task) is a valid objective, but because</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 59 </td>
  <td> Ho et al 2019b has not been formally accepted, it also somewhat then requires this work to explain and justify approaches of that work.</td> 
  <td> some_polarity </td> 
  <td> Ho, it, al 2019b, this work, approaches of that work </td> 
  </tr>  

  <tr>
  <td> 60 </td>
  <td> I believe that challenge has a lot to do with some of the difficulties in the paper around the methods and experiment explanation.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 61 </td>
  <td> While the (within sentence) language is clear, the overall flow of the paper is  difficult to follow.</td> 
  <td> some_polarity </td> 
  <td> the overall flow of the paper, the ( within sentence ) language </td> 
  </tr>  

  <tr>
  <td> 62 </td>
  <td> It feels like the authors were strongly up-against the page limit, so important explanation and discussion was omitted or made very terse.</td> 
  <td> some_polarity </td> 
  <td> It, so important explanation and discussion, the authors, the page limit </td> 
  </tr>  

  <tr>
  <td> 63 </td>
  <td> For example, the ablation studies, while thorough, sort of feel dumped there.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 64 </td>
  <td> There's no discussion as to why those and not other experiments were run and what the results of those experiments tell us more broadly.</td> 
  <td> some_polarity </td> 
  <td> There, no discussion as to why those and not other experiments were run and what the results of those experiments tell us more broadly </td> 
  </tr>  

  <tr>
  <td> 65 </td>
  <td> Similarly, the model and architecture section seem like they should be more intertwined.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 66 </td>
  <td> As another example, some of the methods in Table 2 are not referenced anywhere and it's not clear how they were used in this context (did they start with a low res image, or high-res image).</td> 
  <td> some_polarity </td> 
  <td> some of the methods in Table 2, it, they, another example, a low res image , or high- res image ), they, this context </td> 
  </tr>  

  <tr>
  <td> 67 </td>
  <td> That calls the reproducibility of the comparison studies into question.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 68 </td>
  <td> __5.  Questions__</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 69 </td>
  <td> Overall it seems like every generated image has a red, green, and blue variant.</td> 
  <td> some_polarity </td> 
  <td> it, every generated image, a red , green , and blue variant </td> 
  </tr>  

  <tr>
  <td> 70 </td>
  <td> Were they sampled in a particular manner to guarantee this?</td> 
  <td> some_polarity </td> 
  <td> they, a particular manner, this </td> 
  </tr>  

  <tr>
  <td> 71 </td>
  <td> Obviously it is possible to draw other samples, but do they all largely fall into one of these three coarse categories?</td> 
  <td> some_polarity </td> 
  <td> it, they, all, one of these three coarse categories, other samples </td> 
  </tr>  

  <tr>
  <td> 72 </td>
  <td> When the performance is poor for a given sample, it usually because entire swaths of the image are being painted in with a very non-natural color (like someone’s face being green, or the entire picture having a blue-ish exposure).</td> 
  <td> some_polarity </td> 
  <td> it, the performance, entire swaths of the image, a given sample, a very non-natural color, someone ’s face being green , or the entire picture having a blue-ish exposure ) </td> 
  </tr>  

  <tr>
  <td> 73 </td>
  <td> Can you speak to this and other common “mistakes” that are observed?</td> 
  <td> some_polarity </td> 
  <td> you, this and other common “ mistakes ” that are observed </td> 
  </tr>  

  <tr>
  <td> 74 </td>
  <td> How do these compare with some of the other methods you compared yours against?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 75 </td>
  <td> Are there simply fewer “mistakes” (i.e. non-natural images), or are the types of imperfections created by this approach different that would warrant different use-cases?</td> 
  <td> some_polarity </td> 
  <td> there, simply fewer “ mistakes ” ( i.e. non-natural images ), the types of imperfections created by this approach different that would warrant different use-cases </td> 
  </tr>  

  <tr>
  <td> 76 </td>
  <td> It seems like a lot of compute (16 TPUv2) was used and the batch size was relatively large.</td> 
  <td> some_polarity </td> 
  <td> It, a lot of compute ( 16 TPUv2 ), the batch size </td> 
  </tr>  

  <tr>
  <td> 77 </td>
  <td> Is the large batch size necessary for obtaining these results, or could a smaller amount of compute and smaller batch size be used?</td> 
  <td> some_polarity </td> 
  <td> the large batch size necessary for obtaining these results , or could a smaller amount of compute and smaller batch size be used </td> 
  </tr>  

  <tr>
  <td> 78 </td>
  <td> Why does training baselines with 2x and 4x wider MLP dimensions make “a fair comparison”?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 79 </td>
  <td> Is “Baseline” in Figure 3, x1 (standard) MLP but no conditioning?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 80 </td>
  <td> Why would x1 be better than x4, but worse than x2?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 81 </td>
  <td> The caption of Figure 2 feels a bit imbalanced.</td> 
  <td> some_polarity </td> 
  <td> The caption of Figure 2, a bit </td> 
  </tr>  

  <tr>
  <td> 82 </td>
  <td> ColTran core is called out specifically, but then the ColTran Upsamplers are not referenced.</td> 
  <td> some_polarity </td> 
  <td> ColTran core, the ColTran Upsamplers </td> 
  </tr>  

  <tr>
  <td> 83 </td>
  <td> Is the “Axial Transformer” just the right branch of the ColTran Core (which the figure seems to suggest) or the entire ColTran core, as the caption seems to suggest.</td> 
  <td> some_polarity </td> 
  <td> the “ Axial Transformer, just the right branch of the ColTran Core ( which the figure seems to suggest ) or the entire ColTran core, the caption </td> 
  </tr>  

  <tr>
  <td> 84 </td>
  <td> On pg. 3 “ColTran Core” it is stated that “we also train a parallel prediction head which we found beneficial for regularization”.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 85 </td>
  <td> I think it would be useful to given additional explanation here as it’s a fairly significant architectural choice.</td> 
  <td> some_polarity </td> 
  <td> I, it, additional explanation, it, a fairly significant architectural choice </td> 
  </tr>  

  <tr>
  <td> 86 </td>
  <td> If results of not including this head exist, perhaps it would be useful to show this in the appendix.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 87 </td>
  <td> Otherwise a brief explanation as to why this additional head aids the regularization would be useful.</td> 
  <td> some_polarity </td> 
  <td> a brief explanation as to why this additional head aids the regularization would be useful </td> 
  </tr>  

  <tr>
  <td> 88 </td>
  <td> Since this is an instantiation of the Axial Transformer, is this prediction head added to that approach for this particular task, or is this already a part of the standard Axial Transformer (and therefore maintained here for consistency)?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 89 </td>
  <td> Ah, this is explored further in section 5.3….</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 90 </td>
  <td> It would be helpful to the reader to reference this section when you introduce the prediction head (i.e. that the impact will be explored in section 5.3).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 91 </td>
  <td> In 4.2 it says they “adapt the Axial Transformer model for colorization”.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 92 </td>
  <td> Can you elaborate on the adaptation?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 93 </td>
  <td> It’s not clear (without looking up that reference), what belongs to the original approach vs.</td> 
  <td> some_polarity </td> 
  <td> It, that reference, the original approach </td> 
  </tr>  

  <tr>
  <td> 94 </td>
  <td> what was added/changed here for this specific task.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 95 </td>
  <td> It feels odd to mention the number of axial attention blocks in the training section as opposed to the model or architecture.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 96 </td>
  <td> This is a fundamental architectural choice, is it not?</td> 
  <td> some_polarity </td> 
  <td> This, a fundamental architectural choice, it </td> 
  </tr>  

  <tr>
  <td> 97 </td>
  <td> Why are the set of models compared via FID and Human Evaluation different?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 98 </td>
  <td> __6.  Feedback__</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 99 </td>
  <td> The demonstrated colorization scores and output are compelling, however, I believe the structure of text is very detrimental.</td> 
  <td> some_polarity </td> 
  <td> The demonstrated colorization scores and output, I, the structure of text </td> 
  </tr>  

  <tr>
  <td> 100 </td>
  <td> I think it would potentially be feasible to fully rework the text to make it more readable and reproducible and therefore a solid publication because the result is compelling, but as it stands, there is substantial rewritting which would need to be done in my opinion.</td> 
  <td> some_polarity </td> 
  <td> I, there, substantial rewritting which would need to be done in my opinion, it, it, the result, the text, it, a solid publication </td> 
  </tr>  

  <tr>
  <td> 101 </td>
  <td> In “Model: ColTran Core” fc is described as a conditional, auto-regressive axial transformer.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 102 </td>
  <td> While the definition of pc and pc~ are stated thereafter, there is not any further description as to what this means and/or a citation.</td> 
  <td> some_polarity </td> 
  <td> there, any further description as to what this means and / or a citation, the definition of pc and pc~ </td> 
  </tr>  

  <tr>
  <td> 103 </td>
  <td> The Ho et al. citation is provided in the Figure 2 caption.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 104 </td>
  <td> At a minimum that citation should be given here as well, but it would be good to give a textual description as to what an “a conditional auto-regressive axial transformer” is since it is not a commonly used architecture.</td> 
  <td> some_polarity </td> 
  <td> that citation, it, a minimum, a textual description, an “, a conditional auto- regressive axial transformer ”, it, a commonly used architecture </td> 
  </tr>  

  <tr>
  <td> 105 </td>
  <td> The second paragraph under ColTran Upsamplers (In our experiments…) is slightly confusing.</td> 
  <td> some_polarity </td> 
  <td> The second paragraph under ColTran Upsamplers ( In our experiments … ) </td> 
  </tr>  

  <tr>
  <td> 106 </td>
  <td> It seems to suggest that parallel upsampling is sufficient and advantageous for a number of reasons, but that prediction is chosen to reduce color inconsistencies.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 107 </td>
  <td> Then it seems to go back to again say that Parallel upsampling has a huge advantage of being fast.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 108 </td>
  <td> This is perhaps also confusing because there is a “Sample” label in Figure 2.</td> 
  <td> some_polarity </td> 
  <td> This, there, a “ Sample ” label, Figure 2 </td> 
  </tr>  

  <tr>
  <td> 109 </td>
  <td> The confusion is less about the validity of the approach and more that the language (in conjunction with the figure) is difficult to follow for someone not already familiar with Guadarrama 2017.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 110 </td>
  <td> While not necessary, it would be interesting to see how this approach performs on out of domain images (i.e. not from ImageNet).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 111 </td>
  <td> In 5.5, it’s stayed you follow the protocol used in PixColor.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 112 </td>
  <td> It would probably be best to additionally include the citation here, or the citation in place of “PixColor” even though that work is cited near the beginning of the paper (when the reader comes to this section, they may be unfamiliar with this approach and would like to go directly to that reference as opposed to having the search for “PixColor” and then go find the reference).</td> 
  <td> some_polarity </td> 
  <td> It, the citation in place of “ PixColor ” even though that work is cited near the beginning of the paper ( when the reader comes to this section , they may be unfamiliar with this approach and would like to go directly to that reference as opposed to having the search for “ PixColor ” and then go find the reference ), the citation </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> Summary
1. This paper finds that matrix decomposition (MD) performs well as well as the self-attention.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> 2. According to the paper, MD approximate the given matrix with low-rank, and it might be helpful inductive bias
3. MD can be implemented with vanilla matrix factorization or non-negative matrix factorization.</td> 
  <td> some_polarity </td> 
  <td> 2 ., MD, it, the paper, the given matrix, low-rank, MD, vanilla matrix factorization or non-negative matrix factorization </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> 4. For stable learning, this paper proposes an additional technique, one-step gradient instead of back-propagation through time (BPTT)
5.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> To validate the performance, they experiment on semantic segmentation and image generation.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> Please reply to the below questions.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> 1. What is the advantage of MD over attention?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> According to the paper, the main advantage of MD is a less computational burden, but there are several works for linear time attention.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> Furthermore, it is hard to find sufficient reasons that MD performs better than attention-based models.</td> 
  <td> some_polarity </td> 
  <td> it, sufficient reasons that MD performs better than attention - based models </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> 2. There is no description for Table 1.</td> 
  <td> some_polarity </td> 
  <td> 2 ., There, no description for Table 1 </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> Could you explain the table 1?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> 3. Are there results for MD with BPTT instead of a one-step gradient on segmentation or image generation task?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> 4. What is the "human priors" in the conclusion section?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> Does it denote the "low-rank"?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> It is hard to understand that low-rank will be helpful.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> If MD set r as same as min(d,n) instead of small r, does MD have lower performance?</td> 
  <td> some_polarity </td> 
  <td> MD, small r, lower performance, MD, min( d , n </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> 5. There are several works about 1) analyzing the low-rank problems in multi-head attention and 2) incorporating the low-rank approximation into attention.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> The discussion between this paper and related works is not enough.</td> 
  <td> some_polarity </td> 
  <td> The discussion between this paper and related works </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> 6. The title is "Is Attention Better Than Matrix Decomposition?", but the paper is only for the "self"-attention and MD.</td> 
  <td> some_polarity </td> 
  <td> 6 ., The title, the paper, the " self " - attention and MD, Attention Better Than Matrix Decomposition </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> Are there results for Encoder-Decoder structured tasks (such as Translation)?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> I suggest that this paper discusses the relationship between MD and the below papers.</td> 
  <td> some_polarity </td> 
  <td> I, this paper, the relationship between MD and the below papers </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> a) Factorization and Attention
1.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> A Tensorized Transformer for Language Modeling</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> b) Low-rank problems and attention
1. Low-Rank Bottleneck in Multi-head Attention Models</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 23 </td>
  <td> c) Low-rank attention
1. Transformers are rnns: Fast autoregressive transformers with linear attention
2. Linformer: Self-Attention with Linear Complexity
3. Implicit Kernel Attention
4. Compact Multi-Head Self-Attention for Learning Supervised Text Representations</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 24 </td>
  <td> -----</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 25 </td>
  <td> I read a valuable author's response, and I keep my positive score.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> # Summary:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> The paper presents a method based on matrix decomposition (MD) for encoding global context in computer vision tasks.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> In particular, a "Hamburger" block is proposed encompassing matrix decomposition as its central part, between two linear projection layers.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> Direct comparison and relations are drawn between the proposed method and the widely adopted self-attention paradigm.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> The proposed method leads to improved results when Hamburger blocks are used instead of self-attention blocks, leading at the same time to reduced number of parameters, memory footprint and inference time.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> # Strengths:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> The paper presents a novel and simple method for capturing global context, demonstrated on two challenging computer vision tasks, namely semantic segmentation and image generation.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> Important advantages of the proposed method, with respect to the self-attention method which is usually employed in such tasks, are the fact that it can be easily adapted to a wide range of models and problems, it is more efficient and has reduced memory requirements.</td> 
  <td> some_polarity </td> 
  <td> Important advantages of the proposed method, it, respect to the self-attention method which is usually employed in such tasks, the fact that it can be easily adapted to a wide range of models and problems, memory requirements </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> A one-step gradient method is proposed for propagating the gradients through the MD optimization algorithm during training.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> One-step gradient is shown to overcome the unstable gradient problems of the back-propagation through time (BPTT) algorithm, leading to improved performance.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> A detailed analysis is provided comparing the two methods (one-step vs BPTT).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> The evaluation is quite comprehensive comparing the proposed hamburger block with respect to similar self-attention blocks under several aspects (accuracy/FID score, GPU load, GPU time, FLOPs, nr. of parameters).</td> 
  <td> some_polarity </td> 
  <td> The evaluation, the proposed hamburger block, respect to similar self-attention blocks, several aspects ( accuracy / FID score , GPU load , GPU time , FLOPs , nr. of parameters ) </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> A detailed ablation study is also presented showing the effect of the most important factors of the proposed contribution in the final performance.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> Regarding writing quality, the paper is clear and easy to read.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> The main ideas and contributions are clearly stated and presented.</td> 
  <td> some_polarity </td> 
  <td> The main ideas and contributions </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> Some issues regarding the structure of the sections is discussed below.</td> 
  <td> some_polarity </td> 
  <td> Some issues regarding the structure of the sections </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> # Weaknesses:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> The paper makes the more general claim that the proposed approach can be used for including any human inductive bias expressed through an optimization problem, however, only the problem of capturing global context, in place of self-attention, is explored.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> To support this more general claim it would be important to include some representative examples (even without providing a detailed evaluation on those).</td> 
  <td> some_polarity </td> 
  <td> it, this more general claim, some representative examples, a detailed evaluation on those </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> Possibly related to the previous point is the observation that non-negative matrix factorization (NMF) seems to always perform better than the other two matrix decomposition methods, Vector Quantization (VQ) and Concept Decomposition (CD).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> This leads to certain questions, as for example:
* are there any problems that lend themselves better to the other types of MD?</td> 
  <td> some_polarity </td> 
  <td> This, certain questions, example, there, any problems that lend themselves better to the other types of MD </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> * is the performance of VQ and CD degraded because they are rendered soft?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> * what is the divergence of soft with original MD as far as the original optimization problem is concerned?</td> 
  <td> some_polarity </td> 
  <td> the divergence of soft with original MD, the original optimization problem </td> 
  </tr>  

  <tr>
  <td> 23 </td>
  <td> The results of the ablation on temperature T provided in Appendix G partially show that the "softening" of the algorithm might negatively affect the accuracy.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 24 </td>
  <td> I find it also strange and possibly nearly violating formatting that the related work section is provided in the appendix.</td> 
  <td> some_polarity </td> 
  <td> I, it, formatting that the related work section is provided in the appendix </td> 
  </tr>  

  <tr>
  <td> 25 </td>
  <td> Some directly related work is discussed in the main text, yet a more detailed discussion considering a broader set of works only appears in the appendix.</td> 
  <td> some_polarity </td> 
  <td> Some directly related work, a more detailed discussion considering a broader set of works, the appendix, the main text </td> 
  </tr>  

  <tr>
  <td> 26 </td>
  <td> Also regarding related work, other works employing matrix decomposition in the context of deep learning, are not covered (e.g. (Sainath et al., 2013; Tariyal et al., 2016)).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 27 </td>
  <td> Sainath, T. N., Kingsbury, B., Sindhwani, V., Arisoy, E., & Ramabhadran, B. (2013).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 28 </td>
  <td> Low-rank matrix factorization for deep neural network training with high-dimensional output targets.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 29 </td>
  <td> In 2013 IEEE international conference on acoustics, speech and signal processing (pp. 6655-6659).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 30 </td>
  <td> Tariyal, S., Majumdar, A., Singh, R., & Vatsa, M. (2016).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 31 </td>
  <td> Deep dictionary learning.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 32 </td>
  <td> IEEE Access, 4, 10096-10109.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 33 </td>
  <td> ## Minor Comments
* Table 1, no details are provided for the metric used for the results
* Table 6, it would be better to specify the difference between the two entries of HamGAN
*</td> 
  <td> some_polarity </td> 
  <td> ## Minor Comments * Table 1, no details, it, the metric used for the results * Table 6, the difference between the two entries of HamGAN </td> 
  </tr>  

  <tr>
  <td> 34 </td>
  <td> The text in almost all figure is quite small and very hard to read on typical zoom factors (~100%).</td> 
  <td> some_polarity </td> 
  <td> The text in almost all figure, typical zoom factors ( ~100 % ) </td> 
  </tr>  

  <tr>
  <td> 35 </td>
  <td> # Rating Justification</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 36 </td>
  <td> Overall, I think that the idea of using matrix decomposition as architectural element to capture global context is quite interesting and novel.</td> 
  <td> some_polarity </td> 
  <td> I, the idea of using matrix decomposition as architectural element to capture global context </td> 
  </tr>  

  <tr>
  <td> 37 </td>
  <td> Also the method shows advantages with respect to self-attention as far as efficiency and memory requirements are concerned.</td> 
  <td> some_polarity </td> 
  <td> the method, advantages with respect to self-attention as far as efficiency and memory requirements are concerned </td> 
  </tr>  

  <tr>
  <td> 38 </td>
  <td> There are some issues regarding the generality of the proposed approach and the paper's structure, however, I think that the paper strengths exceed its weaknesses.</td> 
  <td> some_polarity </td> 
  <td> There, I, some issues regarding the generality of the proposed approach and the paper 's structure, the paper strengths, its weaknesses </td> 
  </tr>  

  <tr>
  <td> 39 </td>
  <td> # Rating and comments after the rebuttal</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 40 </td>
  <td> The authors addressed my concerns in their feedback and the revised manuscript they have provided.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 41 </td>
  <td> In particular, I find the claimed contributions much clearer now.</td> 
  <td> some_polarity </td> 
  <td> I, the claimed contributions </td> 
  </tr>  

  <tr>
  <td> 42 </td>
  <td> In my view, they have also suitably addressed the concerns raised in the other reviews.</td> 
  <td> some_polarity </td> 
  <td> they, my view, the concerns raised in the other reviews </td> 
  </tr>  

  <tr>
  <td> 43 </td>
  <td> As a result, I increase my rating to 8 as I think that this work is interesting, novel and impactful.</td> 
  <td> some_polarity </td> 
  <td> I, a result, my rating, 8, I, this work </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> This paper proposed a matrix decomposition-based method to capture spatial long range correlation in the neural network.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> The proposed method employs an optimization method, i.e. non-negative matric factorization, to reconstruct a low-rank embedding of the input data.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> The experimental results show that the proposed method outperforms various popular attention-based methods in recent years for various vision tasks, i.e. semantic segmentation and image generation.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> This paper is basically well written and easy to follow what they have done.</td> 
  <td> some_polarity </td> 
  <td> This paper, they </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> However, I have several concerns that I listed as follows.</td> 
  <td> some_polarity </td> 
  <td> I, several concerns that I listed as follows </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> - According to experimental results, I find the proposed matrix decomposition based method outperforms several attention based methods in mIoU for semantic segmentation and FIN for image generation.</td> 
  <td> some_polarity </td> 
  <td> I, experimental results, the proposed matrix decomposition based method, several attention based methods, mIoU, semantic segmentation and FIN for image generation </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> Nevertheless, the Parameters, FLOPs, memory and running time are only compared with Dual attention network.</td> 
  <td> some_polarity </td> 
  <td> the Parameters , FLOPs , memory and running time, Dual attention network </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> Please compare more attention based methods to verify the proposed method is more efficient attention based methods.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> - The proposed method is similar with EMA-Net, especially for employing concept decomposition as the optimization algorithm.</td> 
  <td> some_polarity </td> 
  <td> The proposed method, EMA - Net, concept decomposition, the optimization algorithm </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> Please discuss the relation and difference between the proposed method and EMA-Net.
- I am very interested in the initialization of matrix decomposition.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> In the supplementary, authors only discuss the initialization of D, so what is the best initialization of C for non-negative matrix factorization?</td> 
  <td> some_polarity </td> 
  <td> the supplementary , authors, the initialization of D, the best initialization of C for non-negative matrix factorization </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> Besides, what is the warm start with online update?</td> 
  <td> some_polarity </td> 
  <td> the warm start with online update </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> - The figures 3 and 4 are low quality, the text of coordinate is too small.</td> 
  <td> some_polarity </td> 
  <td> The figures 3 and 4, the text of coordinate, low quality </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> After rebuttal:
I appreciate the authors' detailed response to my questions, which largely addresses my previous concerns.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> It's very pleasure to reviewing this interesting, innovative and well-written paper.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> A clear accept.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> This paper proposes to use matrix decomposition to construct low-rank representations to find the long-distance correlations in context, which is demonstrated more effective than popular self-attention mechanism.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> Combining linear transformation and matrix decomposition (core part), authors design Hamburger block to model global dependencies from input as residual output.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> The authors propose differentiable modified Vector Quantization and Non-negative Matrix Factorization to perform matrix decomposition.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> They propose one-step gradient, an approximation of Back-Propagation Through Time (BPTT) algorithm, to back-propagate gradient of matrix decomposition.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> They conduct experiments on semantic segmentation and image generation to demonstrate the superiority of their methods regarding modelling global dependencies and computational cost.</td> 
  <td> some_polarity </td> 
  <td> They, experiments on semantic segmentation and image generation, the superiority of their methods regarding modelling global dependencies and computational cost </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> I believe that there is clear novelty in the proposed method.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> The paper is well written.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> One weakness is that the experiment analysis is a little weak.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> It will be great to see stronger experiments in the final.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> The manuscript proposes a neural network for solving partial differential equations.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> This is performed by a supervised learning of a mapping between the PDE coefficients (a) and the PDE solution (u), where a is drawn from a certain distribution.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> This work follows  a work by Li et al.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> 2020b.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> Inspired by the properties of Green’s function, in Li et al.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> the authors suggest to learn a kernel function in an iterative manner.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> In this work, the general kernel is replaced by a convolution kernel which is represented in the Fourier space.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> Pros.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> 1.	The authors address an important and practical problem 
2.	I like the idea of learning a mapping for a class of PDEs, rather than optimizing per instance
3.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> The numerical results are impressive</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> Cons.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> 1.	Novelty.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> I feel that the novelty is incremental with respect to Li et al.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> The motivation for replacing the kernel function (3) by a convolution operator is not entirely clear.</td> 
  <td> some_polarity </td> 
  <td> The motivation for replacing the kernel function ( 3 ) by a convolution operator </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> I guess that efficiency is one of the reasons.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> If so, I would suggest to discuss the complexity of the proposed scheme vs.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> Li et al. complexity.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> Moreover, how does it affect the expressivity of the network?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> 2.	Clarity of the paper.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> I didn’t like the idea that on the one hand entire sentences are taken form Li et al.</td> 
  <td> some_polarity </td> 
  <td> I, the idea that on the one hand entire sentences are taken form Li et al </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> but on the other hand, the paper itself is not self-contained.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> In practice, the reader should follow the intuition and the motivation given in Li et al in order to grasp the idea of neural operator.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> In addition, a concise detailed description of the network architecture is missing.</td> 
  <td> some_polarity </td> 
  <td> a concise detailed description of the network architecture, addition </td> 
  </tr>  

  <tr>
  <td> 23 </td>
  <td> 3.	Missing references and comparison.</td> 
  <td> some_polarity </td> 
  <td> 3 ., references and comparison </td> 
  </tr>  

  <tr>
  <td> 24 </td>
  <td> There are two works, aiming at solving PDEs, which are very relevant in the context of the proposed approach.</td> 
  <td> some_polarity </td> 
  <td> There, two works, PDEs , which are very relevant in the context of the proposed approach </td> 
  </tr>  

  <tr>
  <td> 25 </td>
  <td> The first copes with the structured case, Greenfeld at al.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 26 </td>
  <td> “Learning to optimize multigrid PDE solvers” and the second handles the unstructured case, Luz et al.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 27 </td>
  <td> “Learning algebraic multigrid using graph neural networks”.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 28 </td>
  <td> It resembles the proposed approach in the sense that the training is performed for a class of problems, yielding a mapping from the coefficients to the solution.</td> 
  <td> some_polarity </td> 
  <td> It, the proposed approach, the sense that the training is performed for a class of problems , yielding a mapping from the coefficients to the solution </td> 
  </tr>  

  <tr>
  <td> 29 </td>
  <td> In these works, the learning is unsupervised, while generalizing over boundary conditions and domains.</td> 
  <td> some_polarity </td> 
  <td> the learning, these works, boundary conditions and domains </td> 
  </tr>  

  <tr>
  <td> 30 </td>
  <td> To summarize, I believe that this work should be published.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 31 </td>
  <td> I hope that the authors are able to address my concerns.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> Paper summary:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> Building on previous work on neural operators, the paper introduces the Fourier neural operator, which uses a convolution operator defined in Fourier space in place of the usual kernel integral operator.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> Each step of the neural operator then amounts to applying a Fourier transform to a vector (or rather, a set of vectors on a mesh), performing a linear transform (learnt parameters in this model) on the transformed vector, before performing an inverse Fourier transform on the result, recombining it with a linear map of the original vector, and passing the total result through a non-linearity.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> The Fourier neural operator is by construction (like all neural operators) a map between function spaces, and invariance to discretization follows immediately from the nature of a Fourier transform (just project onto the usual basis).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> If the underlying domain has a uniform discretization, the fast Fourier transformation (FFT) can be used, allowing for an O(nlogn) evaluation of the aforementioned convolution operator, where n is the number of points in the discretization.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> Experiments demonstrate that the Fourier neural operator significantly outperforms other neural operators and other deep learning methods on Burgers’ equation, Darcy Flow, and Navier Stokes, and that that it is also significantly faster than traditional PDE solvers.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> ------------------------------------------</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> Strengths and weaknesses:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> Much of the theoretical legwork for this paper, namely, neural operators, was already carried out in previous papers (Li et al.).</td> 
  <td> some_polarity </td> 
  <td> Much of the theoretical legwork for this paper, neural operators, previous papers ( Li et al . ) </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> The remaining theoretical work, namely writing down the Fourier integral operator and analysing the discrete case, was succinctly explained.</td> 
  <td> some_polarity </td> 
  <td> The remaining theoretical work, the Fourier integral operator, the discrete case </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> The subsequent experimentation was extremely thorough (e.g. demonstrating that activation functions help in recovering high frequency modes) and, of course, the results were very impressive.</td> 
  <td> some_polarity </td> 
  <td> The subsequent experimentation, the results, activation functions, high frequency modes </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> I liked the paper a lot, and it’s definitely a big step-forward in neural operators.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> I’m assigning a score of 8 (a very good conference paper), and I think that the paper is more or less ready for publication as is.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> I’ve included a few questions below (to help my own understanding), as well as some typos I spotted whilst reading the paper.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> ------------------------------------------</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> Questions and clarification requests:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> 1)	Section 4, The Discrete Case and the FFT – could you explain the definition of bounds in the definition of Z_{k_{max}}?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> 2)	Section 4, Parametrizations of R, sentence 2 – could you explain the definition R_{\phi}?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> At present I can’t see how the function signature of R matches the definition given.</td> 
  <td> some_polarity </td> 
  <td> I, the function signature of R, the definition </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> ------------------------------------------</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> Typos and minor edits:
- Page 3, bullet point 3 – “solving Bayesian inference problem” -> “solving Bayesian inference problems”
- Section 1, final paragraph, sentence 2 - “approximate function with any boundary conditions” -> “approximate functions with any boundary conditions”
- Section 4, The discrete case and the FFT, final paragraph, last sentence - “all the task that we consider” -> “all the tasks that we consider”
- Section 4, Parametrizations of R, last sentence - “while neural networks have the worse performance” -></td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> “while neural networks have the worst performance”
- Section 4, final sentence – “Generally, we have found using FFTs to be very efficient, however a uniform discretization if required.” -></td> 
  <td> some_polarity </td> 
  <td> neural networks, the worst performance ” - Section 4 , final sentence – “ Generally , we have found using FFTs to be very efficient , however a uniform discretization if required </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> “Generally, we have found using FFTs to be very efficient.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 23 </td>
  <td> However, a uniform discretization is required.”</td> 
  <td> some_polarity </td> 
  <td> a uniform discretization </td> 
  </tr>  

  <tr>
  <td> 24 </td>
  <td> - Section 5, final paragraph, sentence 2 – “FNO takes 0.005s to evaluate a single instances while the traditional solver” -></td> 
  <td> some_polarity </td> 
  <td> Section 5 , final paragraph , sentence 2 – “ FNO, 0.005s, a single instances, the traditional solver ” -> </td> 
  </tr>  

  <tr>
  <td> 25 </td>
  <td> “FNO takes 0.005s to evaluate a single instance while the traditional solver”
- Section 6, final sentence – “Traditional Fourier methods work only with periodic boundary conditions, however, our Fourier neural operator does not have this limitation.” -></td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 26 </td>
  <td> “Traditional Fourier methods work only with periodic boundary conditions.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 27 </td>
  <td> However, our Fourier neural operator does not have this limitation.”</td> 
  <td> some_polarity </td> 
  <td> our Fourier neural operator, this limitation </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> Paper Summary:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> The authors proposed a novel neural Fourier operator that generalizes between different function discretization schemes, and achieves superior performance in terms of speed and accuracy compared to learned baselines.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> I have to admit that my understanding of this paper is rather limited and I have lots of conceptual questions about the implementation.</td> 
  <td> some_polarity </td> 
  <td> I, my understanding of this paper, I, lots of conceptual questions about the implementation </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> -</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> The authors claimed to novel contributions: one is that the method is fast, one is that the method is discretization invariant.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> These two seem to be in conflict.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> The authors leveraged FFT for performing fast convolutions differentiably.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> However FFT assumes the data to be present on a uniform grid (of 2^n grid cells in each dimension).</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> I understand that since the learned content is a frequency "filter" that can be applied to any discretization, but this can be said for just about any CNN (in that case a learned 3x3 filter in physical space can also just be projected to the spectral space and work with "any discretization").</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> Furthermore (correct me if I'm wrong), it doesn't seem that the authors experimentally illustrated a case of learning on non-discretized data, and performing inference on discretized versions of it, or vice versa.</td> 
  <td> some_polarity </td> 
  <td> it, me, I, the authors, a case of learning on non-discretized data , and performing inference on discretized versions of it , or vice versa </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> - From Fig. 1, it seems that the actual parameters that are being learned is the convolutional kernel R (represented as spectral coefficients), and W.</td> 
  <td> some_polarity </td> 
  <td> it, Fig. 1, the actual parameters that are being learned, the convolutional kernel R ( represented as spectral coefficients ) , and W </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> That basically amounts to a single convolution plus bias operation.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> The expressivity of a single such operation should be pretty limited, since this operation itself is a linear operator, while the underlying equations that the authors demonstrated on are mostly nonlinear.</td> 
  <td> some_polarity </td> 
  <td> The expressivity of a single such operation, this operation itself, a linear operator, the underlying equations that the authors demonstrated on </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> More insights into the two questions above are certainly appreciated.</td> 
  <td> some_polarity </td> 
  <td> More insights into the two questions above </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> Adding a reference that seems related to this work.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> The paper below uses a spectral solver step as a differentiable layer within the neural network for enforcing hard linear constraints in CNNs, also taking the FFT -> spectral operator -></td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> IFFT route.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> References</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> ---------------</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> Kashinath, Karthik, and Philip Marcus.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> "Enforcing Physical Constraints in CNNs through Differentiable PDE Layer."</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> 2020.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  <br/><br/>
  <table class="table">
  <thead>
  <tr>
  <th> Index </th> 
  <th> Sentence </th> 
  <th> Polarity label </th> 
  <th> Maximal NPs </th> 
  </tr>
  </thead>
  <tbody>
  
  <tr>
  <td> 0 </td>
  <td> This work attacks the important problem of learning PDEs with neural networks.</td> 
  <td> some_polarity </td> 
  <td> This work, the important problem of learning PDEs with neural networks </td> 
  </tr>  

  <tr>
  <td> 1 </td>
  <td> To this extent, it proposes a neural network architecture where (in essence), the linearity is not applied in the Fourier space but in the state space.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 2 </td>
  <td> This allows the network to implicitly learn the partial differentials of the equation, and results in a parametrization which is invariant to spatial discretization.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 3 </td>
  <td> This approach yields good results when regressing to data derived from numerical solutions of complex differential equations.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 4 </td>
  <td> Strengths:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 5 </td>
  <td> Overall, the paper is well written.</td> 
  <td> some_polarity </td> 
  <td> the paper </td> 
  </tr>  

  <tr>
  <td> 6 </td>
  <td> The theoretical and experimental sections are, for the most part, clear and concise, although some important details remain unclear/lacking.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 7 </td>
  <td> The method seems to be quite generic and can be applied to a large range of PDEs.</td> 
  <td> some_polarity </td> 
  <td> The method, a large range of PDEs </td> 
  </tr>  

  <tr>
  <td> 8 </td>
  <td> The inverse problem experiment is interesting and highlights a potentially useful application of the proposed method.</td> 
  <td> some_polarity </td> 
  <td> The inverse problem experiment, a potentially useful application of the proposed method </td> 
  </tr>  

  <tr>
  <td> 9 </td>
  <td> Weaknesses:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 10 </td>
  <td> Perhaps most importantly, the data generation is not always clear to me.</td> 
  <td> some_polarity </td> 
  <td> the data generation, me </td> 
  </tr>  

  <tr>
  <td> 11 </td>
  <td> The distinction between training data and test data is not clearly specified.</td> 
  <td> some_polarity </td> 
  <td> The distinction between training data and test data </td> 
  </tr>  

  <tr>
  <td> 12 </td>
  <td> Generalization to different useful initial conditions is obviously of utmost importance in order to properly evaluate the quality of data-driven methods for dynamical systems.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 13 </td>
  <td> The loss function and training method is not clearly specified: are observations acquired every t=k \in N steps for NS?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 14 </td>
  <td> Do you use teacher forcing or compute an error on the full sequence?</td> 
  <td> some_polarity </td> 
  <td> you, teacher forcing or compute an error on the full sequence </td> 
  </tr>  

  <tr>
  <td> 15 </td>
  <td> What about the other experiments?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 16 </td>
  <td> Comments and questions:</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 17 </td>
  <td> It seems that in the proposed model, the Fourier coefficients R do  not vary with the layers.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 18 </td>
  <td> This seems to be quite constraining, could you comment on this?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 19 </td>
  <td> It would have been interesting to analyze the expressiveness of the architecture.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 20 </td>
  <td> You mention that you are not restricted to periodic boundary conditions due to your final decoder (which is never clearly defined), however, does the finite support of the data not lead to artifacts in the Fourier transform, causing you to have to learn a high number of Fourier modes?</td> 
  <td> some_polarity </td> 
  <td> You, the finite support of the data, you, artifacts in the Fourier, you, periodic boundary conditions due to your final decoder ( which is never clearly defined ), a high number of Fourier modes </td> 
  </tr>  

  <tr>
  <td> 21 </td>
  <td> Could you comment on this?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 22 </td>
  <td> In the NS experiment, why have you used N=10 000 when viscosity is set to 1e-4, and N=1000 for the others?</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 23 </td>
  <td> It seems as if you have selected the number of samples where your method outperforms the baselines.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  

  <tr>
  <td> 24 </td>
  <td> Moreover, for the NS experiments, test data seems to be future data (T>10), however this seems to not be the case for the Burgers’ equation, even though it is also time dependent.</td> 
  <td> some_polarity </td> 
  <td> test data, the NS experiments, this, future data ( T>10 ), it, the case, time dependent, the Burgers ’ equation </td> 
  </tr>  

  <tr>
  <td> 25 </td>
  <td> As one of your motivations behind this work is to learn the operator, it could have been interesting to test your approach using different sample points as in the training data.</td> 
  <td> no_polarity </td> 
  <td>  </td> 
  </tr>  
  </tbody>
  </table>
  
</div>
</body>
</HTML>
