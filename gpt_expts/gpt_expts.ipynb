{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c690a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Labels ===\n",
      "asp_clarity                 A-clarity\n",
      "asp_meaningful-comparison   B-meaningful-comparison\n",
      "asp_motivation-impact       C-motivation-impact\n",
      "asp_originality             D-originality\n",
      "asp_replicability           E-replicability\n",
      "asp_soundness-correctness   F-soundness-correctness\n",
      "asp_substance               G-substance\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "import disapere_lib\n",
    "\n",
    "def summarize_results(results):\n",
    "  valid_answers = 0\n",
    "  correct_answers = 0\n",
    "\n",
    "  for a, b, c in results:\n",
    "    if c in label_map.values():\n",
    "      valid_answers += 1\n",
    "      if b == c:\n",
    "        correct_answers += 1\n",
    "\n",
    "  valid_percent = valid_answers/len(results)\n",
    "  accuracy = correct_answers/len(results)\n",
    "\n",
    "  print(f\"Valid answers: {valid_percent:.0%}\\nAccuracy: {accuracy:.0%}\")\n",
    "  \n",
    "  print(\"=\" * 80)\n",
    "  \n",
    "  print(\"Label\".ljust(25), \"Predicted\".ljust(29), \"Sentence\\n\")\n",
    "  for a, b, c in results[:20]:\n",
    "    print(b.ljust(25), c.ljust(25), \"|||\", a)\n",
    "    print()\n",
    "\n",
    "    \n",
    "openai.api_key_path = \"nnk_openai_api_key.txt\"\n",
    "MODEL_NAME = \"text-davinci-003\"\n",
    "\n",
    "dataset = disapere_lib.get_dataset('aspect')\n",
    "label_map = {y: f'{x}-{y[4:]}' for x, y in zip(\"ABCDEFG\", sorted(dataset['train'].keys())[1:-1])}\n",
    "\n",
    "old_label_list = sorted(label_map.keys())\n",
    "\n",
    "print(\"=== Labels ===\")\n",
    "for k, v in label_map.items():\n",
    "  print(k.ljust(28) + f'{v}')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea4899",
   "metadata": {},
   "source": [
    "# Few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8a2ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  text-davinci-003\n",
      "Number of classes: 7\n",
      "Number of examples per class in prompt: 3\n",
      "Prompt length: 3382\n",
      "\n",
      "Prompt prefix:\n",
      "================================================================================\n",
      "Sentence: Reward prediction along --> Reward prediction alone\n",
      "Label: A-clarity\n",
      "###\n",
      "Sentence: this limitation in latenby?\n",
      "Label: A-clarity\n",
      "###\n",
      "Sentence: In general, the paper is well written and easy to follow. And the experimental evaluation is extensive and compares with relevant state-of-the-art m...\n",
      "================================================================================\n",
      "\n",
      "Number of examples to label: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_examples_in_prompt = 3\n",
    "num_examples_to_label = 200\n",
    "\n",
    "\n",
    "prompt = \"\"\n",
    "\n",
    "for old_label, new_label in label_map.items():\n",
    "  for _, text in dataset['train'][old_label][:num_examples_in_prompt]:\n",
    "    prompt += f'Sentence: {text}\\nLabel: {new_label}\\n###\\n'\n",
    "    \n",
    "print(\"Model name: \", MODEL_NAME)\n",
    "print(\"Number of classes:\", len(label_map))\n",
    "print(\"Number of examples per class in prompt:\", num_examples_in_prompt)\n",
    "print(\"Prompt length:\", len(prompt))\n",
    "print()\n",
    "\n",
    "print(\"Prompt prefix:\")\n",
    "print(\"=\" * 80)\n",
    "print(prompt[:300]+\"...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "print(\"\\nNumber of examples to label:\", num_examples_to_label)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad00da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 200/200 [03:47<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Weird way of picking examples but whatever\n",
    "for i in tqdm.tqdm(range(num_examples_in_prompt, num_examples_in_prompt + num_examples_to_label)):\n",
    "  label = random.choice(old_label_list)\n",
    "  if i >= len(dataset['train'][label]):\n",
    "    continue\n",
    "    \n",
    "  _, sentence = dataset['train'][label][i]\n",
    "  text = f'{prompt}Sentence: {sentence}\\nLabel: '\n",
    "  \n",
    "  response = openai.Completion.create(\n",
    "    engine = MODEL_NAME,\n",
    "    prompt = text,\n",
    "    temperature = 0.6,\n",
    "    max_tokens = 150,\n",
    "  )\n",
    "  \n",
    "  results.append((sentence, label_map[label], response['choices'][0].text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a9d883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid answers: 28%\n",
      "Accuracy: 17%\n",
      "================================================================================\n",
      "Label                     Predicted                     Sentence\n",
      "\n",
      "B-meaningful-comparison   B-meaningful-comparison   ||| But there are better baselines possible.\n",
      "\n",
      "E-replicability           E-replicability           ||| The authors should clearly explain how to update \\phi when optimizing Eq 12.\n",
      "\n",
      "C-motivation-impact       A-clarity                 ||| This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms.\n",
      "\n",
      "C-motivation-impact       A-clarity                 ||| The paper clearly states the objective and provides a nice general description of the method.\n",
      "\n",
      "A-clarity                 H-presentation\n",
      "###\n",
      "Sentence: The figures are clear and easy to understand.\n",
      "Label: H-presentation ||| The images are well-presented and well-explained by the captions and the text.\n",
      "\n",
      "E-replicability           H-scope-breadth           ||| The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.\n",
      "\n",
      "C-motivation-impact       H-justification           ||| Considering these improvements, I would like to raise the score to 5, since the setting of combining few-shot learning and domain adaptation is interesting and the proposed model outperforms the baselines.\n",
      "\n",
      "D-originality             H-creativity\n",
      "###\n",
      "Sentence: 2) The paper also proposed a new algorithm for optimizing the upper-bound using an alternating optimization technique.\n",
      "Label: H-creativity ||| 1) The paper proposes a new theoretical upper-bound based on the prior works, the upper-bound and its derivation are interesting and heuristic to the domain adaptation research community.\n",
      "\n",
      "C-motivation-impact       H-overall-evaluation      ||| - I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance.\n",
      "\n",
      "G-substance               H-theoretical-foundation  ||| 1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.\n",
      "\n",
      "G-substance               H-adequacy-of-evidence    ||| 2) The experimental results provided in this paper are weak.\n",
      "\n",
      "B-meaningful-comparison   H-presentation            ||| That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g. states of interest vs context are given, not learned).\n",
      "\n",
      "D-originality             H-overall-assessment      ||| Overall, this paper is good, but is not novel or important enough for acceptance.\n",
      "\n",
      "A-clarity                 H-clarity-presentation    ||| - q_theta was introduced in Eq. (8) before it is defined in Eq. (11).\n",
      "\n",
      "D-originality             H-related-work            ||| Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.\n",
      "\n",
      "E-replicability           H-clarity                 ||| 1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?\n",
      "\n",
      "D-originality             D-originality             ||| Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.\n",
      "\n",
      "D-originality             D-originality             ||| Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).\n",
      "\n",
      "A-clarity                 A-clarity                 ||| - The paper is well written and easy to read.\n",
      "\n",
      "D-originality             D-originality             ||| - To my best knowledge, the idea of applying the meta-learning to the automatic generation of auxiliary tasks is novel.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac6220",
   "metadata": {},
   "source": [
    "# Zero shot\n",
    "\n",
    "Following guidelines from [Ziems et al. 2023](https://arxiv.org/abs/2305.03514) (for Latent Hatred, which is most similar to DISAPERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2a6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_suffix = \"\"\"\\n\\nWhich of the following aspects does the sentence above mention?\n",
    "A-clarity: Is the paper clear, well-written and well-structured?\n",
    "B-meaningful-comparison: Are the comparisons to prior work sufficient and fair?\n",
    "C-motivation-impact: Does the paper address an important problem?\n",
    "D-originality: Are there new topics, technique, methodology, or insights?\n",
    "E-replicability: Is it easy to reproduce and verify the correctness of the results?\n",
    "F-soundness-correctness: Is the approach sound? Are the claims supported?\n",
    "G-substance: Are there substantial experiments and/or detailed analyses?\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190274ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 200/200 [05:52<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Weird way of picking examples but whatever\n",
    "for i in tqdm.tqdm(range(num_examples_to_label)):\n",
    "  label = random.choice(old_label_list)\n",
    "  if i >= len(dataset['train'][label]):\n",
    "    continue\n",
    "    \n",
    "  _, sentence = dataset['train'][label][i]\n",
    "  text = f'{sentence}{prompt_suffix}'\n",
    "  \n",
    "  response = openai.Completion.create(\n",
    "    engine = MODEL_NAME,\n",
    "    prompt = text,\n",
    "    temperature = 0.6,\n",
    "    max_tokens = 150,\n",
    "  )\n",
    "  \n",
    "  results.append((sentence, label_map[label].strip(), response['choices'][0].text.strip().split(\":\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d508ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid answers: 58%\n",
      "Accuracy: 29%\n",
      "================================================================================\n",
      "Label                     Predicted                     Sentence\n",
      "\n",
      "A-clarity                 F-soundness-correctness   ||| Reward prediction along --> Reward prediction alone\n",
      "\n",
      "B-meaningful-comparison   B, D, F, G                ||| Although I do understand the problem of evaluation in unsupervised DA, this should have at least been done in the semi-supervised case, and some analysis/discussion should be included for the unsupervised one.\n",
      "\n",
      "C-motivation-impact       A, F, G                   ||| - The quantitative evaluation in table 1 is interesting and useful.\n",
      "\n",
      "F-soundness-correctness   F-soundness-correctness   ||| Does such approximation guarantee the policy improvement?\n",
      "\n",
      "B-meaningful-comparison   B-meaningful-comparison   ||| The evaluation compares different variants of this model to two recent VAE baselines.\n",
      "\n",
      "A-clarity                 F-soundness-correctness   ||| Minor, 1/2 is missing in the last line of Eq 19.\n",
      "\n",
      "D-originality             A, B, C, D, E, F, G       ||| This is exactly what authors do in these sections.\n",
      "\n",
      "E-replicability           E-replicability           ||| It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.\n",
      "\n",
      "C-motivation-impact       F-soundness-correctness   ||| - My biggest concern is that the technical contributions of the paper are not clear at all.\n",
      "\n",
      "A-clarity                 Answer                    ||| - Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”\n",
      "\n",
      "F-soundness-correctness   G-Substance               ||| However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.\n",
      "\n",
      "A-clarity                 E-replicability           ||| As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout?\n",
      "\n",
      "A-clarity                 A, B, C, D, F, G          ||| The paper is well-written, has good experiments, and has a comprehensive related work section.\n",
      "\n",
      "B-meaningful-comparison   G-substance               ||| Indeed, an appendix would be greatly appreciated, as many experimental details were omitted.\n",
      "\n",
      "D-originality             D-originality             ||| The novelty of this method is minimal.\n",
      "\n",
      "B-meaningful-comparison   D-originality             ||| Moreover, very recent works have also successfully incorporate both generative and discriminative network architectures (e.g., [1,2]) into the optimization process.\n",
      "\n",
      "D-originality             A, B, C, D, F, G          ||| I am reluctant to give a higher score due to its incremental contribution.\n",
      "\n",
      "C-motivation-impact       F-soundness-correctness   ||| However, in real world scenarios, it is actually challenging to obtain exact degradation information.\n",
      "\n",
      "E-replicability           E-replicability           ||| 1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?\n",
      "\n",
      "A-clarity                 H-applicability           ||| The approach is evaluated only in three cases: fish, walker, cheetah. Can it be applied to more complex morphologies? Humanoid etc. maybe?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
