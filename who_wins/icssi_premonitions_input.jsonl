{"identifier": "--GJkm7nt0|||0|||0", "text": "A knowledge distillation framework is proposed for efficient object recognition.", "label": null}
{"identifier": "--GJkm7nt0|||0|||1", "text": "In this framework, the teacher network (TN) performs high accuracy prediction while two student networks (SN) mimic the prediction from TN.", "label": null}
{"identifier": "--GJkm7nt0|||0|||2", "text": "The first SN learns from TN while the second SN is a binarized form (BSN) of the first SN.", "label": null}
{"identifier": "--GJkm7nt0|||0|||3", "text": "The design is made for online inference is that the BSN first recognizes the image, leaving the rare category objects to be recognized by the TN.", "label": null}
{"identifier": "--GJkm7nt0|||0|||4", "text": "Furthermore, an attention supervision scheme is proposed to enhance the CNN prediction by focusing on meaningful image content.", "label": null}
{"identifier": "--GJkm7nt0|||0|||5", "text": "The proposed method has been validated on CIFAR-100 and Tiny-Imagenet.", "label": null}
{"identifier": "--GJkm7nt0|||0|||6", "text": "While the distillation for fast recognition and attention supervisions sound interesting, there are a few issues to make the current manuscript not convincing on the contribution side.", "label": null}
{"identifier": "--GJkm7nt0|||0|||7", "text": "The details are listed in the following:", "label": null}
{"identifier": "--GJkm7nt0|||0|||8", "text": "1. The major issue is on the motivation side of this framework design.", "label": null}
{"identifier": "--GJkm7nt0|||0|||9", "text": "In the paper, a BSN and a TN are both adopted to recognize objects.", "label": null}
{"identifier": "--GJkm7nt0|||0|||10", "text": "The reason claimed is for the efficient recognition for rare category objects.", "label": null}
{"identifier": "--GJkm7nt0|||0|||11", "text": "However, there is no motivation for why distillation can indeed solve this problem.", "label": null}
{"identifier": "--GJkm7nt0|||0|||12", "text": "This reviewer agrees that objects from category distribute unevenly and class imbalance occurs during training.", "label": null}
{"identifier": "--GJkm7nt0|||0|||13", "text": "How class imbalance correlates to knowledge distillation is not clear.", "label": null}
{"identifier": "--GJkm7nt0|||0|||14", "text": "A common strategy is to adopt a focal-loss-based loss function to reduce contributions from easy samples.", "label": null}
{"identifier": "--GJkm7nt0|||0|||15", "text": "Suppose we want to use two CNNs for cascaded recognition, which is the core idea in this paper, the typical choice is to collect ordinary category objects for the first stage training and rare objects for the second stage training.", "label": null}
{"identifier": "--GJkm7nt0|||0|||16", "text": "A pure distillation from the TN does not ensure the differentiation of ordinary and rare categories.", "label": null}
{"identifier": "--GJkm7nt0|||0|||17", "text": "2. The claim of online distillation is weird.", "label": null}
{"identifier": "--GJkm7nt0|||0|||18", "text": "Normally the CNN is trained offline and there is no sign of online model training in the manuscript.", "label": null}
{"identifier": "--GJkm7nt0|||0|||19", "text": "As online distillation appears everywhere in the manuscript, this reviewer doubts whether this claim is suitable for illustrating the CNN training process.", "label": null}
{"identifier": "--GJkm7nt0|||0|||20", "text": "3. The attention scheme is from Grad-CAM, which back-propagates CNN prediction to formulate attention supervision terms.", "label": null}
{"identifier": "--GJkm7nt0|||0|||21", "text": "The number of categories indicates the number of back-propagation during one training iteration.", "label": null}
{"identifier": "--GJkm7nt0|||0|||22", "text": "This computational complexity is tremendous compared to the fast convergence claim of BSN.", "label": null}
{"identifier": "--GJkm7nt0|||0|||23", "text": "There shall be some analysis on how to compute the attention maps efficiently in practice.", "label": null}
{"identifier": "--GJkm7nt0|||0|||24", "text": "4. In the experiments, the comparison shall be made to the sota object recognition methods (ResNet, EfficientNet based methods) besides outlier detection methods,", "label": null}
{"identifier": "--GJkm7nt0|||0|||25", "text": "As the proposed method focuses on efficient object recogntion.", "label": null}
{"identifier": "--GJkm7nt0|||1|||0", "text": "The authors tackle the problem of efficient object recognition and outlier detection using online distillation.", "label": null}
{"identifier": "--GJkm7nt0|||1|||1", "text": "They propose to complement the standard distillation loss with a triplet loss using attention maps from the teacher to help the student focusing on the relevant part of the input images.", "label": null}
{"identifier": "--GJkm7nt0|||1|||2", "text": "They also provide a large set of experiments to validate the proposed approach.", "label": null}
{"identifier": "--GJkm7nt0|||1|||3", "text": "The paper is overall well written.", "label": null}
{"identifier": "--GJkm7nt0|||1|||4", "text": "I have two main concerns about the paper:\n- If my understanding is correct, the *hard-positive* samples used to compute the attention triplet loss are selected from the most probable class assigned by the TN while the *hard-negative* are using the second most probable class.", "label": null}
{"identifier": "--GJkm7nt0|||1|||5", "text": "Why is it desired for these first and second most probable categories to have different attention maps?", "label": null}
{"identifier": "--GJkm7nt0|||1|||6", "text": "Intuitively, if they are semantically similar it would make sense to focus on the same regions of the image to be able to distinguish them from each other.", "label": null}
{"identifier": "--GJkm7nt0|||1|||7", "text": "- Even though discussed in section A.3, I think that the claim \"BSN is adaptively trained only on IL class images without any knowledge of OL class images\" is incorrect (first paragraph of page 3).", "label": null}
{"identifier": "--GJkm7nt0|||1|||8", "text": "Since the TN is used as supervision and as a guide for the SN representation, a lot of knowledge about the OL classes can still be transmitted to the SN.", "label": null}
{"identifier": "--GJkm7nt0|||1|||9", "text": "And a few questions:\n- How does the SN perform when directly trained on all classes?", "label": null}
{"identifier": "--GJkm7nt0|||1|||10", "text": "It would give a tight lower bound to the performance of ENVISE, since some architectures used for the SN are already very competitive (especially Resnet 18 and 50).", "label": null}
{"identifier": "--GJkm7nt0|||1|||11", "text": "-", "label": null}
{"identifier": "--GJkm7nt0|||1|||12", "text": "It is surprising to see the BSN outperform the RvSN.", "label": null}
{"identifier": "--GJkm7nt0|||1|||13", "text": "Is there any reason for which the real-valued network can't converge to the same level of performance as the one achieved by the BSN?\n- Since table 5 shows that similar performances can be reached using a Resnet-18, why using a Densnet-201 for most of the experiments?", "label": null}
{"identifier": "--GJkm7nt0|||1|||14", "text": "It seems that it can improve the performance in terms of GiE but would just increase the overall inference latency of ENVISE.", "label": null}
{"identifier": "--GJkm7nt0|||1|||15", "text": "Minor remark:\n- Legends of fig 2b, 3 and 4 are too small.", "label": null}
{"identifier": "--GJkm7nt0|||2|||0", "text": "The proposed work trains a teacher-student network using an online distillation paradigm.", "label": null}
{"identifier": "--GJkm7nt0|||2|||1", "text": "The student is a binarized network (BSN) trained to be accurate on frequent classes.", "label": null}
{"identifier": "--GJkm7nt0|||2|||2", "text": "An attention triplet loss is employed to improve the accuracy of the BSN and its ability to detect outlier classes.", "label": null}
{"identifier": "--GJkm7nt0|||2|||3", "text": "Faster convergency of BSN vs Real Valued Student network is claimed.", "label": null}
{"identifier": "--GJkm7nt0|||2|||4", "text": "A new metric to evaluate the actual gain in network efficiency is proposed.", "label": null}
{"identifier": "--GJkm7nt0|||2|||5", "text": "Strengths\n-", "label": null}
{"identifier": "--GJkm7nt0|||2|||6", "text": "The idea of using the attention triplet loss to improve the quality of attention maps of the BSN and thus increasing BSN accuracy is interesting\n- Experimental evaluation is very thorough", "label": null}
{"identifier": "--GJkm7nt0|||2|||7", "text": "Weaknesses", "label": null}
{"identifier": "--GJkm7nt0|||2|||8", "text": "The main weakness of this work lies in the presentation and organization of the paper.", "label": null}
{"identifier": "--GJkm7nt0|||2|||9", "text": "-", "label": null}
{"identifier": "--GJkm7nt0|||2|||10", "text": "It is not clear how outlier detection is performed and why it is needed for the approach to correctly function.", "label": null}
{"identifier": "--GJkm7nt0|||2|||11", "text": "-", "label": null}
{"identifier": "--GJkm7nt0|||2|||12", "text": "It is not clear why at inference time false outliers are then processed by the teacher network.", "label": null}
{"identifier": "--GJkm7nt0|||2|||13", "text": "Is this because of the online setting?", "label": null}
{"identifier": "--GJkm7nt0|||2|||14", "text": "The setting should be better specified.", "label": null}
{"identifier": "--GJkm7nt0|||2|||15", "text": "- Sentence \"In conventional knowledge distillation, the attention map of the BSN can focus on the background\neven when the attention map of the TN emphasizes the semantically meaningful regions of the image.\", should be followed by one (possibly more) citations supporting this claim.", "label": null}
{"identifier": "--GJkm7nt0|||2|||16", "text": "- Lemma 3.1 should be followed by a sketch proof, being one of the main contribution, with the full proof in the appendix (as it is already).", "label": null}
{"identifier": "--GJkm7nt0|||2|||17", "text": "Related work missing:\nExisting literature binarizing CNN in distillation exists:\n[a]Distillation Guided Residual Learning for Binary Convolutional Neural Networks, 2018\nTriplet losses are used in distillation:\nTriplet Loss for Knowledge Distillation, 2017 (arxiv), IJCNN(2020)", "label": null}
{"identifier": "--GJkm7nt0|||2|||18", "text": "The main reason for my score regards the overall presentation of the paper.", "label": null}
{"identifier": "--GJkm7nt0|||2|||19", "text": "The main two contributions are the faster convergence and the use of triplet attention loss.", "label": null}
{"identifier": "--GJkm7nt0|||2|||20", "text": "The lemma should have been better highlighted with a sketch proof or at least some intuition so that readers not willing to sift through the appendix could get a grasp of it.", "label": null}
{"identifier": "--GJkm7nt0|||2|||21", "text": "The triplet attention loss should be better framed in the introduction (see above).", "label": null}
{"identifier": "--GJkm7nt0|||2|||22", "text": "Finally some of the mechanisms of the approach are unclear (how to get the OL score, why the TN must evaluate fp of the BSN) and so on.", "label": null}
{"identifier": "--GJkm7nt0|||3|||0", "text": "- The idea is interesting to focus on the frequent labels by distilling a special binary network.", "label": null}
{"identifier": "--GJkm7nt0|||3|||1", "text": "But the technical details are not clearly presented and important experiments are missing.", "label": null}
{"identifier": "--GJkm7nt0|||3|||2", "text": "Overall, this paper does not reach the acceptance threshold.", "label": null}
{"identifier": "--GJkm7nt0|||3|||3", "text": "Detailed Comments:", "label": null}
{"identifier": "--GJkm7nt0|||3|||4", "text": "- Why is this method an 'online' distillation considering that the teacher network is pretrained and fixed?", "label": null}
{"identifier": "--GJkm7nt0|||3|||5", "text": "How do you define offeline distillation and online distillation?\n- \"The OL detector uses the softmax output of the BSN...\"", "label": null}
{"identifier": "--GJkm7nt0|||3|||6", "text": "How is the softmax output is used?", "label": null}
{"identifier": "--GJkm7nt0|||3|||7", "text": "Do you have a separate class named outlier apart from all the object categories?", "label": null}
{"identifier": "--GJkm7nt0|||3|||8", "text": "It should be made clear.", "label": null}
{"identifier": "--GJkm7nt0|||3|||9", "text": "-", "label": null}
{"identifier": "--GJkm7nt0|||3|||10", "text": "Why does BSN converge faster than RvSN considering that BSN is a binarized version of RvSN?", "label": null}
{"identifier": "--GJkm7nt0|||3|||11", "text": "What is the retionale behind this?", "label": null}
{"identifier": "--GJkm7nt0|||3|||12", "text": "-", "label": null}
{"identifier": "--GJkm7nt0|||3|||13", "text": "In the experimental part, only comparison to baseline methods are provided, but not to state-of-the-art efficient object recognition methods in the literature.", "label": null}
{"identifier": "--GJkm7nt0|||3|||14", "text": "Various methods have been proposed recently to improve the efficiency of an object recognition network, such as distillation, pruning, tensor decomposition, quatization.", "label": null}
{"identifier": "--GJkm7nt0|||3|||15", "text": "There should be comparison to these methods as well as baselines.", "label": null}
{"identifier": "--GJkm7nt0|||3|||16", "text": "Besides, the performance on Imagenet (not only tiny imagenet) is a common practice in the literature, but is missing in this paper.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||0", "text": "Post revision update", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||1", "text": "-------------------------------", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||2", "text": "The authors have been very helpful and addressed many of my concerns, and I think the revised paper is a substantial improvement.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||3", "text": "I have rated the paper as a 7, although I do have some lingering concerns.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||4", "text": "Most crucially, it's still not clear to me that the compositional rules the authors highlight are the correct way to characterize the differences in patterns of behavior, since, for example, both models significantly outperform humans at the tree rule.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||5", "text": "However, the authors do point out that humans perform better at these tasks than the null distribution.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||6", "text": "Still, I worry that the authors are focusing on the wrong dimension along which the compositional and null task distributions differ.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||7", "text": "However, I think the fact that the authors followed the suggestion to include the results in the appendix is helpful in this regard, at least future researchers will be able to see the full pattern of results to draw their own conclusions.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||8", "text": "Original Review\n---------------------------", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||9", "text": "This paper proposes to explore whether meta-learning approaches can exploit a compositional structure in their tasks to generalize, and compares this ability to humans.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||10", "text": "To do so, the paper introduces a grid dataset consisting of generative grammars for generating compositional grids, as well as a null task distribution which is non-compositional but matches on certain low-order statistics.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||11", "text": "These tasks are interesting and new.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||12", "text": "They have humans and agents perform a task to reveal rewarding squares on the grid, and compare to agents that meta-learn this task.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||13", "text": "Human subjects perform better at the compositional distribution, whereas models perform better at the null distribution.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||14", "text": "They conclude that \"compositional structure remains difficult for these systems and that they prefer other statistical features\" and that this \"highlights the importance of endowing artificial systems with this bias.\"", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||15", "text": "While the topic is timely, and the tasks are interesting, there are a number of limitations to the model and training which I think seriously limit the conclusions.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||16", "text": "I think that the task is really only compositional for a model which is able to fixate on different locations on the grid.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||17", "text": "Thus, I recommend rejecting for now, although I think a revision with a more sophisticated model and more thorough discussion could be a valuable contribution.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||18", "text": "Strengths:", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||19", "text": "* Interactive tasks for humans and models are a great improvement over prior toy datasets on compositionality, e.g. SCAN.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||20", "text": "* I think the tasks are very interesting, and offer directions that aren't really address by prior compositional generalization datasets that mostly rely on composition of words, or on composition of visual properties like color and shape.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||21", "text": "* It's great to see actual comparisons to human performance, and careful thinking about how to evaluate compositional generalization vs. other types (though this could be developed further, see below).", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||22", "text": "Areas for improvement:", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||23", "text": "* Is generalization equally good for humans under each rule type?", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||24", "text": "If not, this might affect the conclusions.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||25", "text": "For example, perhaps humans would be very good at inferring chains, but not the more complex structures (like trees).", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||26", "text": "If so, it could be that \"compositionality\" per se is not the construct underlying their performance, but rather \"chains\" or some other, simpler construct.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||27", "text": "Because performance is not presented broken down by structure type, it is difficult to determine whether some pattern like this could explain the results.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||28", "text": "Thus, it is difficult to conclude that compositionality is the factor underlying the results.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||29", "text": "* There are a number of features of the agent that confound the comparison with the humans.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||30", "text": "These limit the ability to draw a strong conclusion about e.g. \"the importance of endowing artificial systems with [a compositional] bias.\" \n    *", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||31", "text": "Why are the main comparisons run using the non-convolutional model, when the convolutional one is clearly more closely matched to humans (as the discussion acknowledges)?", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||32", "text": "It seems like most (although not all) the difference in performance is due to spatial bias, rather than compositionality.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||33", "text": "* Indeed, this spatial bias is partially addressed on the input (by the convolutional experiments), but it is *never* addressed on the output.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||34", "text": "Humans know that there is a spatial structure to the tiles they are clicking on, the agent can access this information only implicitly.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||35", "text": "*", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||36", "text": "To address this, one could build a recurrent attention model (e.g. Mnih et al, 2014; Gregor et al, 2015) which can make visual saccades around the grid before deciding whether to reveal the square at the current point of fixation, or whether to fixate to another location.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||37", "text": "This would likely match the human process better, since the humans are likely fixating their gaze on the locations they are considering, rather than fixating in the center of the grid without moving their eyes.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||38", "text": "It's also motivated by the observation that agents generalize better if they receive ego-centric input rather than visual input fixed on the grid (c.f.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||39", "text": "Hill et al, 2020; Ye et al, 2020).", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||40", "text": "This is an important issue to the claims at stake.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||41", "text": "For an agent which could fixate on each location it was considering, the compositional rules would be much more consistent than for an agent that perceives the whole grid from a fixed perspective.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||42", "text": "For a fixating agent, the compositional rules would also be much more consistent than the null distribution.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||43", "text": "In fact, I would suggest that it is *only from the perspective of a model which can fixate that this distribution can be considered compositional at all.*", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||44", "text": "How can we tell that the difference between the humans and the model isn't due to the human ability to fixate, rather than some abstract bias toward compositionality?\n    *", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||45", "text": "The paper may not be able to address all the ways in which the model's experience of the task is unlike humans, but then the there should be a *corresponding tempering of the conclusion that the comparison to humans says something specific about the difference between the model and humans.*", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||46", "text": "That is, given the current experiments, the discussion of this paper should focus at least as much on the limitations of the present model as on general conclusions about failures of the model class and the need for additional inductive biases.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||47", "text": "* Furthermore, exploring compositionality in toy tasks can be misleading.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||48", "text": "Hill et al. (2020) show that compositional generalization is significantly improved in more realistic settings (for example an RL agent that executes actions over time achieves 100% compositional generalization on a task that a feed-forward classifier only achieves 80% generalization on).", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||49", "text": "They argue that toy stimuli remove one of the most important elements for training deep models \u2014 the rich environments in which humans, also, are trained.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||50", "text": "Even if the input and output of the model and humans were better matched on this dataset, it may be misleading to conclude something as general as \"the importance of endowing artificial systems with this [compositional] bias\" without giving these models training on a distribution of stimuli and tasks that more closely match the rich variety which humans experience over development.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||51", "text": "Of course, it is not feasible in practice to do so (yet).", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||52", "text": "But this limitation and its relevance to the conclusions should at least be acknowledged in the discussion.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||53", "text": "* The paper could use some more discussion of the distinction between statistical patterns and compositional rules.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||54", "text": "This seems like an important point, but it wasn't entirely clear to me.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||55", "text": "Compositional rules correspond to a certain statistical distribution over grids.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||56", "text": "The fact that up to 2nd order Ising statistics are matched does not mean that the distributions of outcomes are matched \"statistically,\" it merely means they match in certain low-order statistics.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||57", "text": "It's not clear if these are the right statistics by which to compare the distributions (especially for the non-convolutional model, which has no spatial awareness).", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||58", "text": "The paper would be strengthened by justifying the choice of these statistics more carefully, and articulating a clear distinction between what counts as a \"statistical\" pattern vs. a rule.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||59", "text": "References \n---------", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||60", "text": "Gregor, Karol, et al.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||61", "text": "\"Draw: A recurrent neural network for image generation.\" arXiv preprint arXiv:1502.04623 (2015).", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||62", "text": "Hill, Felix, et al.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||63", "text": "\"Environmental drivers of systematicity and generalization in a situated agent.\"", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||64", "text": "International Conference on Learning Representations, 2020.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||65", "text": "Mnih, Volodymyr, Nicolas Heess, and Alex Graves.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||66", "text": "\"Recurrent models of visual attention.\"", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||67", "text": "Advances in neural information processing systems.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||68", "text": "2014.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||69", "text": "Ye, Chang, et al.", "label": null}
{"identifier": "--gvHfE3Xf5|||0|||70", "text": "\"Rotation, Translation, and Cropping for Zero-Shot Generalization.\" arXiv preprint arXiv:2001.09908 (2020).", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||0", "text": "*Summarize what the paper claims to contribute.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||1", "text": "Be positive and generous.*", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||2", "text": "This paper offers an interesting comparison between humans and a meta-learning algorithm [1,2] learning to uncover structured patterns on a 7x7 grid.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||3", "text": "The patterns are either generated using simple \u201ccompositional rules\u201d (lines, loops or trees) or sampled from a distribution that has almost identical 0th, 1st and 2nd order statistics as the ones generated through the \u201ccompositional rules\u201d.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||4", "text": "The board is initially covered, and the agent (either a human or a RL meta-learning algorithm) is tasked with selecting one by one which tiles are to be uncovered.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||5", "text": "The aim is to guess which tiles are covered by the pattern (red tiles) and avoid the background (blue tiles), thus uncovering the red tiles as accurately as possible.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||6", "text": "The authors go on to show supporting evidence that most likely, humans are using a compositional inductive bias when trying to uncover the hidden pattern, whereas the meta-learning algorithms do not.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||7", "text": "This conclusion is arrived at using the relative performance of both humans and the algorithm on the cases where the pattern was generated using the two different schemes.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||8", "text": "They then go on to conclude that there is a strong difference between the strategy learned by the algorithm, which seems to use mostly the statistics, compared to humans that seem to make use of a compositional inductive bias.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||9", "text": "[1] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||10", "text": "Rl^2: Fast reinforcement learning via slow reinforcement learning.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||11", "text": "arXiv preprint arXiv:1611.02779, 2016.\n[2]", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||12", "text": "Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||13", "text": "Learning to reinforcement learn.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||14", "text": "arXiv preprint arXiv:1611.05763, 2016.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||15", "text": "*List strong and weak points of the paper.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||16", "text": "Be as comprehensive as possible.*", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||17", "text": "**Strong points**", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||18", "text": "- The paper is very clear and very well written.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||19", "text": "I like the simplicity of the experiment and the approach taken by the authors in trying to uncover whether humans and a particular algorithm follow similar strategies when solving tasks.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||20", "text": "- I see no reason why this approach should not be re-used by other researchers, and I hope that it will inspire others to go to similar lengths when analysing the operation of their algorithms.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||21", "text": "Specifically, I consider this a strong point of the paper as I believe it offers an obvious and accessible set of future work.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||22", "text": "**Weak points**", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||23", "text": "- It is not completely clear how the statistically similar patterns are constructed.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||24", "text": "Is it possible to get patterns that are identical to the compositionally generated ones?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||25", "text": "If so, have you checked and removed these?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||26", "text": "Please elaborate a bit more on this aspect, show more examples of the non-compositional patterns, and describe how you decided on the Compositional-passing/not compositional passing split in Figure 3C.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||27", "text": "-", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||28", "text": "One somewhat deeper weakness of this work is that one can claim that there is no inherent notion of compositionality in the patterns of the data per-se, but rather in the manner through which they were generated.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||29", "text": "This is perhaps an underlying reason why the meta-learning algorithm never picks up the compositionality inductive bias exhibited by humans.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||30", "text": "My point does not mean to invalidate the findings and conclusion, but rather to emphasise that perhaps we shouldn\u2019t be looking at this type of experiments, as they are \u201cdoomed to fail\u201d.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||31", "text": "Of course this might be obvious in hindsight, but I would be interested to read what the authors think about this.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||32", "text": "*Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||33", "text": "Provide supporting arguments for your recommendation.*", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||34", "text": "I recommend accepting the paper, mainly for the two strong points earlier.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||35", "text": "I believe what the length at which authors have gone to for understanding what an algorithm does (or what it doesn\u2019t do) should be an example for our field, which often lacks imagination when it comes to analysing proposed models.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||36", "text": "I do think that a condition for accepting is at least ensuring that the details of all the generated samples (compositional or not) are included in the paper.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||37", "text": "How many (unique) of each were generated/used?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||38", "text": "How did you decide the compositional-passing/not compositional passing split.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||39", "text": "Did you check whether any non-compositional pattern matched the compositional one?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||40", "text": "If so, how and did you reject that sample?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||41", "text": "*Provide additional feedback with the aim to improve the paper.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||42", "text": "Make it clear that these points are here to help, and not necessarily part of your decision assessment.*", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||43", "text": "- In the intro: \u201cSecond, humans represent this learned information compositionally\u201d.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||44", "text": "In my opinion this is a very strong statement about the nature of the representation in the human brain and it\u2019s best avoided unless there\u2019s neurophysiological evidence.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||45", "text": "Consider rephrasing to a softer version with a reference.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||46", "text": "- 4th paragraph in the introduction: \u201cOur methodological contribution in this work is *to* develop novel tasks.\u201d\n- Last paragraph of introduction: \u201cSince a large swath of real-world tasks contain compositional structure..\u201d", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||47", "text": "Which ones?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||48", "text": "Please include some examples.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||49", "text": "Also, is it that they contain compositional structure (i.e. in the way they are generated) or that the compositional inductive bias that humans exhibit can more efficiently solve them?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||50", "text": "- Second sentence of 3.2 Results:", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||51", "text": "\u201cWe demonstrate that humans have a clear bias toward compositional distributions, without extensive training and even while directly controlling for statistical complexity.\u201d", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||52", "text": "Couldn\u2019t we claim that the patterns generated by the compositional distributions are easier/simpler?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||53", "text": "They are after all only 3 different rules.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||54", "text": "Wouldn\u2019t that be a good enough explanation of why humans solve those much more easily than the statistically matched ones?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||55", "text": "You did touch upon the issue of spatial proximity later on.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||56", "text": "Can you touch upon this point too?", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||57", "text": "I think it\u2019s a possible (and perhaps simpler) alternative explanation of why humans would be better at them.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||58", "text": "- Appendix A.1.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||59", "text": "The hyperparameters are reported in an unnecessary accuracy - I would assume that results are not sensitive to that many significant figures.", "label": null}
{"identifier": "--gvHfE3Xf5|||1|||60", "text": "Please consider 2 or 3 s.f. in scientific notation for simplicity.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||0", "text": "This paper sets out to determine something about the form of the bias acquired by a standard meta-learning algorithm, and compare the form of that bias to the inherent bias that humans have.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||1", "text": "The authors point out, rightly, that meta-learning algorithms have meta-biases and it is important to understand these, from both the scientific and engineering perspectives.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||2", "text": "It is well written and raises good questions.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||3", "text": "There is a cleverly constructed test domain and a set of well-executed computer and human experiments (I think---I don't really know about how to construct a human experiment.)", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||4", "text": "Unfortunately, I can't end up agreeing or disagreeing with the claims made in the paper, or really understands how well they are supported by evidence, because I find that they use terms that don't seem to be sufficiently technically well defined.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||5", "text": "For example:\n- what exactly is compositional structure?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||6", "text": "- what is statistical structure?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||7", "text": "- what is your measure of task complexity?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||8", "text": "How can we tell if what the agent learns is compositional?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||9", "text": "Is that an externally measurable property of the agent's behavior and the way it generalizes to new environments?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||10", "text": "Or is it a property of the internal representation?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||11", "text": "(It is common to have an intuition that \"compositional\" also implies \"compact\" or \"low complexity\" in some sense.)", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||12", "text": "It feels like generalization be a way to get more clearly at the presence of a compositional representation:  could you train on small grids and have the learned agent generalize to big ones?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||13", "text": "It seems like if a fixed-size representation can generalize to very large instances, then that is more clear evidence of compositionality (but then I'm thinking of compositionality as a property of a representation, not of externally-measurable behavior.)", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||14", "text": "I also feel that I don't quite understand the meta-learning training regime.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||15", "text": "What exactly constituted a \"task\" from the meta-learning perspective?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||16", "text": "Is it a single board?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||17", "text": "If so, then the meta-learning problem is to learn the task distribution, in some sense.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||18", "text": "I was expecting something more \"meta\":  that is, to test whether the system is actually meta-learning the *idea* of compositionality, it seems like set-up would be that a task corresponds to a particular grammar with multiple boards drawn from the distribution induced by the grammar;  then we'd know that it had meta-learned compositionality if it could learn *new grammars* quickly.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||19", "text": "Smaller points\n- I didn't completely understand the production rule (nor the examples) for the loop structure.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||20", "text": "-", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||21", "text": "It would help me understand the task set better if there were a slightly more in-depth description of the chains, trees, and loops and described how the grammar generates the compositional tasks in figure 2.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||22", "text": "Is it not the case that every connected configuration of red tiles could be described as a tree?", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||23", "text": "- Rather than showing just one number for the final performance,", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||24", "text": "It would be helpful to show learning curves for the RL algorithms so the reader can assess the stability, convergence, etc.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||25", "text": "Similarly, learning curves for humans would be interesting, but less important since I assume they just look flat.", "label": null}
{"identifier": "--gvHfE3Xf5|||2|||26", "text": "- \"is develop\"", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||0", "text": "This work is an exploration of model behaviour upon meta-learning tasks with compositional structure.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||1", "text": "The authors discover that, unlike humans, machine learning models do not readily pick up on the underlying compositional generative structure of a set of tasks, and hence cannot match the performance of humans.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||2", "text": "Conversely, when the task is structured to leverage other statistical patterns, models do well.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||3", "text": "Taken as a whole, this is a nice piece of work.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||4", "text": "The presentation is well crafted, and I believe the experiments are well planned.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||5", "text": "There are many nice analyses and some welcome statistics, such as shown in Figure 3.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||6", "text": "The authors are commended for their work.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||7", "text": "I wish to lay out a few criticisms, and I'd like the authors to know that the points are all very easily fixable.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||8", "text": "The authors design a set of structure-learning tasks using a generative grammar.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||9", "text": "The exact details of the grammar are not given, and the reader is to rely on rough intuitions based on the figures.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||10", "text": "I encourage the authors to spell out some more details of the methods.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||11", "text": "The authors argue that the non-compositional boards could not have been generated by the defined grammar, which seems fair, but they also argue that these boards are necessarily non-compositional.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||12", "text": "I am having a lot of trouble with this statement, because it is not entirely clear by what the authors mean by compositional, as it hasn't been clearly defined.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||13", "text": "This is a particular problem in the machine learning field as a whole, as it pertains to research on compositionality; rarely is the term defined in any rigorous sense, and from paper to paper there are seemingly different definitions.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||14", "text": "I encourage the authors to clearly explain what entails compositionality as they refer to it here, and to explain what makes their non-compositional boards non-compositional according to their definition.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||15", "text": "The \"non-compositional\" boards might not match the generative grammar as used in the compositional setting, but it does not entail that there does not exist a compositional grammar that can produce the non-compositional boards.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||16", "text": "In fact, due to the discrete, simplified nature of the game, it would seem almost certainly true that there exists *some* generative grammar that can produce the non-compositional boards seen here.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||17", "text": "It might not be a \"simple\", \"interesting\", or human interpretable one, but it would nonetheless be a grammar, and would be compositional.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||18", "text": "This fact makes it all the more important to define compositionality.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||19", "text": "It seems to me that what is more precisely being illustrated is the ability for humans to perceive, and infer the implications of abstract, \"simple\" *structures*, and not compositional rules or grammars per se.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||20", "text": "Without fully defining compositionality and establishing the non-compositionality in the null setting, I'm afraid that the results are misstated and misrepresented.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||21", "text": "A note to the authors: I'm fully aware that I could be misunderstanding how the MLP+", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||22", "text": "Gibbs sampling method here can entail non-compositionality, and am more than open to being corrected on this matter.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||23", "text": "I look forward to a discussion in the rebuttal.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||24", "text": "The previous point leads me to a broader point about the background presented throughout.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||25", "text": "The authors include many broad, and quite bold statements regarding humans, and how they learn, especially in regards to their capacity for \"learning compositionality\".", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||26", "text": "It is claimed that humans learn rapidly, with very few samples, which is contrasted with machine models that require an enormous amount of data.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||27", "text": "This is true in some superficial sense, but does not account for the the entire evolutionary trajectory that produced humans; indeed, humans at birth are not blank slates to the degree that randomly initialized neural networks are.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||28", "text": "It's also claimed that humans learn compositional representations.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||29", "text": "While this idea is certainly in vogue in some circles in cognitive science, it is certainly not widely agreed upon.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||30", "text": "I'm not even sure how such a strong statement can be proven.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||31", "text": "The citations given point to a cople computational modeling papers, which, in my opinion, insufficiently corroborate such a claim.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||32", "text": "Moreover, it's not even clear how many non-artificial pieces of data that humans deal with are even truly compositional (for a taste of the issues and controversy surrounding one particular example --- language --- see the following entry: https://plato.stanford.edu/entries/compositionality/).", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||33", "text": "Please note that this is not at all to say the views presented in this paper are necessarily false.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||34", "text": "I merely suggest that the authors take another pass at their writing, and tame a few of the broader, bolder claims about the nature of human learning, because it's not clear that they are necessarily true, either.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||35", "text": "I believe the authors have missed out on some possible interpretations to their results.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||36", "text": "They claim that a meta-learned model cannot learn the compositional structure of the tasks, since meta-learning is insufficient to establish the inductive biases required for compositional understanding.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||37", "text": "However, the inductive bias of a model is determined by more than its parameters (which in this case, are established via meta-learning).", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||38", "text": "The functions a model comes to learn are also dependent on the nature of the computations, manifest through the architecture, and other such things.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||39", "text": "The authors are well aware of this, since they include a condition wherein the model uses convolutions.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||40", "text": "But the fact that the convolutional model does better entails that there might exist further architectural variants that do even better than it, and potentially, better than humans.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||41", "text": "If this were the case, then we'd no longer be able to claim that meta-learning cannot establish the proper inductive bias for compositional understanding.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||42", "text": "Therefore, the existence of a gradient in model performances warrants more careful wording in regards to the claims; we cannot so broadly categorize meta-learning as insufficient for establishing the right inductive biases for compositional understading without caveating according to the other sources of inductive bias.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||43", "text": "One piece of proof that the humans understand the compositional structure of the task is that they improve over the course of the task.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||44", "text": "But could this not simply reflect the fact that humans have a better capacity to improve behaviour over short time periods?", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||45", "text": "In other words, how do we know that the problem with the models doesn't have to do with, say, working memory, rather than compositional understanding per se?", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||46", "text": "Altogether, this is a well put together paper.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||47", "text": "There is a lot of interesting work here, and the authors have done well to explore various facets of the setup.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||48", "text": "My main criticisms have to do with the way the work is pitched, and the way the results are interpreted.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||49", "text": "It is very much presented with a veneer that speaks to a very particular crowd in cognitive-science inspired machine learning community.", "label": null}
{"identifier": "--gvHfE3Xf5|||3|||50", "text": "But since the views in this community are not necessarily broadly shared, many of the statements, interpretations, and claims come across as quite strong and not fully corroborated.", "label": null}
{"identifier": "--rcOeCKRh|||0|||0", "text": "This paper introduces a new method for training an object detector on a dataset that consists of some object categories with instance-level bounding box annotations, as well as some other object categories with only image-level labels.", "label": null}
{"identifier": "--rcOeCKRh|||0|||1", "text": "The topic is interesting, important, and potentially very useful for real applications.", "label": null}
{"identifier": "--rcOeCKRh|||0|||2", "text": "The authors propose an idea to transfer knowledge from a weakly supervised (WS) detection head into a fully supervised (FS) detection head, by producing pseudo-ground-truth bounding boxes for classes with image-level labels.", "label": null}
{"identifier": "--rcOeCKRh|||0|||3", "text": "The idea is straightforward and interesting.", "label": null}
{"identifier": "--rcOeCKRh|||0|||4", "text": "Experiments show significant and consistent gains in various scenarios.", "label": null}
{"identifier": "--rcOeCKRh|||0|||5", "text": "The paper is well-written.", "label": null}
{"identifier": "--rcOeCKRh|||0|||6", "text": "The main drawback of this paper is the lack of substantial novelty.", "label": null}
{"identifier": "--rcOeCKRh|||0|||7", "text": "The authors claim to propose a \"novel paradigm\" named cross-supervised object detection, while this task has already been studied as reviewed in Section 2.", "label": null}
{"identifier": "--rcOeCKRh|||0|||8", "text": "Particularly, the idea of jointly training WS and FS heads on WS and FS labels with a shared backbone has been explored before (e.g. in [1], which should have been discussed in the related work).", "label": null}
{"identifier": "--rcOeCKRh|||0|||9", "text": "The idea of using a WS model to create pseudo-ground-truth bounding boxes for training another model has also been studied before (e.g. in [2]).", "label": null}
{"identifier": "--rcOeCKRh|||0|||10", "text": "The only novel idea of this paper seems to be the Spatial Correlation Module (SCM), which is discussed more below.", "label": null}
{"identifier": "--rcOeCKRh|||0|||11", "text": "SCM is used to transform proposal boxes selected by the WS detector head into more accurate bounding boxes to serve as pseudo-ground-truth for WS classes.", "label": null}
{"identifier": "--rcOeCKRh|||0|||12", "text": "To this end, the authors train a class-agnostic bounding box refinement module on FS classes, and apply it on the proposals of WS classes.", "label": null}
{"identifier": "--rcOeCKRh|||0|||13", "text": "However, a similar result could have been (probably) achieved by simply replacing the bounding box regression head of the Faster R-CNN with a class-agnostic head, and training it on base classes, while using it to refine the proposals of both base and novel classes during test.", "label": null}
{"identifier": "--rcOeCKRh|||0|||14", "text": "The authors did not explore such simple alternatives to SCM.", "label": null}
{"identifier": "--rcOeCKRh|||0|||15", "text": "Another issue is that although the authors cited several existing methods for cross-supervised object detection in Section 2, they did not discuss why the proposed method is superior, and they did not include them in performance tables, claiming \"these methods can only perform object localization in single object scenes.\"", "label": null}
{"identifier": "--rcOeCKRh|||0|||16", "text": "I cannot verify the correctness of this claim, as any proposal-based model can detect multiple objects per image and per class.", "label": null}
{"identifier": "--rcOeCKRh|||0|||17", "text": "Nevertheless, the paper has a clear motivation and idea, a scientifically sound analysis, and significant results and insights that can be helpful for future work.", "label": null}
{"identifier": "--rcOeCKRh|||0|||18", "text": "Therefore, I recommend acceptance.", "label": null}
{"identifier": "--rcOeCKRh|||0|||19", "text": "If the authors can convince me that the paper also has substantial novelty and advantages over all existing works, I am willing to raise my rating.", "label": null}
{"identifier": "--rcOeCKRh|||0|||20", "text": "Minor comment:", "label": null}
{"identifier": "--rcOeCKRh|||0|||21", "text": "In equation (1), the two terms of the binary cross-entropy should probably be placed in parentheses, so the sum is applied on both terms.", "label": null}
{"identifier": "--rcOeCKRh|||0|||22", "text": "Also in the paragraph above eq (1), that loss term should be defined as a multi-label cross-entropy, rather than multi-class.", "label": null}
{"identifier": "--rcOeCKRh|||0|||23", "text": "[1] Yang, Hao, Hao Wu, and Hao Chen.", "label": null}
{"identifier": "--rcOeCKRh|||0|||24", "text": "\"Detecting 11k classes: Large scale object detection without fine-grained bounding boxes.\"", "label": null}
{"identifier": "--rcOeCKRh|||0|||25", "text": "Proceedings of the IEEE International Conference on Computer Vision.", "label": null}
{"identifier": "--rcOeCKRh|||0|||26", "text": "2019.", "label": null}
{"identifier": "--rcOeCKRh|||0|||27", "text": "[2] Tang, Peng, et al.", "label": null}
{"identifier": "--rcOeCKRh|||0|||28", "text": "\"Multiple instance detection network with online instance classifier refinement.\"", "label": null}
{"identifier": "--rcOeCKRh|||0|||29", "text": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "label": null}
{"identifier": "--rcOeCKRh|||0|||30", "text": "2017.", "label": null}
{"identifier": "--rcOeCKRh|||0|||31", "text": "######## Post-Rebuttal Updates:", "label": null}
{"identifier": "--rcOeCKRh|||0|||32", "text": "After reading the authors' response and other reviewers' opinions (especially R3), I would like to downgrade my rating slightly from 7 to 6.", "label": null}
{"identifier": "--rcOeCKRh|||0|||33", "text": "I still think the paper makes valuable contributions, but I also think the contributions are overstated and not precisely justified.", "label": null}
{"identifier": "--rcOeCKRh|||0|||34", "text": "Particularly:", "label": null}
{"identifier": "--rcOeCKRh|||0|||35", "text": "1. I agree with R3 that the limitation of novelty should be considered from the two perspectives of \"task\" and \"method\".", "label": null}
{"identifier": "--rcOeCKRh|||0|||36", "text": "The task is certainly not new, which should be made clear in the paper.", "label": null}
{"identifier": "--rcOeCKRh|||0|||37", "text": "The newest version of the paper still claims \"we define a new task\u2014cross-supervised object detection\"", "label": null}
{"identifier": "--rcOeCKRh|||0|||38", "text": "2. The method is indeed somewhat new, due to the use of multi-task learning and SCM, but its novelty should be clarified, and compared to all similar methods, not only some of them which are weaker.", "label": null}
{"identifier": "--rcOeCKRh|||0|||39", "text": "For instance, the authors did not adequately justify whether/why their method is superior to YOLO 9000, or Yang et al ([1] above).", "label": null}
{"identifier": "--rcOeCKRh|||0|||40", "text": "They did mention some differences in response to R3's comments and mine, but I am not convinced.", "label": null}
{"identifier": "--rcOeCKRh|||0|||41", "text": "Moreover, the authors did not explicitly discuss and compare those distinctions in the paper, neither quantitatively nor intuitively.", "label": null}
{"identifier": "--rcOeCKRh|||0|||42", "text": "3. In response to R3, The authors claim they \"are the first to address this problem in situations of realistic complexity\", which is not accurate.", "label": null}
{"identifier": "--rcOeCKRh|||0|||43", "text": "Particularly, the paper reads \"While several works [...] have explored this problem before, [...]", "label": null}
{"identifier": "--rcOeCKRh|||0|||44", "text": "They struggle to learn under more complex and realistic scenarios, where there are multiple objects from potentially very different classes\"", "label": null}
{"identifier": "--rcOeCKRh|||0|||45", "text": "This is not entirely true, as Yang et al. successfully evaluate on Open Images (and also VOC and COCO in their supplementary materials).", "label": null}
{"identifier": "--rcOeCKRh|||0|||46", "text": "The authors do not provide a convincing reason or evidence of existing methods \"struggling\" in realistic settings.", "label": null}
{"identifier": "--rcOeCKRh|||0|||47", "text": "YOLO 9000 is open-source, and could have been compared to the proposed method to confirm that claim.", "label": null}
{"identifier": "--rcOeCKRh|||0|||48", "text": "Accordingly, I strongly encourage the authors to refine their claims and lay more emphasis on the actually novel aspects of their method, by thoroughly comparing those novelties with all similar methods.", "label": null}
{"identifier": "--rcOeCKRh|||0|||49", "text": "I still hope to see this paper accepted, but cannot endorse it due to insisting on inaccurate claims.", "label": null}
{"identifier": "--rcOeCKRh|||1|||0", "text": "**Summary and contributions**:", "label": null}
{"identifier": "--rcOeCKRh|||1|||1", "text": "The paper presents a new task formulation for transferring knowledge for object detection from fully labeled classes to weakly labeled ones, with better results for complex scenes.", "label": null}
{"identifier": "--rcOeCKRh|||1|||2", "text": "So the authors propose to learn object detectors for novel classes (which were not seen at training time but specified apriori), based only on that object class label and the bounding boxes for other objects in the image (from the base classes).", "label": null}
{"identifier": "--rcOeCKRh|||1|||3", "text": "Compared with other works, the solution claims to be better on the localization aspect, focusing on the object as a whole, not only on its discriminative parts.", "label": null}
{"identifier": "--rcOeCKRh|||1|||4", "text": "Nevertheless, there are very few competitors taken into account.", "label": null}
{"identifier": "--rcOeCKRh|||1|||5", "text": "The model combines three components: a detection head and a recognition head, based on the same, unified backbone architecture, and a spatial correlation module that aligns the two heads.", "label": null}
{"identifier": "--rcOeCKRh|||1|||6", "text": "The authors test their solution, Cross-Supervised Object Detection, on PASCAL VOC and COCO (multi-object scenes).", "label": null}
{"identifier": "--rcOeCKRh|||1|||7", "text": "**Strengths**:\n- Proposing a model that succeeds in learning both novel and base classes at the same time, on two heads, extracting information from the weak labels (recognition head) and using it as supervision for the second head (detection head).", "label": null}
{"identifier": "--rcOeCKRh|||1|||8", "text": "-", "label": null}
{"identifier": "--rcOeCKRh|||1|||9", "text": "The qualitative results look significantly better compared with single heads, containing more of the entire object, not only the important/discriminatory parts as the authors pointed out.", "label": null}
{"identifier": "--rcOeCKRh|||1|||10", "text": "- FCOS with only 5 conv layers reaches a good enough performance.", "label": null}
{"identifier": "--rcOeCKRh|||1|||11", "text": "**Weaknesses**:\n* How does this work compare with other WSOD recent methods, e.g. C-MIL, [A, B, C] from the experimental results point of view, and the amount of supervision?\n*", "label": null}
{"identifier": "--rcOeCKRh|||1|||12", "text": "The text/figure should be adjusted to better map one each other: Sec. 3.2: \u201cThe image and proposals are fed into several convolutional layers\u201d, but in the figure, the proposals skip those layers.", "label": null}
{"identifier": "--rcOeCKRh|||1|||13", "text": "* Section 3.2 is rather hard to read and understand (it has several inaccuracies in the formulas).", "label": null}
{"identifier": "--rcOeCKRh|||1|||14", "text": "**Quality**:", "label": null}
{"identifier": "--rcOeCKRh|||1|||15", "text": "The paper is technically sound.", "label": null}
{"identifier": "--rcOeCKRh|||1|||16", "text": "**Clarity**:", "label": null}
{"identifier": "--rcOeCKRh|||1|||17", "text": "The paper is mainly clearly written (except for Section 3.2 Recognition Head).", "label": null}
{"identifier": "--rcOeCKRh|||1|||18", "text": "**Novelty**:", "label": null}
{"identifier": "--rcOeCKRh|||1|||19", "text": "The components are not novel, but the proposed learning paradigm is novel.", "label": null}
{"identifier": "--rcOeCKRh|||1|||20", "text": "**Significance of this work**:", "label": null}
{"identifier": "--rcOeCKRh|||1|||21", "text": "The impact of the work is significant, enabling a new way (CSOD) of transferring knowledge between classes, using weak labels.", "label": null}
{"identifier": "--rcOeCKRh|||1|||22", "text": "**Typos**:\n- \u201ccorresponds to the respective element of the matrix...\u201d the following formula is wrong (d should be replaced with r) \n- what does the upper index R mean and why is it useful?", "label": null}
{"identifier": "--rcOeCKRh|||1|||23", "text": "- matrices should be capital letters in bold\n- \u201cwhich is expresses\u201d", "label": null}
{"identifier": "--rcOeCKRh|||1|||24", "text": "While I have some doubts regarding the comparison with other works, I hope they will be clarified during the rebuttal.", "label": null}
{"identifier": "--rcOeCKRh|||1|||25", "text": "[A] High-Quality Proposals for Weakly Supervised Object Detection, Cheng, G., Yang, J., Gao, D., Guo, L., & Han, J..Transactions on Image Processing 2020", "label": null}
{"identifier": "--rcOeCKRh|||1|||26", "text": "[B] Instance-aware, Context-focused, and Memory-efficient Weakly Supervised Object Detection.", "label": null}
{"identifier": "--rcOeCKRh|||1|||27", "text": "Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Yong Jae Lee, Alexander G. Schwing, Jan Kautz.", "label": null}
{"identifier": "--rcOeCKRh|||1|||28", "text": "CVPR 2020", "label": null}
{"identifier": "--rcOeCKRh|||1|||29", "text": "[C] Mixed Supervised Object Detection with Robust Objectness Transfer, Yan Li, Junge Zhang, Kaiqi Huang, Jianguo Zhang.", "label": null}
{"identifier": "--rcOeCKRh|||1|||30", "text": "PAMI 2019", "label": null}
{"identifier": "--rcOeCKRh|||2|||0", "text": "Paper summary:", "label": null}
{"identifier": "--rcOeCKRh|||2|||1", "text": "The paper proposes a new task cross-supervised object detection, which trains object detectors on the combination of base class images with instance-level annotations and novel class image with only image-level annotations.", "label": null}
{"identifier": "--rcOeCKRh|||2|||2", "text": "A network with a recognition head which is trained by image-level annotations and a detection head which is trained by instance-level annotations is proposed for the task.", "label": null}
{"identifier": "--rcOeCKRh|||2|||3", "text": "To generate instance-level annotations for novel class images with only image-level annotations, the paper proposes a spatial correlation module to generate pseudo gt boxes from high-confidence boxes.", "label": null}
{"identifier": "--rcOeCKRh|||2|||4", "text": "Results on PASCAL VOC and COCO show that the proposed method obtains very promising object detection results for novel classes.", "label": null}
{"identifier": "--rcOeCKRh|||2|||5", "text": "Strengths:", "label": null}
{"identifier": "--rcOeCKRh|||2|||6", "text": "+ The paper is well written.", "label": null}
{"identifier": "--rcOeCKRh|||2|||7", "text": "+ Promising experimental results are obtained by the proposed method.", "label": null}
{"identifier": "--rcOeCKRh|||2|||8", "text": "+ The proposed spatial correlation module is interesting.", "label": null}
{"identifier": "--rcOeCKRh|||2|||9", "text": "Weaknesses:", "label": null}
{"identifier": "--rcOeCKRh|||2|||10", "text": "- Duplicate task settings.", "label": null}
{"identifier": "--rcOeCKRh|||2|||11", "text": "The proposed new task, cross-supervised object detection, is almost the same as the task defined in (Hoffman et al.", "label": null}
{"identifier": "--rcOeCKRh|||2|||12", "text": "2014, Tang et al. 2016, Uijlings et al. 2018).", "label": null}
{"identifier": "--rcOeCKRh|||2|||13", "text": "Both of these previous works study the task of training object detectors on the combination of base class images with instance-level annotations and novel class image with only image-level annotations.", "label": null}
{"identifier": "--rcOeCKRh|||2|||14", "text": "The work (Uijlings et al. 2018) also conducts experiments on COCO which contains multi-objects in images.", "label": null}
{"identifier": "--rcOeCKRh|||2|||15", "text": "In addition, the work  (Khandelwal et al. 2020) unifies the setting of training object detectors on the combination of fully-labeled data and weakly-labeled data, and conducts experiments on multi-object datasets PASCAL VOC and COCO.", "label": null}
{"identifier": "--rcOeCKRh|||2|||16", "text": "The task proposed by this paper could be treated as a special case of the task studied in (Khandelwal et al. 2020).", "label": null}
{"identifier": "--rcOeCKRh|||2|||17", "text": "We should avoid duplicate task settings.", "label": null}
{"identifier": "--rcOeCKRh|||2|||18", "text": "- Limited novelty.", "label": null}
{"identifier": "--rcOeCKRh|||2|||19", "text": "The novelty of the proposed method is limited.", "label": null}
{"identifier": "--rcOeCKRh|||2|||20", "text": "Combining recognition head and detection head is not new in weakly supervised object detection.", "label": null}
{"identifier": "--rcOeCKRh|||2|||21", "text": "The weakly supervised object detection networks (Yang et al. 2019, Zeng et al. 2019) also generate pseudo instance-level annotations from recognition head to train detection head (i.e., head with bounding box classification and regression) for weakly-labeled data.", "label": null}
{"identifier": "--rcOeCKRh|||2|||22", "text": "Review summary:", "label": null}
{"identifier": "--rcOeCKRh|||2|||23", "text": "In summary, I would like to give a rejection to this paper due to the duplicate task settings and limited novelty.", "label": null}
{"identifier": "--rcOeCKRh|||2|||24", "text": "Khandelwal et al., Weakly-supervised Any-shot Object Detection, 2020", "label": null}
{"identifier": "--rcOeCKRh|||2|||25", "text": "----------", "label": null}
{"identifier": "--rcOeCKRh|||2|||26", "text": "Post rebuttal ----------", "label": null}
{"identifier": "--rcOeCKRh|||2|||27", "text": "After discussions with authors and reading other reviews, I acknowledge the contribution that this paper advances the performance of cross-supervised object detection.", "label": null}
{"identifier": "--rcOeCKRh|||2|||28", "text": "However, I would like to keep my original reject score.", "label": null}
{"identifier": "--rcOeCKRh|||2|||29", "text": "The reasons are as follows.", "label": null}
{"identifier": "--rcOeCKRh|||2|||30", "text": "Extending datasets from PASCAL VOC to COCO is not a significant change comparing to previous tasks.", "label": null}
{"identifier": "--rcOeCKRh|||2|||31", "text": "The general object detection papers also evaluated on PASCAL VOC only about five years ago and now evaluate mainly on COCO.", "label": null}
{"identifier": "--rcOeCKRh|||2|||32", "text": "With the development of computer vision techniques, it is natural to try more challenging datasets.", "label": null}
{"identifier": "--rcOeCKRh|||2|||33", "text": "So although this paper claims that this paper focuses on more challenging datasets, there is no significant difference between the tasks studied in previous works like [a] and this paper.", "label": null}
{"identifier": "--rcOeCKRh|||2|||34", "text": "In addition, apart from ImageNet, the work [b] also evaluates their method on the Open Images dataset which is even larger and more challenging than COCO.", "label": null}
{"identifier": "--rcOeCKRh|||2|||35", "text": "The difference between the tasks studied in [b] and this paper is only that, [b] adds a constraint that weakly-labeled classes have semantic correlations with fully-labeled classes and this paper doesn't.", "label": null}
{"identifier": "--rcOeCKRh|||2|||36", "text": "This difference is also minor.", "label": null}
{"identifier": "--rcOeCKRh|||2|||37", "text": "Therefore, the task itself cannot be one of the main contributions of this paper (especially the most important contribution of this paper).", "label": null}
{"identifier": "--rcOeCKRh|||2|||38", "text": "I would like to suggest the authors change their title / introduction / main paper by 1) giving lower wights to the task parts 2) giving higher weights to intuitions of why previous works fail on challenging datasets like COCO and motivations of the proposed method.", "label": null}
{"identifier": "--rcOeCKRh|||2|||39", "text": "[a] YOLO9000: Better, Faster, Stronger, In CVPR, 2017", "label": null}
{"identifier": "--rcOeCKRh|||2|||40", "text": "[b] Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes, In ICCV, 2019", "label": null}
{"identifier": "--rcOeCKRh|||3|||0", "text": "This paper defines cross-supervised object detection which learns a detector from both image-level and instance-level annotations.", "label": null}
{"identifier": "--rcOeCKRh|||3|||1", "text": "It proposes a unified framework along with a spatial correlation module for the task.", "label": null}
{"identifier": "--rcOeCKRh|||3|||2", "text": "The spatial correlation module is used for transfer mapping information from base categories to novel categories.", "label": null}
{"identifier": "--rcOeCKRh|||3|||3", "text": "It conducts experiments on the PASCAL VOC dataset and COCO dataset, demonstrating the effectiveness.", "label": null}
{"identifier": "--rcOeCKRh|||3|||4", "text": "Pros:\n(1) The proposed spatial correlation module is a novel and effective transfer module.", "label": null}
{"identifier": "--rcOeCKRh|||3|||5", "text": "(2) The ablation studies are relatively complete.", "label": null}
{"identifier": "--rcOeCKRh|||3|||6", "text": "Cons:\n(1) The structure of the proposed spatial correlation module should be described in more detail.", "label": null}
{"identifier": "--rcOeCKRh|||3|||7", "text": "What is the meaning of \u201creplacing the backbone and feature pyramid network with five max-pooling layers.\u201d in the heatmap detection part?", "label": null}
{"identifier": "--rcOeCKRh|||3|||8", "text": "(2) In Table 1, are the experimental settings of those competitors such as MSD-VGG16, MSD-Ens, and Weight Transfer et al. exactly the same as those used in this paper?", "label": null}
{"identifier": "--rcOeCKRh|||3|||9", "text": "(3) In Table 2, it seems like the method taking the non-VOC as the base classes while Hu et al. (2018) use the non-VOC as those classes without mask annotation.", "label": null}
{"identifier": "--rcOeCKRh|||3|||10", "text": "Can you tell me the reasons for this choice?", "label": null}
{"identifier": "--rcOeCKRh|||3|||11", "text": "(4) Some similar problem settings are defined in [a] and [b].", "label": null}
{"identifier": "--rcOeCKRh|||3|||12", "text": "The paper fails to compare the problem settings and justify the usefulness of the proposed problem setting in real application scenarios.", "label": null}
{"identifier": "--rcOeCKRh|||3|||13", "text": "Overall evaluation:", "label": null}
{"identifier": "--rcOeCKRh|||3|||14", "text": "The major contribution of this paper comes from the spatial correlation module.", "label": null}
{"identifier": "--rcOeCKRh|||3|||15", "text": "However, I still have some doubts about the structure of this module.", "label": null}
{"identifier": "--rcOeCKRh|||3|||16", "text": "Since this task is first presented, I want to make sure that the comparison is as fair as possible.", "label": null}
{"identifier": "--rcOeCKRh|||3|||17", "text": "[a]", "label": null}
{"identifier": "--rcOeCKRh|||3|||18", "text": "Weakly- and Semi-Supervised Fast Region-Based CNN for Object Detection.", "label": null}
{"identifier": "--rcOeCKRh|||3|||19", "text": "Journal of Computer Science and Technology (JCST) 34(6): 1269\u20131278 Nov. 2019.\n[b]", "label": null}
{"identifier": "--rcOeCKRh|||3|||20", "text": "LSTD: A Low-Shot Transfer Detector for Object Detection, AAAI 2018", "label": null}
{"identifier": "-2FCwDKRREu|||0|||0", "text": "# Paper Summary", "label": null}
{"identifier": "-2FCwDKRREu|||0|||1", "text": "The paper presents a new method for embedding visual images into a state space suitable for effective control by an actor-critic style RL algorithm.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||2", "text": "They show how a previously explored idea of using a bisimulation between state abstractions and reward sequences to group states that are similar from a decision theoretic perspective can be extended to a continuous deep embedded representation using twin network style learning through standard gradient descent optimization.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||3", "text": "They call the approach Deep Bisimulation for Control (DBC).", "label": null}
{"identifier": "-2FCwDKRREu|||0|||4", "text": "The paper also argues for the correctness of their approach using a contraction proof and a theoretical argument for generalization to new problem domains.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||5", "text": "The approach contrasts directly with algorithms that use an autoencoder to find a compact representation by reconstructing input frames or predicting future input frames.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||6", "text": "These approaches necessarily represent enough information to reconstruct both task relevant and incidental details.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||7", "text": "Experimentally, the paper shows that performance of reconstruction-based approaches degrades by a large and significant amount when extraneous background detail is present in image frames, while the proposed method is immune.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||8", "text": "The evaluation is done on widely respected benchmark of Deep Mind MuJoCo based articulated figure simulations and a CARLA realistic image simulation of car driving which shows that focusing on task specific detail is important on realistic tasks.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||9", "text": "# Pros and Cons", "label": null}
{"identifier": "-2FCwDKRREu|||0|||10", "text": "The work addresses a key problem in reinforcement learning, which is learning effective policies from unlabeled visual images.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||11", "text": "The ability to find compact, task relevant representations of high-dimensional inputs is central to the ICLR community.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||12", "text": "The paper covers relevant background in state abstraction in RL and it is clear how it relates to the paper's contribution.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||13", "text": "The paper clearly explains the background of bisimulation and how the idea of grouping states according to their probable reward sequences can be practically realized by grouping states by their immediate reward and future state distributions.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||14", "text": "It is also clear in principle that a twin network can be used to induce a latent space with these properties.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||15", "text": "The paper shows that the policy will converge and that the error will be bounded and that the representation will generalize to any problem domain in which causal dependencies are a subset of the problem domain the representation was trained on.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||16", "text": "Which is nice.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||17", "text": "There is a nice evaluation on both MuJoCo articulated figure problems from the Deep Mind Control suite as well as the CARLA simulated driving application that uses large realistically rendered images.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||18", "text": "Comparison against state-of-the-art RL algorthims makes results convincing.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||19", "text": "I think the CARLA example is nice as it shows that practical problems without artificial augmentations have the property that the input details can be distracting if fully reconstructed.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||20", "text": "The superiority of DBC in this case really makes this point well.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||21", "text": "The paper does not report computational load associated with their approach.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||22", "text": "Presumably due to the closed form Wasserstein approximations, we are probably looking at only a fractional increase in time for the encoder network above the policy, dynamics and reward models.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||23", "text": "So roughly 30% more??", "label": null}
{"identifier": "-2FCwDKRREu|||0|||24", "text": "Key figures for central results are too small to get meaningful interpretations unless enlarged by a factor of 4 or 5.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||25", "text": "Somehow this seems to go against the spirit of page limits to me.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||26", "text": "Figure 6 and the description don't seem to be aligned, but I can imagine that it can be readily fixed.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||27", "text": "# Recommendation", "label": null}
{"identifier": "-2FCwDKRREu|||0|||28", "text": "I recommend acceptance.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||29", "text": "The paper shows how to extend bisimulation principle for grouping states to continuous deep actor critic methods and provides convincing evidence on standard benchmarks that it is effective in extracting a task-relevant abstraction that is robust to noise in observations and focuses on task specific detail.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||30", "text": "# Questions", "label": null}
{"identifier": "-2FCwDKRREu|||0|||31", "text": "Figure 6: it seems that the \u201csimple_distractors\u201d environment is different than \u201csetting 2\u201d natural video setting.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||32", "text": "This caption could use some reworking to get parallelism clear.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||33", "text": "The \u2018ideal gas\u2019 is the same as simple_distractors?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||34", "text": "Also the graphs seems to be about different experimental types.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||35", "text": "Does the first experiment also use frozen encoder?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||36", "text": "The graphs do not seem to relate to the text which talks about walker_stand, walker_run reward functions.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||37", "text": "I am confused.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||38", "text": "Can the paper have any insight into the relative performance of their algorithm versus benchmarks algorithms across different tasks (finger spin, cheetah, walker) given that they are all stick figures?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||39", "text": "# Feedback", "label": null}
{"identifier": "-2FCwDKRREu|||0|||40", "text": "I was able to work through Definition 1 and assure myself it made sense.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||41", "text": "In the end I made a small picture that helped.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||42", "text": "There seems to be some ambiguity in notation between observations, underlying state and latent variable spaces.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||43", "text": "For instance, in section 3, script S is defined as a state space.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||44", "text": "In section 4, the function d is defined on script S x script S which is described as an observation space.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||45", "text": "Admittedly, the work seems to be situated in an approximately fully observable world in which states and observations are somewhat equivalent.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||46", "text": "I suspect that is why the 5-camera 300-degree suround view was necessary in the CARLA experiments.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||47", "text": "Interesting that the paper employ stop gradients on the latent representation terms when they appear in the reward and Wasserstein terms in the loss function.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||48", "text": "This is to enforce the separateness of the optmizations in algorithm 1?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||49", "text": "Definition 2, the bisimulation metric contains a max \u2026 so this is worst case discrepancy between the futures between state space actions and empirical actions.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||50", "text": "Was average discrepancy considered?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||51", "text": "This might be relevant later in discussion about intractability in previous approaches (section 4 paragraph 3).", "label": null}
{"identifier": "-2FCwDKRREu|||0|||52", "text": "Equation 4 specifies an L1 metric for the distance in abstraction space ||z_i \u2013 Z_j ||.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||53", "text": "Is there a motivation for this choice?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||54", "text": "Again trying to bound the worst error?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||55", "text": "Theorem 4 references", "label": null}
{"identifier": "-2FCwDKRREu|||0|||56", "text": "Theorem 5 which is not in the main text.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||57", "text": "In section 5, ideally epsilon would be briefly described before it appears in a bound.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||58", "text": "Figure 3 is very small... the labels \u2013 particularly the subscripts are unreadable.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||59", "text": "It may not be making a point important enough to include it.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||60", "text": "The causal variables section could be shorted in general.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||61", "text": "It is a good point but not completely surprising.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||62", "text": "Figure 4 is a key figure to support the paper's hypothesis that deep bisimulation is effective for state abstraction in noisy images.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||63", "text": "This figure really needs to be bigger.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||64", "text": "In particular, I had to strain to read the legends to understand if the axes were different between the uncluttered video of articulated figures and the cluttered video of articulated figures with a background movie in them.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||65", "text": "It was also hard to make out which line was which.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||66", "text": "In particular, the \u201ccheetah\u201d column does not show DBC improving on cluttered video scenario --- it tops out at around 250 in both top and bottom graphs, but the scales are very different.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||67", "text": "It confuses the message a bit.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||68", "text": "There is a statement \u201ca single loss function would be less stable and require balancing the components\u201d.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||69", "text": "Is this speculation or based on experience?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||70", "text": "Separate optimizations implicitly define a balance between these terms wouldn\u2019t they?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||71", "text": "Namely equal balance?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||72", "text": "To some degree, figure 9 is making the same argument as figure 5.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||73", "text": "Could leave this out if you were tight for space.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||74", "text": "Figure 5: What is the first column of figures to the left?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||75", "text": "It doesn\u2019t seem to be relevant?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||76", "text": "Otherwise, I think the figure is very effective in conveying the structured embedding space does a better job of grouping similar states.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||77", "text": "It is also very small.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||78", "text": "Drawing a white border between the figure pairs would underscore visually that there are two states in each \u201cimage\u201d", "label": null}
{"identifier": "-2FCwDKRREu|||0|||79", "text": "Figure 9: I could not make sense of the images on the sides of the figure.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||80", "text": "Particularly, the images on the left side.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||81", "text": "They seemed to be abstract geometric shapes and I could not get an intuition about what the driving scenario was.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||82", "text": "Section 6.4 \u201creward highway progression an penalizes collisions\u201d  an => and", "label": null}
{"identifier": "-2FCwDKRREu|||0|||83", "text": "Future work \u2013 another likely avenue for future work would be to introduce some sort of memory to handle partially observable worlds.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||84", "text": "For instance, can the agent drive a car with only a forward view if given memory?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||85", "text": "Does this break down if it does not have memory?", "label": null}
{"identifier": "-2FCwDKRREu|||0|||86", "text": "This could either be an explicit neural memory or implicit memory such as an LSTM \u2026  Estimating uncertainty could also be important to produce agents that can work in the real world and assess when they know what they are doing and when they do not.", "label": null}
{"identifier": "-2FCwDKRREu|||0|||87", "text": "Another future work area is in modeling of transition distributions as something more complex than Gaussians \u2026 what if there are distinct possible futures that are equally valid: it is ok for the robot to turn left or right as long as it avoids the object straight ahead?", "label": null}
{"identifier": "-2FCwDKRREu|||1|||0", "text": "**Summary**", "label": null}
{"identifier": "-2FCwDKRREu|||1|||1", "text": "The paper focuses on how learning state-representations that encode information relevant to the task can improve reinforcement learning from pixels.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||2", "text": "Often, observations in an MDP can contain information that are irrelevant (\u201cdistractors\u201d) to the task at hand and can likely \u201cdistract\u201d the downstream RL algorithm used.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||3", "text": "Unlike existing reconstruction based approaches (which don\u2019t explicitly incentivize ignoring task-irrelevant information), the authors propose Deep Bisimulation Control (DBC) that relies on bi-simulation metrics (as the task-aware criterion) that encode behavioral similarity b/w states with respect to the reward structure.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||4", "text": "Instead of explicitly learning a bi-similarity distance function, authors enforce the representations which under L1 distances correspond to bi-simulation metrics.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||5", "text": "DBC demonstrates learning these representations in conjunction with the control policy, reward and a dynamics model.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||6", "text": "Furthermore, the authors highlight connections to causal inference which can hopefully further provide insights into which \u201cnew\u201d reward structures can the learned representations generalize to (since bi-similarity metrics themselves are heavily dependent on the reward structure).", "label": null}
{"identifier": "-2FCwDKRREu|||1|||7", "text": "Results obtained by the authors demonstrate that DBC can learn task specific representations and a control policy in a robust manner in the presence of distractors on the Deepmind Control Suite and the CARLA simulator.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||8", "text": "Additionally, the authors also demonstrate how DBC can generalize to new reward functions on Mujoco.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||9", "text": "**Strengths**", "label": null}
{"identifier": "-2FCwDKRREu|||1|||10", "text": "- The paper is generally well-written and easy to follow for the most part.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||11", "text": "The authors do a good job of walking the reader through the preliminaries, highlighting distinctions with prior usage of bisimilarity metrics and how DBC ties in with slight modifications to the Soft Actor-Critic algorithm.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||12", "text": "-", "label": null}
{"identifier": "-2FCwDKRREu|||1|||13", "text": "The results obtained on DM control suite demonstrate that while DBC is competitive or slightly worse compared to other approaches in the absence of distractors, it performs better compared the set of baselines when distractors are introduced \u2014 significantly outperforming the reconstruction and the contrastive approaches.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||14", "text": "Furthermore, compared to a VAE, DBC places observations that are similar in terms of task information closer in the learnt embedding space.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||15", "text": "-", "label": null}
{"identifier": "-2FCwDKRREu|||1|||16", "text": "The authors also show that compared to an adaptation of prior usage of bisimulation metrics, DBC is more sample efficient.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||17", "text": "Additionally, within the considered family of reward functions to generalize to \u201cwalker_run\u201d and \u201cwalker_stand\u201d, DBC shows stronger generalization compared to DeepMDP.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||18", "text": "Furthermore, learned embeddings on CARLA demonstrate qualitatively that DBC learns to group behaviorally similar distractor states (manifesting as obstacles).", "label": null}
{"identifier": "-2FCwDKRREu|||1|||19", "text": "**Weaknesses**\nI don\u2019t have any major weaknesses to point out.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||20", "text": "I will highlight minor comments / weaknesses which, I think if addressed, would definitely make the paper stronger.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||21", "text": "-", "label": null}
{"identifier": "-2FCwDKRREu|||1|||22", "text": "While the authors state that DBC in practice can be combined with any model-free of model-based algorithm, the paper would definitely benefit if this claim was backed up with results demonstrating DBC combined with another model-free or model-based algorithm since it might be unclear off-the-shelf if an algorithm requires major or minor changes to work in conjunction with DBC.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||23", "text": "- Task generalization (or generalizing over different reward functions) relies on the assumption that the new reward function depends on contributing factors that likely influence the earlier \u201cseen\u201d reward function as well.", "label": null}
{"identifier": "-2FCwDKRREu|||1|||24", "text": "While empirically validating this claim from walker_walk -> walker_run / walker_stand is a reasonable starting point, demonstrating another instance where inferring the source and target reward functions is not immediately obvious would definitely benefit the paper.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||0", "text": "The authors propose an approach to robust representation learning of observations for reinforcement learning by training a model to align the euclidean distance between two observations with bisimulation metrics that quantify how similar the states that generated the observations are in terms of the control problem.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||1", "text": "This reduces the effect of irrelevant features in the observations on the representations.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||2", "text": "The paper is well written, the problem is clearly motivated and the approach and technical contribution is easy to follow.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||3", "text": "The approach to use the state bisimulation metric to supervise observation representation is intuitive and clearly motivated.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||4", "text": "Theoretical analysis is provided with generalization guarantees.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||5", "text": "\"As an example, in the context of autonomous driving, an intervention can be a change in weather, or a\nchange from day to night which affects the observation space but not the dynamics or reward.\"", "label": null}
{"identifier": "-2FCwDKRREu|||2|||6", "text": "I do not agree with the example as weather can directly alter the dynamics and desired behavior of an AV system.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||7", "text": "The point in this paragraph is still clear but I would suggest a different example.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||8", "text": "The evaluations are strong and run on a number of different experiment settings against multiple strong SOTA models.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||9", "text": "In Figure 4 the proposed approach is outperformed by contrastive learning in the default setting.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||10", "text": "I understand that the goal is to learn robust representations for the natural setting, but can the authors comment on why it fails to beat the contrastive approach here and provide insight on how this may be addressed.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||11", "text": "Some problems have only a few distractors and may fall between the natural and the default setting.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||12", "text": "recommendation and reasoning", "label": null}
{"identifier": "-2FCwDKRREu|||2|||13", "text": "The paper is well written and presents a clear approach to a well motivated problem with strong evaluation results.", "label": null}
{"identifier": "-2FCwDKRREu|||2|||14", "text": "I recommend acceptance.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||0", "text": "This paper proposes to represent system logs at five levels of abstraction (including log sequence, parsed log, field embedding, log embedding, and sequence embedding).", "label": null}
{"identifier": "-5VpoDCExrU|||0|||1", "text": "The representation at each level can be computed from the previous level.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||2", "text": "Transformer Network is utilized for time encoding.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||3", "text": "The paper also describes various log-related applications based on the proposed log representation.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||4", "text": "Some experiments were conducted to evaluate the proposed approach.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||5", "text": "Logs are useful for understanding and diagnosing software intensive systems.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||6", "text": "It is good to see that this paper proposes a new neural representation of log data.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||7", "text": "The authors also suggested various applications of the proposed log representation.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||8", "text": "In section 4, the authors only described the proposed time encoding technique, while other parts of the log representation (such as encoding a log entry) were not described.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||9", "text": "Also, it is not clear if the proposed time encoding technique is better than the related methods (there are many related methods for encoding/representing time).", "label": null}
{"identifier": "-5VpoDCExrU|||0|||10", "text": "The evaluation of the proposed approach is very weak.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||11", "text": "In section 5, the authors mentioned the Radio datasets.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||12", "text": "However, the use of Radio dataset is not described.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||13", "text": "The authors only evaluated the anomaly detection model on the HDFS log dataset, which is not enough.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||14", "text": "Also, the obtained results on HDFS were not very different from the results of the related work (DeepLog).", "label": null}
{"identifier": "-5VpoDCExrU|||0|||15", "text": "Furthermore, no experiments were conducted for the causal analysis task.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||16", "text": "Therefore, the effectiveness and generalizability of the proposed approach are not clear.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||17", "text": "The paper only compared with a few related methods for log-related tasks.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||18", "text": "Actually, this area has been widely studied and there are a lot more research work (some also utilized deep learning and language models).", "label": null}
{"identifier": "-5VpoDCExrU|||0|||19", "text": "The authors could discuss and compare with them.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||20", "text": "Just a few examples:\nZhang et al., Robust log-based anomaly detection on unstable log data.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||21", "text": "In Proc. ESEC/FSE 2019, 807-817.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||22", "text": "Zhu et al., Learning to log: Helping developers make informed logging decisions, in Proc. ICSE 2015.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||23", "text": "pp. 415\u2013425.\nP.", "label": null}
{"identifier": "-5VpoDCExrU|||0|||24", "text": "He et al., \u201cCharacterizing the natural language descriptions in software logging statements,\u201d  in Proc. ASE 2018, pp. 178\u2013189.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||0", "text": "The paper deals with log data representation and analysis.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||1", "text": "Logs are important to understand the status of a system and do various root cause analysis.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||2", "text": "This paper proposes a transformer based approach to obtain vector representation of log data, in various levels such as for key-value pair with a log entry, a log entry within a sequence of logs and various blocks of log sequences.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||3", "text": "The usefulness of log embeddings are shown on multiple log downstream tasks.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||4", "text": "The topic of the paper is very interesting and relatively less studied in machine learning domain, though important for various applications.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||5", "text": "However, it has several drawbacks, as follows.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||6", "text": "1. The technical contribution of the paper is very limited.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||7", "text": "The transformer based approach is not novel.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||8", "text": "The specific format of time encoding in Section 4.1 is not well motivated.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||9", "text": "2. In Section 3, is it reasonable to consider a log entry as a sequence of characters, instead of sequence of words?", "label": null}
{"identifier": "-5VpoDCExrU|||1|||10", "text": "Most of the log entries consist of a set of key words (presented in a readable format) and some arguments (can be numeric).", "label": null}
{"identifier": "-5VpoDCExrU|||1|||11", "text": "3. One key difficulty in handling log data is the presence of large number of arguments (or parameters) within each log entry.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||12", "text": "Thus, it becomes difficult to mine logs to different templates.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||13", "text": "The exact parameter values can be very different in different logs.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||14", "text": "So, applying text modeling approaches directly on log data may not be the best approach.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||15", "text": "4. In Section 3, What is the granularity used for sequence embedding in logs?", "label": null}
{"identifier": "-5VpoDCExrU|||1|||16", "text": "Is it for a whole log file, or a block of logs?", "label": null}
{"identifier": "-5VpoDCExrU|||1|||17", "text": "How to determine the appropriate granularity for some downstream application?", "label": null}
{"identifier": "-5VpoDCExrU|||1|||18", "text": "5. Fir unsupervised anomaly detection, it is not clear why anomalous points would fall into a small cluster.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||19", "text": "Rather, they would probably be distributed over multiple clusters, but still will be far away from the respective cluster center.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||20", "text": "The paper could have also used some standard anomaly detection algorithm such as Isolation Forest once the vector embeddings are generated.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||21", "text": "6. For supervised anomaly detection, what is the % of anomalous logs used in the training?", "label": null}
{"identifier": "-5VpoDCExrU|||1|||22", "text": "7. Being an application-oriented paper, more importance should be given on hyperparameter setup and tuning for all the experiments.", "label": null}
{"identifier": "-5VpoDCExrU|||1|||23", "text": "The lack of availability of source code also makes the reproducibility of the results difficult.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||0", "text": "**Summary**", "label": null}
{"identifier": "-5VpoDCExrU|||2|||1", "text": "Logs are widely used in computer systems to record their events.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||2", "text": "The recorded logs can be applied to a wide variety of diagnostic applications such as anomaly detection, root cause analysis, and causal analysis.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||3", "text": "This paper proposes levels of abstraction for log representation.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||4", "text": "There are in total 5 levels of abstraction, which are log sequences, parsed logs, field embeddings, log embeddings, and sequence embeddings.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||5", "text": "Each of the aforementioned levels can be derived from its proceeding levels.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||6", "text": "The paper uses the transformer model to generate a log representation.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||7", "text": "Moreover, the authors propose a time encoding method and add the encoding time to the transformer to improve the representation ability of the proposed log representation method.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||8", "text": "In the evaluation section, the authors apply the generated log representation to various downstream applications to test the effectiveness of the proposed method.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||9", "text": "**Strengths**:\n1. The idea of using different levels of abstraction to generate log embedding is interesting.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||10", "text": "2. The paper conducts a thorough experiment over various downstream tasks based on the learned log representation.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||11", "text": "**Weaknesses**:\n1. The motivation for adding time encoding to the transformer is not clear.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||12", "text": "It would be better if the intuition behind this variation could be discussed rather than simply using the result of the ablation study.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||13", "text": "2. It seems that the paper just applies the Transformer model to the log representation task and there is no comparison between the proposed method and other log representation methods in the evaluation part.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||14", "text": "In the related work section, the authors mentioned a recently proposed log embedding work (Logsy).", "label": null}
{"identifier": "-5VpoDCExrU|||2|||15", "text": "It would be better if there is a comparison between the two different methods in the evaluation section.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||16", "text": "3. There is little discussion about the results of the experiments.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||17", "text": "It would be better if some discussion of why the use of the proposed representation can or cannot improve the performance could be mentioned at the end of each experiment.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||18", "text": "**Minor Weaknesses**:", "label": null}
{"identifier": "-5VpoDCExrU|||2|||19", "text": "The presentation of the paper could be improved.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||20", "text": "1. The figures for the experiment results are too far away from the section that describes and discusses the experiment.", "label": null}
{"identifier": "-5VpoDCExrU|||2|||21", "text": "2. The \u201coh\u201d in the caption of figure 3 should be \u201con\u201d.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||0", "text": "This paper proposes a multi-level abstraction for representing logs that appear in a wide range of application domains and a Transformer-based model for embedding those representations in a vector space.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||1", "text": "The authors show that a variety of log processing tasks (anomaly detection, predictive analysis, search, etc.) can be implemented on top of this foundation along with empirical results for some of them based on two real-world log datasets.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||2", "text": "Strong points:\n-", "label": null}
{"identifier": "-5VpoDCExrU|||3|||3", "text": "The paper is well written.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||4", "text": "It nicely educates the reader about the prior state of the art, motivates how the contributions of the work fit in, and then presents those contributions in a systematic manner with empirical evidence where applicable.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||5", "text": "- Log processing is a widespread application with increasingly large and diverse datasets and need for a range of automated data analysis tasks over them.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||6", "text": "The multi-level abstraction to provide a common representation for this domain as a whole is a neat idea that can help organize existing knowledge into a common framework as well as facilitating future innovation based on that framework.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||7", "text": "-", "label": null}
{"identifier": "-5VpoDCExrU|||3|||8", "text": "The paper shows how transformer networks can be extended with the notion of time, which can be useful for time-oriented data in general, including logs.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||9", "text": "-", "label": null}
{"identifier": "-5VpoDCExrU|||3|||10", "text": "The proposed models are implemented in practice as part of a transformer library in PyTorch and are applied to a variety of log processing tasks with good results.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||11", "text": "Weak points:\n-", "label": null}
{"identifier": "-5VpoDCExrU|||3|||12", "text": "The levels of abstraction could have been connected more clearly with the transformer model and the log processing applications in the experimental part.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||13", "text": "The mapping between these parts is not as easy to understand as it could have been based on the current writing.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||14", "text": "-", "label": null}
{"identifier": "-5VpoDCExrU|||3|||15", "text": "It would be good to add a figure to Section 3 to illustrate the various abstraction levels and how they are related (e.g., in the form of a pipeline together with an example log entry representation for each).", "label": null}
{"identifier": "-5VpoDCExrU|||3|||16", "text": "Additional comments:\n- Releasing the telecommunications dataset would be a good contribution to the research community.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||17", "text": "- Section 4.1, \"Given log sequence <(l_1, t_1), ..\": Please make sure to use consistent terminology.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||18", "text": "\"log sequence\" is defined slightly differently in Section 3.\n- Figure 4: In the caption, it looks like \"Middle\" and \"Right\" should be in the opposite order.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||19", "text": "Also, the colors in the third graph are not easily readable.", "label": null}
{"identifier": "-5VpoDCExrU|||3|||20", "text": "- Typos:\nlearnt -> learned\nanomaies -> anomalies\nuseful applications -> useful (for?) applications\noh HDFS dataset -> on HDFS dataset\npytorch -> PyTorch\nfigure x ->", "label": null}
{"identifier": "-5VpoDCExrU|||3|||21", "text": "Figure x", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||0", "text": "This paper develops EXP4-style algorithms for Gaussian bandit and RL, proving upper and lower bounds and give empirical evaluations.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||1", "text": "I have a few questions:", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||2", "text": "1. I think MAB with unbounded loss is a solved problem?", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||3", "text": "At least these regret bound should be known.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||4", "text": "For example , see [Hannan consistency in on-line learning in case of unbounded losses under partial monitoring]", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||5", "text": "2. The RL part is even more confusing.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||6", "text": "Say we have n experts:\n(1) when n is big, then we need to maintain n DQN simultaneously, which sounds unrealistic.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||7", "text": "(2) when n is small, as in this paper, a good ensemble can only guarantee you get to match the better one of the two experts.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||8", "text": "However, a simple way to do that is to train them and evaluate the policies separately.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||9", "text": "A lot of technicalities can be avoided this way.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||10", "text": "So why bother using EXP4?", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||11", "text": "3. Actually many existing algorithms, at least in tabular RL, are inspired by EXP4.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||12", "text": "Indeed, consider the policy gradient method on a MAB, then you get an EXP-style algorithm (of course this also depends on the feedback model).", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||13", "text": "See [On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift] for example.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||14", "text": "More sophisticated ways like the Nash V-learning algorithm in [ Near-Optimal Reinforcement Learning with Self-Play], could be applied to solve competitive RL problems.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||15", "text": "Of course, these papers are for tabular RL, which is too simplified.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||16", "text": "How to use function approximation with EXP4 is an interesting yet challenging question.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||17", "text": "I think the main weakness of the approaches in this paper is that, it is not clear why exploration with EXP4 is more desirable and why a naive combination with DQN should work.", "label": null}
{"identifier": "-5W5OBfFlwX|||0|||18", "text": "Indeed, I think this comination can be very data inefficient.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||0", "text": "This paper contributes to the study of EXP-based algorithms in two aspects.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||1", "text": "One is on the theoretical aspect:", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||2", "text": "It analyzes the lower and upper bounds of EXP-3 for Gaussian multi-bandit setting for which the reward can be unbounded.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||3", "text": "The other is on the empirical aspect:", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||4", "text": "It applied EXP4, originally developed for MAB, to Rl applications and demonstrated its advanced empirical performance.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||5", "text": "This paper is overall clearly written.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||6", "text": "However, my general feeling is EXP3 part and EXP4 part are quite separated and have different flavors.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||7", "text": "Other than both are algorithms in the EXP family, I did not find much connection between them.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||8", "text": "For the EXP3P part, I have difficulty to appreciate the value of a linear-in-T lower bound for small T. (And this small T range further depends on bandit setting such as \\mu, q).", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||9", "text": "In fact, the lower bound has in Theorem also involves \\epsilon, which together with T are restricted by an inequality.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||10", "text": "If I further manipulate the lower bound expression and the required T range, I think it is not hard to obtain quadric or cubic in T lower bound.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||11", "text": "(One extremal but obviously non-interesting lower bound is  constant*T^n for any  with T<=1).", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||12", "text": "So I really want to understand why a bound for small T is useful?", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||13", "text": "Even if it is useful, why the order regarding T matters?", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||14", "text": "The lower bound part also has some basic conceptual mistakes.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||15", "text": "When you study a lower bound, please use \\Omega() or \\Theta() instead of O().", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||16", "text": "You can say your regret has a lower bound given by \\Theta() or simply say your regret = (or \\in) \\Omega().", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||17", "text": "But don't say your regret has a lower bound O(), which means nothing.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||18", "text": "(Technically, T^2 has a lower bound O(T^3) is still correct because T = (or \\in) O(T^3).", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||19", "text": "Further, big O notation is asymptotical notation, i.e., meaningful only for large T. I don't think you can use O(T) to represent a lower bound for small T.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||20", "text": "The experiments only compare Alg 2 with RND.", "label": null}
{"identifier": "-5W5OBfFlwX|||1|||21", "text": "What if you consider a baseline that involve multiple different DQNs and randomizes/explores among them?", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||0", "text": "The authors consider analyzing the EXP3.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||1", "text": "P algorithm for the case of unbounded reward functions, in the sense that the rewards are governed by a Gaussian distribution.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||2", "text": "The authors first demonstrate a regret lower bound result on the Gaussian MABs when the time horizon is bounded from above.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||3", "text": "Then, the authors proceed to the analysis of the EXP3.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||4", "text": "P algorithm on the Gaussian MABs, and establish a regret bound similar to that of Auer et al.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||5", "text": "2002.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||6", "text": "Finally, the authors apply the EXP3.P, where an expert corresponds to a Q-learning network, in the EXP4-RL algorithm, and evaluate it on multiple RL instances.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||7", "text": "Major comments:", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||8", "text": "The major technical contributions seem to be the regret bound for EXP3.P for the case of Gaussian MAB, as stated in Theorem 4.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||9", "text": "Based on the authors' notation in the first paragraph of page 3, the Gaussian reward distribution is stationary across time.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||10", "text": "This contribution appears marginal, since the Theorem appears to be a straightforward consequence of the EXP3.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||11", "text": "P regret by Auer et al.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||12", "text": "2002 by conditioning on all realized rewards to lie in [-\\Delta, \\Delta].", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||13", "text": "The technical part on how to identify the best expert is already dealt with by the analysis in Auer et al.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||14", "text": "2002.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||15", "text": "Another contributions are Theorems 1-3, which are regret lower bounds for Gaussian MABs.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||16", "text": "I am not sure how to interpret these regret lower bounds, since they require the horizon length to be bounded from above.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||17", "text": "More precisely, the authors show that for any algorithm, there exists a Gaussian MAB instance such that $\\text{", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||18", "text": "Reg}(T) \\geq c T$ when $T\\leq C$, where $c, C$ are instance-dependent  constants.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||19", "text": "While this bound is a mathematically sound statement, it does not imply anything about the difficulty of the underlying problem when T is large.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||20", "text": "For example, for regret upper bound of an MAB algorithm, one almost always establishes a guarantee of the form $\\text{", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||21", "text": "Reg}(T) \\leq \\text{Bound}(T)$ for all $T \\geq C'$, where $C'$ an instance dependent constant.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||22", "text": "I am not too sure what is the message the authors are trying to convey here, since we know that the state-of-the-art regret lower bound is $\\Omega(\\sqrt{KT})$ for sufficiently large T.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||23", "text": "Finally, if I understand the underlying motivation of the authors correctly, the ultimate problem that the authors are trying to address seems to be a stochastic best arm identification problem with (sub-)Gaussian rewards, where an arm here corresponds to a Q-network.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||24", "text": "I am not sure why the authors resort to EXP type algorithms for a stochastic problem.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||25", "text": "Minor Comments:", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||26", "text": "I believe that the inequality w RL(T) \u2265 O(\u221aT \u00b7 \u03b3 ) on page 4 should be \\leq.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||27", "text": "In Algorithm 1, in the initialization, w_i should be replaced by w_i(1).", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||28", "text": "In Algorithm 2, it requires to compute  y_k = E[\u02c6x_{kj} ], and the authors should elaborate on how the expectation is computed.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||29", "text": "In general, there are quite a few typos, and some parts of the writing are  a bit ambiguous in the way they are phrased.", "label": null}
{"identifier": "-5W5OBfFlwX|||2|||30", "text": "I advise the authors to proofread and also polish the writing.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||0", "text": "Summary:", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||1", "text": "This paper proposes a new algorithm called EGRL to improve computation graph running time by optimizing placement of the graph's components on memory.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||2", "text": "Specifically, the authors demonstrate the algorithm on the Intel Neural Networks Processor for Inference (NNP-I), which allows them to map neural network components on one of three memory hierarchies, each with different tradeoffs.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||3", "text": "The authors demonstrate that this technique provides speedups on BERT, Resnet-50 and Resnet-101.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||4", "text": "Pros:", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||5", "text": "- Some past papers (for eg. [1]) in this domain evaluate their work in simulators instead of real hardware, and often, the simulators make assumptions that are not realisitc.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||6", "text": "The paper tests its technique on actual hardware, and this is definitely a plus.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||7", "text": "-", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||8", "text": "The authors promise that they will open-source their code.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||9", "text": "This is important since many of the efforts in this domain remain fragemented and difficult to reproduce.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||10", "text": "This is primarily due to the lack of open source code or a standard benchmark.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||11", "text": "-", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||12", "text": "The paper is well written.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||13", "text": "-", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||14", "text": "The visualisations of the learned policy vs the baseline in Figure 6 are quite good.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||15", "text": "- EGRL directly builds upon CERL so it is not very novel, but it has not been applied before to this domain.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||16", "text": "Cons:", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||17", "text": "- The paper evaluates the technique on just 3 workloads.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||18", "text": "This is in contrast to [1] who evaluate on 372 different workloads and [2] who evaluate on 96 synthetic graphs. [3] and [4] also evaluate on a very small number of workloads, but I believe they probably got a freepass since they were the earliest works in their domain.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||19", "text": "-", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||20", "text": "The baseline that the experiments are being compared against might be weak - in figure 3, it looks the policy from both EGRL and EA in iteration 0 itself beat the baseline for Resnet-101 and BERT! \n-", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||21", "text": "How long does it take to perform one iteration?", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||22", "text": "And how long does it take to train the policy?", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||23", "text": "This would be useful to get an idea of how EGRL fairs against [3] and [4] which also trained on real hardware and took many hours to finish training the policy.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||24", "text": "-", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||25", "text": "The demonstration of generalizability is insufficient - it is difficult to conclude that EGRL can generalize to other workloads.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||26", "text": "For eg. in Figure 4 (left), the policy performs worse than the baseline for Resnet-50.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||27", "text": "Moreover, two of the workloads are from the Resnet family.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||28", "text": "-", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||29", "text": "If it takes a long time to train each policy and if the model also shows poor zero-shot generalizability, it makes me question if this approach is practical for a compiler setting where a user would typically want the compilation to be completed quickly.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||30", "text": "Overall:", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||31", "text": "I felt that the paper has some interesting ideas but needs more experiments.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||32", "text": "Questions and Clarifications:", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||33", "text": "- I believe that the related work section should add a clarification - [1], [2], [3] and [4] primarily deal with device placement, i.e., placing components of computation graph on different CPUs/GPUs to optimize run time via better parallelization.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||34", "text": "While this work is concerned with mapping components to different memory hierarchies on the same device.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||35", "text": "- While EGRL's action space is larger than [5], the action space in [1] is much larger - for a graph with 2000 nodes to be placed on 2 devices, there are 2^2000 possible choices ~ 10^603", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||36", "text": "- In Figure 6 (Bottom), is there any reason why you didn't show the result for BERT?", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||37", "text": "References:", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||38", "text": "[1] Aditya Paliwal,  Felix Gimeno,  Vinod Nair,  Yujia Li,  Miles Lubin,  Pushmeet Kohli,  and Oriol Vinyals.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||39", "text": "Reinforced genetic algorithm learning for optimizing computation graphs.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||40", "text": "arXiv preprint arXiv:1905.02494, 2020", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||41", "text": "[2] Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan, Shreyan Gupta, Hongzi Mao, and Moham-mad Alizadeh.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||42", "text": "Placeto: Efficient progressive device placement optimization.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||43", "text": "In NIPS MachineLearning for Systems Workshop, 2018", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||44", "text": "[3] Azalia Mirhoseini, Anna Goldie, Hieu Pham, Benoit Steiner, Quoc V. Le, and Jeff Dean.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||45", "text": "A hierarchical model for device placement.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||46", "text": "In International Conference on Learning Representations, 2018.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||47", "text": "URL https://openreview.net/forum?id=Hkc-TeZ0W.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||48", "text": "[4] Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, NaveenKumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||49", "text": "Device placement optimization with reinforcement learning.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||50", "text": "arXiv preprint arXiv:1706.04972, 2017.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||51", "text": "[5] Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Sungmin Bae, Azade Nazi, Jiwoo Pak, Andy Tong, KavyaSrinivasa, William Hang, Emre Tuncer, Anand Babu, Quoc V. Le, James Laudon, Richard Ho,Roger Carpenter, and Jeff Dean.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||52", "text": "Chip placement with deep reinforcement learning.", "label": null}
{"identifier": "-6vS_4Kfz0|||0|||53", "text": "arXiv preprint arXiv:2004.10746, 2020.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||0", "text": "The paper proposes Evolutionary Graph Reinforcement Learning to solve the memory placement problem.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||1", "text": "Main ideas are using GNN as the network architecture for reinforcement learning agents that look for more informed priors for evolutionary algorithms.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||2", "text": "Overall novelty of the paper comes from the neat combination of RL, EA, and GNN, and applying it to memory placement (ML for Systems).", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||3", "text": "The paper indeed tackles an important problem that can affect the overall performance and efficiency of the hardware.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||4", "text": "I believe the reorganization of various off-the-shelf ML techniques to solve real problems in the systems domain marks a large contribution, hence the positive overall rating.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||5", "text": "One of the main drawbacks of the paper is that the paper only tests on a single type/configuration of hardware.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||6", "text": "While this is fine to some extent, this makes it hard to get confirmation about the generality of the overall method considering the large variance of the speedup.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||7", "text": "Another related question comes from how this work relates to the optimizations of the dataflows [1,2].", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||8", "text": "As it is difficult to evaluate the overall memory communication without considering the order of operations, etc. the work in turn neglects the big question and focuses on only the partial view of the problem.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||9", "text": "It would provide a nice reference point if some of these points are discussed in the paper.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||10", "text": "Last question comes from the baselines.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||11", "text": "While the previous works on tensor optimizations [3,4] are very closely related and many of the ideas provide a good comparison point, these have not been discussed nor cited.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||12", "text": "For example, I guess AutoTVM's way of approximating the search space using TreeGRU or XGBoost can help.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||13", "text": "Also, Chameleon's way of sampling the examples using adaptive sampling may provide an interesting reference point in terms of reduction of number of samples.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||14", "text": "Overall, I have enjoyed reading the paper and I find the ideas in the paper interesting.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||15", "text": "I am currently weakly pro for the paper, and look forward to the authors' response :)", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||16", "text": "Questions\n1. Could you provide and overview of the NNP-I's architecture in the appendix?", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||17", "text": "Also, possibly ablation studies over different configurations of the hardware.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||18", "text": "2. What are the relative communication speed of SRAM, LLC, and DRAM?", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||19", "text": "3. It is well discussed in the computer architecture community that the memory communication is very much determined by the dataflows of the architecture.", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||20", "text": "How are the results affected by these dataflows?", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||21", "text": "4. How does the work compare to the methods described in [3,4]?", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||22", "text": "[1] \"Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks\", ISCA 2016", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||23", "text": "[2] \"Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators\", ASPLOS 2020", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||24", "text": "[3] \"Learning to optimize tensor programs\", NeurIPS 2018", "label": null}
{"identifier": "-6vS_4Kfz0|||1|||25", "text": "[4] \"Chameleon: Adaptive code optimization for expedited deep neural network compilation\", ICLR 2020", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||0", "text": "Optimizing the execution of deep neural networks has tremendous impact on the cost and performance in many industries due to the proliferation of \"Deep Learning\".", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||1", "text": "There has recently been an interesting line of work of using learning to optimize policies related to placement and scheduling of the neural network computation graph outperforming tediously hand-crafted heuristics.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||2", "text": "The proposed paper would be a nice extension along this line.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||3", "text": "The impact of memory placement for DNN has been clearly motivated and is easy to appreciate.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||4", "text": "The paper overall is clear and easy to follow.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||5", "text": "The methodology is sound and justified.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||6", "text": "Experiments and results make a compelling case in supporting the claims.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||7", "text": "I think one aspect that is insufficiently motivated is the need to use a hybrid approach between RL and evolutionary algorithms.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||8", "text": "Results show improvement in performance but it is not clear to me why.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||9", "text": "Perhaps this is addressed in the CERL paper which I am not familiar with.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||10", "text": "In the results section, it would be nice to see the sample complexity of each of the methods.", "label": null}
{"identifier": "-6vS_4Kfz0|||2|||11", "text": "I see that the number of iterations are shown in Figure 3 but it is not clear to me if that also corresponds to the number of samples consumed by each of the approaches before they converge.", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||0", "text": "The paper describes a machine learning method for mapping computational nodes in a neural network onto different levels of the memory hierarchy (DRAM, LLC, and SRAM) to minimize latency of inference.", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||1", "text": "The proposed approach builds on CERL, combining policy gradient, reinforcement learning, and graph neural network, achieving 28-78% speed-up over the native NNP-I compiler on vision and language benchmarks (ResNet-50, ResNet-101, and BERT).", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||2", "text": "Overall, the paper was well-written, targets an impactful problem, and the reported improvements (28-78% over native compiler) are impressive.", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||3", "text": "In the related work section, I did have a concern, as the authors state \u201cFor example, previous work with manual grouping (sic) operate at most in 5^280 \\~= 10^196 dimensional action space (Mirhoseini et al., 2020), compared to 10~ 20^358 for our BERT problem\u201d.", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||4", "text": "However, Mirhoseini et al., 2020 (\u201cChip placement with deep reinforcement learning\u201d) places \u201ca few thousand clusters\u201d (>=2000 nodes) onto a grid with \u201can average of 30 rows and columns\u201d (~900 cells), so wouldn\u2019t the action space be at least 900^2000?", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||5", "text": "Also, didn\u2019t that work use a heuristic grouper (hMETIS), but maybe that\u2019s close enough to \u201cmanual\u201d?", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||6", "text": "The authors only look at three benchmarks, but they were well-chosen (two representative vision models and one large language model).", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||7", "text": "It\u2019s also good that they compare against PG and EA alone as a form of ablation, given that their method is effectively a combination of these two.", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||8", "text": "It would have been better if they also had compared with prior state-of-the-art (e.g. HDP, REGAL, Placeto, or (the unmentioned) GDP / GO), but it is somewhat understandable given that their code does not seem to be open-sourced.", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||9", "text": "I liked that the authors report mean and standard deviation for the five runs, and measured \u201ctrue\u201d reward by running on hardware.", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||10", "text": "I also thought they did a good job motivating their method (aside from the questionable statements about action spaces in prior work), and of analyzing and visualizing its performance.", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||11", "text": "Nits:", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||12", "text": "In the Method section, \u201cthe compiler rectifies them and outputs a modified map, M_c, that is fully executable (Line 6).\u201d", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||13", "text": "It would probably be good to add \u201cin Algorithm 1\u201d so as not to confused the reader.", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||14", "text": "\u201cIt comprises of a single PG learner\u201d ->", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||15", "text": "\u201cIt is comprised of\u2026\u201d", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||16", "text": "\u201cBoth methods are known to produce highly performant and stable solutions but are also significantly slow compared to Deep RL\u201d (\u201csignificantly slower than\u201d?)", "label": null}
{"identifier": "-6vS_4Kfz0|||3|||17", "text": "\u201cWhile the transferred policies are clearly underperform those from scratch\u201d -> \u201cunderperforming\u201d", "label": null}
{"identifier": "-757TnNDwIn|||0|||0", "text": "In this paper, the authors introduce a NAS technique with an adversarial component.", "label": null}
{"identifier": "-757TnNDwIn|||0|||1", "text": "The discriminator learns to tell the difference between a set of good networks and randomly generated ones.", "label": null}
{"identifier": "-757TnNDwIn|||0|||2", "text": "This is quite a nice idea.", "label": null}
{"identifier": "-757TnNDwIn|||0|||3", "text": "A few comments.", "label": null}
{"identifier": "-757TnNDwIn|||0|||4", "text": "I don\u2019t think Section 3.1 adds anything and would be better off in an appendix.", "label": null}
{"identifier": "-757TnNDwIn|||0|||5", "text": "The connection between Algorithm 1 and Algorithms 2 seems fairly tenuous to me (although I could be wrong).", "label": null}
{"identifier": "-757TnNDwIn|||0|||6", "text": "The bibliography is unacceptable.", "label": null}
{"identifier": "-757TnNDwIn|||0|||7", "text": "All papers with more than two authors are written as \u201cet al.\u201d, and there are glaring inconsistencies.", "label": null}
{"identifier": "-757TnNDwIn|||0|||8", "text": "arXiv is mentioned in multiple different ways and different fonts are used for different entries.", "label": null}
{"identifier": "-757TnNDwIn|||0|||9", "text": "On a formatting issue, the text in the paper doesn\u2019t look right compared to other ICLR submissions (it\u2019s too pale).", "label": null}
{"identifier": "-757TnNDwIn|||0|||10", "text": "It would be worth looking in to this.", "label": null}
{"identifier": "-757TnNDwIn|||0|||11", "text": "The paper is otherwise fairly well written.", "label": null}
{"identifier": "-757TnNDwIn|||0|||12", "text": "Referring to EfficientNet as \u201cGoogle\u2019s EfficientNet\u201d is quite odd.", "label": null}
{"identifier": "-757TnNDwIn|||0|||13", "text": "ResNet is not written as \u201cMicrosoft\u2019s ResNet\u201d.", "label": null}
{"identifier": "-757TnNDwIn|||0|||14", "text": "I would recommend crediting the authors and not the institution.", "label": null}
{"identifier": "-757TnNDwIn|||0|||15", "text": "Table 2 appears above Table 1.", "label": null}
{"identifier": "-757TnNDwIn|||0|||16", "text": "The details regarding training and the generator architecture are relegated to the appendix.", "label": null}
{"identifier": "-757TnNDwIn|||0|||17", "text": "These are very elaborate, which makes it very difficult to tell what is exactly contributing to the algorithm working.", "label": null}
{"identifier": "-757TnNDwIn|||0|||18", "text": "Table 7 seems to indicate that the discriminator itself can be removed for quite a small change in mean accuracy (94.2ish to 94.1ish).", "label": null}
{"identifier": "-757TnNDwIn|||0|||19", "text": "Without the discriminator the algorithm appears to be REINFORCE but with a more complicated generator network.", "label": null}
{"identifier": "-757TnNDwIn|||0|||20", "text": "On a related note, the comparison to REINFORCE is missing in table 1 which makes me suspect that this algorithm is basically the same thing in outcome, if not in effect.", "label": null}
{"identifier": "-757TnNDwIn|||0|||21", "text": "Table 9 in the appendix (minor note \u2014 it\u2019s not really an appendix when it\u2019s in a separate file!) seems to show that different means of varying the size of the pool of networks over training has very little effect.", "label": null}
{"identifier": "-757TnNDwIn|||0|||22", "text": "(Having 94.227 in bold above 94.22 doesn\u2019t change the fact that we are talking about 0.007%!)", "label": null}
{"identifier": "-757TnNDwIn|||0|||23", "text": "The evaluation in Table 1 is very odd, as the authors are reporting the best acc instead of mean+-std as is common practice.", "label": null}
{"identifier": "-757TnNDwIn|||0|||24", "text": "This is unreliable, as when deploying these algorithms in the wild we are far more interested in how they do in expectation (particularly if they fail completely some of the time).", "label": null}
{"identifier": "-757TnNDwIn|||0|||25", "text": "Although the idea of using an adversarial framework to tell apart architectures in a search space is nice, the implementation has many moving parts, and doesn\u2019t appear noticeably different to the standard REINFORCE NAS approach (which just has a generator as an RNN).", "label": null}
{"identifier": "-757TnNDwIn|||0|||26", "text": "The presence of a discriminator has a very minimal effect, which is a shame.", "label": null}
{"identifier": "-757TnNDwIn|||0|||27", "text": "Some evaluation choices are very questionable.", "label": null}
{"identifier": "-757TnNDwIn|||0|||28", "text": "I am inclined towards rejection.", "label": null}
{"identifier": "-757TnNDwIn|||1|||0", "text": "This paper proposes a Neural Architecture Search algorithm (GA-NAS) based on adversarial learning.", "label": null}
{"identifier": "-757TnNDwIn|||1|||1", "text": "The generator constructs architectures auto-regressively, which receives feedback from a GNN discriminator.", "label": null}
{"identifier": "-757TnNDwIn|||1|||2", "text": "Reinforcement learning (PPO) is used for training, to solve non-differentiability.", "label": null}
{"identifier": "-757TnNDwIn|||1|||3", "text": "GA-NAS\u2019s effectiveness is demonstrated on several architecture search benchmarks for CIFAR-10 and 100, and is shown to improve EfficientNet for ImageNet.", "label": null}
{"identifier": "-757TnNDwIn|||1|||4", "text": "Disclosure: I\u2019m only vaguely familiar with the neural architecture search literature.", "label": null}
{"identifier": "-757TnNDwIn|||1|||5", "text": "Pros:\n1.\tGA-NAS appears to consistently find high ranking architectures compared to the baselines, often at the cost of fewer queries.", "label": null}
{"identifier": "-757TnNDwIn|||1|||6", "text": "2.\tA fairly extensive set of experiments are included in the Experiments section and Appendix.", "label": null}
{"identifier": "-757TnNDwIn|||1|||7", "text": "3.\tGA-NAS is able to incorporate constraints into search.", "label": null}
{"identifier": "-757TnNDwIn|||1|||8", "text": "The authors demonstrate that GA-NAS is able to find a variation that slightly outperforms EfficientNet-B0.", "label": null}
{"identifier": "-757TnNDwIn|||1|||9", "text": "Cons:\n1.\tAs the name implies, the goal of NAS is search.", "label": null}
{"identifier": "-757TnNDwIn|||1|||10", "text": "GANs have proven excellent over the years at interpolation, but not extrapolation, which is what search/exploration requires.", "label": null}
{"identifier": "-757TnNDwIn|||1|||11", "text": "I\u2019m concerned that a GAN-based approach is therefore limited in its search ability.", "label": null}
{"identifier": "-757TnNDwIn|||1|||12", "text": "How would GA-NAS compare with a random search policy constrained to be close to known \u201cgood\u201d architectures?", "label": null}
{"identifier": "-757TnNDwIn|||1|||13", "text": "2.\tThe GA-NAS generator and discriminator are initialized with an initial set of good architectures X_0.", "label": null}
{"identifier": "-757TnNDwIn|||1|||14", "text": "In the experiments, X_0 takes on the value of 50 and 100.", "label": null}
{"identifier": "-757TnNDwIn|||1|||15", "text": "The assumption that such a large number of \u201cgood\u201d architectures are available ahead of time seems rather strong.", "label": null}
{"identifier": "-757TnNDwIn|||1|||16", "text": "3.\tThe authors state that previous work has determined that other architecture search methods are not any better than random search.", "label": null}
{"identifier": "-757TnNDwIn|||1|||17", "text": "The gains in accuracies of the architectures produced by GA-NAS over the baseline seem moderate at best.", "label": null}
{"identifier": "-757TnNDwIn|||1|||18", "text": "Questions:\n1.\tWhere in the f-GAN paper is it stated that the JS divergence is more robust than the assymetric KL?", "label": null}
{"identifier": "-757TnNDwIn|||1|||19", "text": "Can you demonstrate this claimed advantage in GA-NAS by comparing objectives?", "label": null}
{"identifier": "-757TnNDwIn|||1|||20", "text": "2.\tIs X_0 counted in the number of queries reported?", "label": null}
{"identifier": "-757TnNDwIn|||1|||21", "text": "How were these initial architectures chosen?", "label": null}
{"identifier": "-757TnNDwIn|||1|||22", "text": "What rank are they?", "label": null}
{"identifier": "-757TnNDwIn|||1|||23", "text": "Miscellaneous:\n1.\tThis paper doesn\u2019t adhere to the ICLR citation guidelines; citations in the text should include the first author\u2019s last name and year.", "label": null}
{"identifier": "-757TnNDwIn|||1|||24", "text": "Additionally, the entire author list should appear in the references, not just \u201c[First author] et al.\u201d", "label": null}
{"identifier": "-757TnNDwIn|||1|||25", "text": "Please correct this during the rebuttal phase.", "label": null}
{"identifier": "-757TnNDwIn|||1|||26", "text": "2.\tTable 1: Typically, values in a table are bolded to represent the best values.", "label": null}
{"identifier": "-757TnNDwIn|||1|||27", "text": "BANANAS has the same accuracy value as GA-NAS.", "label": null}
{"identifier": "-757TnNDwIn|||1|||28", "text": "3.\tWhy is the variance of GA-NAS\u2019s accuracy so small in Table 1?", "label": null}
{"identifier": "-757TnNDwIn|||1|||29", "text": "4.\tTable 2 caption could be more descriptive.", "label": null}
{"identifier": "-757TnNDwIn|||1|||30", "text": "Besides the columns, how is this table differ from Table 1?", "label": null}
{"identifier": "-757TnNDwIn|||1|||31", "text": "5.\tThere\u2019s an extra space between \u201cImageNet\u201d and footnote 2.", "label": null}
{"identifier": "-757TnNDwIn|||1|||32", "text": "Rating:", "label": null}
{"identifier": "-757TnNDwIn|||1|||33", "text": "While the results seem good, I\u2019m not entirely convinced that a generative adversarial approach can effectively explore a neural architecture space, as the discriminator will inherently disincentivize deviating from previously seen architectures.", "label": null}
{"identifier": "-757TnNDwIn|||1|||34", "text": "I\u2019m also concerned about the validity of the assumption that an initial set of known \u201cgood\u201d architectures would be available to a search algorithm; the authors should clarify how these were selected.", "label": null}
{"identifier": "-757TnNDwIn|||1|||35", "text": "I lean toward rejection for now, but would be willing to raise my score if the authors could address my concerns.", "label": null}
{"identifier": "-757TnNDwIn|||1|||36", "text": "========\nPost-rebuttal\n========", "label": null}
{"identifier": "-757TnNDwIn|||1|||37", "text": "I thank the authors for answering my questions.", "label": null}
{"identifier": "-757TnNDwIn|||1|||38", "text": "While I am satisfied with the authors response to my concern about the \"initial good architectures\" assumption, I still remain unconvinced that adversarial learning can help search find new good architectures.", "label": null}
{"identifier": "-757TnNDwIn|||1|||39", "text": "I keep my score.", "label": null}
{"identifier": "-757TnNDwIn|||1|||40", "text": "I also encourage the authors to carefully re-read the f-GAN paper, which explains exactly how any f-divergence (including the KL) can be implemented for adversarial learning.", "label": null}
{"identifier": "-757TnNDwIn|||1|||41", "text": "Switching to any f-divergence requires only a simple change to the loss function.", "label": null}
{"identifier": "-757TnNDwIn|||1|||42", "text": "It also appears that the authors significantly misunderstand VAEs.", "label": null}
{"identifier": "-757TnNDwIn|||1|||43", "text": "The difference between GANs and VAEs is not JS-divergence vs KL-divergence.", "label": null}
{"identifier": "-757TnNDwIn|||1|||44", "text": "Using a KL loss for adversarial learning does not require switching to a VAE.", "label": null}
{"identifier": "-757TnNDwIn|||1|||45", "text": "Given the central role they play in this paper's motivation, a better understanding of these subjects is important.", "label": null}
{"identifier": "-757TnNDwIn|||2|||0", "text": "A flexible but complex and expensive NAS method.", "label": null}
{"identifier": "-757TnNDwIn|||2|||1", "text": "Summary:", "label": null}
{"identifier": "-757TnNDwIn|||2|||2", "text": "The authors introduce a method for NAS that repeatedly trains a generator to sample candidate architectures.", "label": null}
{"identifier": "-757TnNDwIn|||2|||3", "text": "The method is evaluated on three NAS oracle benchmarks as well as constrained NAS settings.", "label": null}
{"identifier": "-757TnNDwIn|||2|||4", "text": "While there are some promising experimental results, I lean slightly against acceptance due to poor presentation, limited comparisons on most evaluations, and what seems like fairly limited benefits of the approach given its complexity and cost.", "label": null}
{"identifier": "-757TnNDwIn|||2|||5", "text": "Strengths:\n1. The method can easily incorporate constraints on computation and memory.", "label": null}
{"identifier": "-757TnNDwIn|||2|||6", "text": "2. The method outperforms existing non-weight-sharing methods on several benchmarks.", "label": null}
{"identifier": "-757TnNDwIn|||2|||7", "text": "Weaknesses:\n1. The method introduces a lot of complexity such as a graph NN, a recurrent NN, and a full round of generative adversarial training in each search iteration.", "label": null}
{"identifier": "-757TnNDwIn|||2|||8", "text": "The computational cost of the latter is not discussed.", "label": null}
{"identifier": "-757TnNDwIn|||2|||9", "text": "2. As with most non-weight-sharing methods, GA-NAS requires several hundred queries on each benchmark, which translates to GPU-weeks of search time.", "label": null}
{"identifier": "-757TnNDwIn|||2|||10", "text": "It is not clear that the benefits over weight-sharing methods, which are not quantified for most cases, outweigh this large search cost.", "label": null}
{"identifier": "-757TnNDwIn|||2|||11", "text": "3. The results section for unconstrained search is confusing and it is hard to make comparisons (see notes 4-7 below).", "label": null}
{"identifier": "-757TnNDwIn|||2|||12", "text": "4. From a look at the NAS-Bench-301 paper, it seems that BANANAS was the best non-weight-sharing method evaluated, but the authors compare only to EA and RS.", "label": null}
{"identifier": "-757TnNDwIn|||2|||13", "text": "5. In the constrained search section, there are no comparisons to any other NAS methods.", "label": null}
{"identifier": "-757TnNDwIn|||2|||14", "text": "For example, random search is just as easy to apply to constrained problems as GA-NAS and should be used as a baseline.", "label": null}
{"identifier": "-757TnNDwIn|||2|||15", "text": "6. There is no code in the supplementary materials.", "label": null}
{"identifier": "-757TnNDwIn|||2|||16", "text": "Will code be released?", "label": null}
{"identifier": "-757TnNDwIn|||2|||17", "text": "Notes:\n1. \u201cRemain hard to be assessed\u201d -> \u201cRemain hard to assess\u201d\n2.", "label": null}
{"identifier": "-757TnNDwIn|||2|||18", "text": "The citation style does not follow ICLR guidelines.", "label": null}
{"identifier": "-757TnNDwIn|||2|||19", "text": "3. \u201carchitectures are sampled, which are discretized graphs\u201d ->", "label": null}
{"identifier": "-757TnNDwIn|||2|||20", "text": "\u201carchitectures, which are discretized graphs, are sampled\u201d\n4.", "label": null}
{"identifier": "-757TnNDwIn|||2|||21", "text": "What does it mean to \u201cdiscover the Nth best architecture in Q queries\u201d?", "label": null}
{"identifier": "-757TnNDwIn|||2|||22", "text": "Is \u201cbest\u201d here according to test or validation?", "label": null}
{"identifier": "-757TnNDwIn|||2|||23", "text": "Why is Q a good metric for speed given that the algorithm doesn\u2019t know to stop after query Q since in practice it won\u2019t know the rank N of the current architecture?", "label": null}
{"identifier": "-757TnNDwIn|||2|||24", "text": "5. Many numbers in paragraph 4 for Section 4.1 do not correspond to any number in any table.", "label": null}
{"identifier": "-757TnNDwIn|||2|||25", "text": "6. Why isn\u2019t the BANANAS performance bolded in Table 1?", "label": null}
{"identifier": "-757TnNDwIn|||2|||26", "text": "7. Table 4 should include at least one weight-sharing method such as GDAS (Dong & Yang, 2019), which is much faster and performs reasonably well.", "label": null}
{"identifier": "-757TnNDwIn|||2|||27", "text": "# Post-response update", "label": null}
{"identifier": "-757TnNDwIn|||2|||28", "text": "Thank you to the authors for answering some of my questions and clarifying the search and evaluation of GA-NAS.", "label": null}
{"identifier": "-757TnNDwIn|||2|||29", "text": "I believe my original assessment that the contributed method was complex remains accurate; while the authors note that other methods like ENAS also use an RNN controller, in my view those methods are also complex.", "label": null}
{"identifier": "-757TnNDwIn|||2|||30", "text": "This paper increases this complexity with a GNN and an adversarial training setup.", "label": null}
{"identifier": "-757TnNDwIn|||2|||31", "text": "Use of such additions require showing significant improvements over baselines like random search, which I do not believe is achieved.", "label": null}
{"identifier": "-757TnNDwIn|||2|||32", "text": "I thus stand by my initial rating.", "label": null}
{"identifier": "-757TnNDwIn|||3|||0", "text": "Thanks for your informative response addressing my comments.", "label": null}
{"identifier": "-757TnNDwIn|||3|||1", "text": "After the revision, the description of the method is clearer (Sec 3.2), and the experimental results are clearer (Sec 4).", "label": null}
{"identifier": "-757TnNDwIn|||3|||2", "text": "I'll stay with my original accept-score.", "label": null}
{"identifier": "-757TnNDwIn|||3|||3", "text": "===============", "label": null}
{"identifier": "-757TnNDwIn|||3|||4", "text": "Summary:", "label": null}
{"identifier": "-757TnNDwIn|||3|||5", "text": "The paper provides interesting results for neural architecture search.", "label": null}
{"identifier": "-757TnNDwIn|||3|||6", "text": "In particular, this paper proposes a search strategy for NAS problems, Generative Adversarial NAS (GA-NAS), using importance sampling, which can be applied to micro/macro, constrained/unconstrained search problems.", "label": null}
{"identifier": "-757TnNDwIn|||3|||7", "text": "GA-NAS beats the state-of-the-art search algorithms proposed for NAS on public benchmarks, including NAS-Bench-101, NAS-Bench-201, and NAS-Bench-301.", "label": null}
{"identifier": "-757TnNDwIn|||3|||8", "text": "Also, on the EfficientNet macro search space, GA-NAS finds a new architecture with higher ImageNet accuracy and a lower number of parameters than EfficientNet-B0.", "label": null}
{"identifier": "-757TnNDwIn|||3|||9", "text": "Pros:", "label": null}
{"identifier": "-757TnNDwIn|||3|||10", "text": "1. The proposed method achieves higher performance to compare to previous methods with better robustness, reproducibility, and efficiency.", "label": null}
{"identifier": "-757TnNDwIn|||3|||11", "text": "2. The idea of NAS based on importance sampling for rare event simulation in the method seems interesting.", "label": null}
{"identifier": "-757TnNDwIn|||3|||12", "text": "The proposed method at the same time could be broadly applied to micro/macro, constrained/unconstrained search problems.", "label": null}
{"identifier": "-757TnNDwIn|||3|||13", "text": "3. This paper provides comprehensive experiments, including various ablation studies, to show the effectiveness of the proposed framework.", "label": null}
{"identifier": "-757TnNDwIn|||3|||14", "text": "Cons:", "label": null}
{"identifier": "-757TnNDwIn|||3|||15", "text": "1. I suggest the authors conduct further ablation studies to enhance the understanding of the approach and readability of the paper:", "label": null}
{"identifier": "-757TnNDwIn|||3|||16", "text": "(1) Comparison of computational resources (e.g. wall clock inference time) required for each query in Table 1 and Table 3.", "label": null}
{"identifier": "-757TnNDwIn|||3|||17", "text": "To be a fair comparison, it would be better to compare [number of queries * resource consumed per query].", "label": null}
{"identifier": "-757TnNDwIn|||3|||18", "text": "(2) For the update algorithm of the generator, the proposed method uses JS-divergence minimization referring to [29].", "label": null}
{"identifier": "-757TnNDwIn|||3|||19", "text": "Section 3.1 mentions that JS-divergence is more robust than KL-divergence, but I think a further analysis could strengthen the point.", "label": null}
{"identifier": "-757TnNDwIn|||3|||20", "text": "(3) Adding FLOPs or inference speed to Table 5 and Table 6 would be helpful in explaining the performance of the new architecture found by GA-NAS.", "label": null}
{"identifier": "-757TnNDwIn|||3|||21", "text": "2. Section 3, 4 need to be polished for better readability.", "label": null}
{"identifier": "-757TnNDwIn|||3|||22", "text": "For example, an explanation about the method and the concept of evaluation metric \"rank\" should be improved for the readers.", "label": null}
{"identifier": "-757TnNDwIn|||3|||23", "text": "Some typos:", "label": null}
{"identifier": "-757TnNDwIn|||3|||24", "text": "(1) In the section 3.2 \u201c\\tau=\\{C_0,C_1,\\ldotsC_{N-1}\\}\u201d -> \u201c\\tau=\\{C_0,C_1,\\ldots,C_{N-1}\\}\u201d \n(2) In the equation (10) of appendix, \u201c-(1-\\rho) + P(X\\leq \\zeta^*)\\geq 0.\u201d -> \u201c-(1-\\rho) + P(X\\leq \\zeta^*)\\geq 0,\u201d", "label": null}
{"identifier": "-757TnNDwIn|||3|||25", "text": "Some suggestions:\n\"\\cdots\"s are used in the expression such as \"X_1,\\cdots, X_N\" and \"S(X_1),\\cdots, S(X_N)\" in the proof of the theorem 6.2 in the appendix.", "label": null}
{"identifier": "-757TnNDwIn|||3|||26", "text": "I think \"\\ldots\" is more syntactically typical here.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||0", "text": "This paper proposes a method for a deep generative model utilizing a trained autoencoder in conjunction with kernel conditional embeddings.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||1", "text": "The authors validate their model experimentally on CelebA and a brain imaging dataset.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||2", "text": "Overview : I found this paper very confusing with crucial mathematical definitions and exposition often inconsistent or imprecise.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||3", "text": "Outside of this the general exposition in many ways very strange and confusing.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||4", "text": "Finally the lack of experimental code makes this paper a very clear reject in my opinion.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||5", "text": "The rest of the review will list some concrete examples of the issues I mentioned.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||6", "text": "Mathematical imprecision:\n- The Phi operator in this paper is problematic.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||7", "text": "I'm guessing this is meant to be the kernel feature map, but in Def 1 it is introduced as an arbitrary element of the RKHS.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||8", "text": "In Def 5 Phi_H and Phi_G are initially introduced via the phrase \"where an element is denoted by Phi_G\" seeming to imply that is again simply an example element of their respective RKHSs.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||9", "text": "Later in this definition the well-known (kernel) cross covariance operator \"C_YX = E_YX [ Phi_G (y) \\otimes Phi_H (x)]\" which I am assuming implies that it is the kernel feature map, since otherwise this quantity is simply a scalar.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||10", "text": "Finally in the description of their main algorithm step 1 the refer to Phi_G^T which is highly problematic since Phi_G is presumably well defined here, meaning it is the kernel feature map which is not a linear operator (the whole point of kernel methods being that this operator is nonlinear), thus its \"transpose\" isn't something that makes sense.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||11", "text": "The authors give a vector which is equal to the expression in which Phi_G appears, this however depends on a \\bold{1} vector defined as \" are the top lambda (a hyper-parameter) latent representations\" which is not explained further.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||12", "text": "Presumably this is referring to approximate inverse kernel feature map (which has seen some research), but this needs to be further explained, or at the very least a citation is needed.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||13", "text": "General Exposition:\n-I am not sure why the approach to the paper is being framed in a Markov setting, when it seems like the authors really only need kernel conditional embeddings which have the exact same form as the Perron Frobenius operator the authors use.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||14", "text": "The authors even cite the kernel conditional embedding paper Song et al. 2009.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||15", "text": "Things like (1) which only serve to complicate the exposition unnecessarily considering there is no Markov process type of structure in the main algorithm.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||16", "text": "-p. 2 Rationale: \"For instance, it is known that given infinite-dimensional observables of the input measurements, there exists a linear transfer operator that perfectly pushes one probability density to another.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||17", "text": "Of course, this is not practically beneficial because explicitly constructing such infinite-dimensional observables for the data could be intractable.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||18", "text": "Nonetheless, results in control theory demonstrate that the idea can still be effective in specific cases, using approximations with either spectral analysis of large but finite number of observable functions \" This sort of thing needs to be decompressed, I don't know what is meant by an \"observable\" I assume it has something to do with control theory or Markov process theory.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||19", "text": "This seems to niche for me for the deep learning crowd.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||20", "text": "-The Riemannian geometry in the paper needs to be explained more for the average deep learning researcher, e.g. its not clear why the \"geodesic interpolation\" is doing at or why its algorithm makes sense.", "label": null}
{"identifier": "-BA38x6Cf2|||0|||21", "text": "-The main algorithm needs to be explained with more details as to _why_ we are doing the steps.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||0", "text": "##### Post-rebuttal update", "label": null}
{"identifier": "-BA38x6Cf2|||1|||1", "text": "Dear authors,", "label": null}
{"identifier": "-BA38x6Cf2|||1|||2", "text": "Thanks for the response.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||3", "text": "I strongly encourage you to revise the paper using languages other than flow-based generative models.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||4", "text": "From the rebuttal I could not see whether you understand my point or not:", "label": null}
{"identifier": "-BA38x6Cf2|||1|||5", "text": "The conditional mean embedding operator defines an **integration** instead of an invertible **transformation**, which differentiate itself from any flow-based model (at least those you referenced).", "label": null}
{"identifier": "-BA38x6Cf2|||1|||6", "text": "As for now, I cannot recommend for acceptance.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||7", "text": "--------------", "label": null}
{"identifier": "-BA38x6Cf2|||1|||8", "text": "The paper proposes a different kind of generative model that is composed of autoencoder in the bottom, and a standard distribution p(z), and a conditional kernel mean embedding defined by a collection of sample pairs (z_i, l_i).", "label": null}
{"identifier": "-BA38x6Cf2|||1|||9", "text": "The distribution of the latent codes p(l) is modeled by the kernel sum rule that corresponds to the marginalization \\int p(l|z)p(z) dz (ultimately defined by the collection of sample pairs).", "label": null}
{"identifier": "-BA38x6Cf2|||1|||10", "text": "##### Originality & Significance", "label": null}
{"identifier": "-BA38x6Cf2|||1|||11", "text": "The proposal of modeling the latent distribution of an autoencoder in a nonparametric way (using kernel mean embeddings) is original to my best knowledge.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||12", "text": "To do this, you need to solve the pre-image problem (we can get the mean embedding of p(l) through kernel operations but it is generally difficult to map it back to samples).", "label": null}
{"identifier": "-BA38x6Cf2|||1|||13", "text": "This paper adopts a previous approach (geodesic interpolation, Salehian et al. 2015), which I'm not aware of but appears to be theoretically sound: \"the gI algorithm converges asymptotically to the true solution.\"", "label": null}
{"identifier": "-BA38x6Cf2|||1|||14", "text": "##### Clarity", "label": null}
{"identifier": "-BA38x6Cf2|||1|||15", "text": "The clarity is low.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||16", "text": "The title (and introduction) is very misleading as it says a lot about flow-based generative model.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||17", "text": "However, this paper is really about modeling the latent distribution of an autoencoder using conditional mean embeddings (what the paper calls embedded Perron-Frobenius operator)..", "label": null}
{"identifier": "-BA38x6Cf2|||1|||18", "text": "In every aspect I find it more close to a latent-variable model (there is a latent prior over the z space).", "label": null}
{"identifier": "-BA38x6Cf2|||1|||19", "text": "And the distribution in the L space is the result of the kernel sum rule that corresponds  to the integral \\int p(l|z)p(z) dz.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||20", "text": "Why do the authors prefer to call it a flow?", "label": null}
{"identifier": "-BA38x6Cf2|||1|||21", "text": "##### Others", "label": null}
{"identifier": "-BA38x6Cf2|||1|||22", "text": "* One strength of this paper is that the work is very careful in choosing appropriate kernels, the importance of which is often ignored in works that deploy kernel methods.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||23", "text": "Figure 5 clearly shows the benefits of a good kernel (NTK) for this task.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||24", "text": "The result also shows that Nystrom methods seem to work well with NTK", "label": null}
{"identifier": "-BA38x6Cf2|||1|||25", "text": "* Definition 1: the RKHS should be the completion of the span.", "label": null}
{"identifier": "-BA38x6Cf2|||1|||26", "text": "* I don't understand the method used to generate figure 6 (1), what is \"interpolation among 10 random latent points\"?", "label": null}
{"identifier": "-BA38x6Cf2|||1|||27", "text": "Why is this a sensible baseline?", "label": null}
{"identifier": "-BA38x6Cf2|||2|||0", "text": "This paper starts with an autoencoder trained on vision data.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||1", "text": "Autoencoders aren't generative models, strictly, so in order to make it so, they leverage a simple linear transformation on over RKHSs.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||2", "text": "The kernel they use is the NTK.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||3", "text": "In order to generate, the construct a reduced sample version of the Kernel using Nystrom's approximation, then use a geodesic interpolation algorithm to inverse map the transferred sample from the prior in the RKHS space back to the latent space for use in the decoder.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||4", "text": "The method appears to work, which in itself is very interesting as this means one should be able to construct generative models (a la VAEs) starting only from a simple autoencoder.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||5", "text": "The results aren't that impressive compared to the baselines compared, but surprisingly they don't compare to any flow-based algorithms, such as Normalizing Flows or related methods like Hamiltonian Variational Inference.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||6", "text": "VAEs using NICE or Real-NVP to transfer the latent space from a autoencoder would be good baselines.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||7", "text": "It would be good to have clearer references for the \"two-stage VAE\" in the experimental results section: I was unable to easily find what this model is from this draft.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||8", "text": "I found the geodesic interpolation part a bit difficult to follow and not clearly motivated.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||9", "text": "Could you go into more detail on its component parts and why they were included (e.g., what appears to be the importance sampling part)?", "label": null}
{"identifier": "-BA38x6Cf2|||2|||10", "text": "Linking this more clearly to the \"properties\" section would help I think.", "label": null}
{"identifier": "-BA38x6Cf2|||2|||11", "text": "What about the effect of the pretrained AE?", "label": null}
{"identifier": "-BA38x6Cf2|||2|||12", "text": "Is reconstruction a good measure for the fitness of an AE for this or is the regularization important?", "label": null}
{"identifier": "-BA38x6Cf2|||2|||13", "text": "How does one know one has a good autoencoder for this procedure?", "label": null}
{"identifier": "-BA38x6Cf2|||2|||14", "text": "Could you provide results comparing metrics on the autoencoder (e.g., reconstruction loss, L2 or L1 on the parameters or latent space) vs NLL using the kernel method?", "label": null}
{"identifier": "-BA38x6Cf2|||2|||15", "text": "Some other comments:\nSection 3.2 seems to have a few errors:\non 1. k -> k_z\non 2. is lambda supposed to be gamma?", "label": null}
{"identifier": "-BA38x6Cf2|||3|||0", "text": "**** Summary ****", "label": null}
{"identifier": "-BA38x6Cf2|||3|||1", "text": "The authors build a generator that builds on top of the latent space of a \u201cwell-trained auto-encoder\u201d.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||2", "text": "The generator consists of several steps: 1) sampling an latent element from the spherical latent space, 2) using a kernel Perron-Frobenius operator to embed the sampled latent element 3) selecting latent representations of real samples based on the indices of the highest values of the newly calculated embedding 4) calculating the geodesic interpolation of these latent representations 5) using the decoder of the autoencoder to generate new samples from the latent element resulting from geodesic interpolation.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||3", "text": "The authors describe the related work, the methods they propose and present experimental results on 4 datasets.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||4", "text": "To summarise, I believe that the contributions of the authors are the following:\n- Proposing a generator that relies on a kernel Perron-Frobenius operator (relevant: medium)\n- Comparing variants of their generator to state of the art generative models on four datasets (relevance: medium-low)", "label": null}
{"identifier": "-BA38x6Cf2|||3|||5", "text": "**** Positive points of the paper: ****", "label": null}
{"identifier": "-BA38x6Cf2|||3|||6", "text": "Relation to prior work and clarity:", "label": null}
{"identifier": "-BA38x6Cf2|||3|||7", "text": "The authors attempt to describe extensively the theory behind their work.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||8", "text": "However, as we will see below, the prior work section could be clearer.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||9", "text": "Additionally, the link between concepts introduced in prior work and presented in the newly developed algorithm should be more straightforward.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||10", "text": "See below for detailed comments on improvable points.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||11", "text": "Novelty:", "label": null}
{"identifier": "-BA38x6Cf2|||3|||12", "text": "The method consists of a novel combination between an existing generative model and an existing kernel transfer operator.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||13", "text": "I believe that the combination is new however it is not thoroughly justified.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||14", "text": "Theoretical setting:", "label": null}
{"identifier": "-BA38x6Cf2|||3|||15", "text": "The authors introduce in prior work several of the necessary definitions for the following part.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||16", "text": "However, some required hypotheses do not always seem to be carefully verified.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||17", "text": "See below for detailed comments on improvable points.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||18", "text": "Experimental setting:", "label": null}
{"identifier": "-BA38x6Cf2|||3|||19", "text": "The authors compare their model to several state-of-the-art generative models and show some results on one dataset using different types of kernels.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||20", "text": "However, details from the experiments are missing to be able to evaluate it thoroughly and the experiments are not reproducible.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||21", "text": "See below for detailed comments on improvable points.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||22", "text": "**** Improvable points of the paper ****", "label": null}
{"identifier": "-BA38x6Cf2|||3|||23", "text": "Relation to prior work and clarity:", "label": null}
{"identifier": "-BA38x6Cf2|||3|||24", "text": "In general, some sentences of the paper would benefit from being rewritten as they are too convoluted in my opinion.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||25", "text": "For example, p3, Section 3, first paragraph: \u201cWe will follow this rational\u2026 variance is decreased\u201d; p6, second paragraph, \u201cto push a uniform \u2026 wrapped on S2\u201d.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||26", "text": "Other sentences are not theoretically justified: p3, last paragraph: \u201cMoreover, the projection \u2026 that we will use later\u201d; p.4 Remark 2, \u201cBut notice that performing these \u2026. is more sensible\u201d.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||27", "text": "Additionally, it seems that the prior work section starts with the introduction and ends at Section 3.2 (excluded).", "label": null}
{"identifier": "-BA38x6Cf2|||3|||28", "text": "The structure of the paper does not allow a simple identification of the prior work from the method as Section 3 contains prior work and new developments.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||29", "text": "Also, it seems that most of the elements of the prior work sections are coming from Fukumizu et al. (2013) and Klus et al. (2020).", "label": null}
{"identifier": "-BA38x6Cf2|||3|||30", "text": "However, it is not always possible to understand the content of the paper without having to read the definitions/properties in the respective papers.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||31", "text": "For example, a) Fukumizu et al does not seem to mention the name of the Perron-Frobenius operator, b) in Definition 5 it is not clear why there is a P_eps and a P_k, especially when the variable eps is not defined before, c) Definition 5, it is not clear whether P_kg should belong or not to an RKHS for the Definition to be valid (as mentioned in Klus et al.) d) p4, last paragraph, the equation giving P_kg and P_eps are not introduced properly (especially the need for epsilon) and we need to look at the referred paper to understand where it is coming from e) p5, second paragraph, G_xx is not defined (I understand it is the gram matrix but it should be written), the same holds for N f) p5, second paragraph, equation 3 should be better explained and a reference to Klus et al. and Fukumizu should be made.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||32", "text": "g) p5, second paragraph, we don\u2019t understand what mu_t is.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||33", "text": "h)", "label": null}
{"identifier": "-BA38x6Cf2|||3|||34", "text": "Proposition 1 is not proven (and is written as a definition for reversibility in Klus et al) i) p5, in Notations, \\mathcal{L} is not introduced.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||35", "text": "j) p5, Section 3.2, D (decoder, but should be introduced) and top \\lambda are not clear.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||36", "text": "k) p5, Section 3.2, it is not clear what s is and it has a different typesetting in the bullet points 1 and 2. l)", "label": null}
{"identifier": "-BA38x6Cf2|||3|||37", "text": "The transpose symbol is not always the same along the paper and can be confused with the iteration number in Figure 4. m) it is not clear where Proposition 1 is needed in practice.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||38", "text": "Theoretical setting:\nSome concepts are not well defined.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||39", "text": "For example:\n- The authors always talk about a well-behaved VAE.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||40", "text": "However, the authors do not define what they mean by well-behaved.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||41", "text": "In addition, looking at Table 1, it seems that the proposed model is not systematically the best performing when the VAE is well behaved (the FID for vanilla VAEs are often better than the proposed model).", "label": null}
{"identifier": "-BA38x6Cf2|||3|||42", "text": "- I am not sure what \u201ctrue solution\u201d means p6, end of Properties.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||43", "text": "It would be useful if the authors could give the intuition to make the reader understand what the true solution means and the need of the geodesic interpolation.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||44", "text": "-", "label": null}
{"identifier": "-BA38x6Cf2|||3|||45", "text": "Some hypotheses are not verified, for example, a) is K_z invertible in practice (see Properties p6), b) is the experimental covariance is a good approximation for the true covariance.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||46", "text": "In general, the algorithm seems to be explained twice with different degrees of precision at the bottom of p5 and in Figure 3 (which should be referred to in the text).", "label": null}
{"identifier": "-BA38x6Cf2|||3|||47", "text": "The paper would benefit from merging both explanations.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||48", "text": "It is not clear to me why the new sampling strategy would follow better the distribution of real samples.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||49", "text": "I think this should be better explained.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||50", "text": "Experimental setting:", "label": null}
{"identifier": "-BA38x6Cf2|||3|||51", "text": "The experiments are described in a way that is not reproducible.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||52", "text": "We do not know which hyperparameters are chosen (for the AE and for the new development) and how they were chosen.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||53", "text": "We also do not understand how the 4000 landmark points are selected.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||54", "text": "It is not clear what SRAE_rand_interp is.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||55", "text": "I do not understand how Figure 5 is done, is it 2D projection of the generated samples?", "label": null}
{"identifier": "-BA38x6Cf2|||3|||56", "text": "If yes, with which dimension reduction method?", "label": null}
{"identifier": "-BA38x6Cf2|||3|||57", "text": "(\u201cwrapped on S^2\u201d a bit above the figure is not clear).", "label": null}
{"identifier": "-BA38x6Cf2|||3|||58", "text": "A better ablation study could be done:\n- In the Supplementary, the authors say that they use a regularised AE (with spectral normalisation) for the SRAE.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||59", "text": "However, the results of a regularised AE without the development of the authors are not shown.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||60", "text": "-", "label": null}
{"identifier": "-BA38x6Cf2|||3|||61", "text": "The authors do not justify theoretically the need for selecting the top \\gamma samples.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||62", "text": "Therefore, it would be interesting to vary \\gamma and observe differences in the results.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||63", "text": "In general, the results seem rather poor.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||64", "text": "The new method outperforms vanilla VAE or basic SRAE only in one of the three datasets for which we have quantitative results.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||65", "text": "The generative adversarial models outperform systematically the proposed model.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||66", "text": "**** Typos: ****\n\u201cWhich is benefit, given the efficient\u201d -> beneficial, efficiency\nGrammian -> Gramian or Gram", "label": null}
{"identifier": "-BA38x6Cf2|||3|||67", "text": "*****************", "label": null}
{"identifier": "-BA38x6Cf2|||3|||68", "text": "RECOMMANDATIONS:\nBecause of the points enumerated above, I recommend to reject the paper in its current form.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||69", "text": "I would increase the grade if a) the paper is rewritten in order to be more structured and clear b) the experiment section is better described and more thorough (ablation study) c) some answers are brought to the theoretical concerns.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||70", "text": "*****************", "label": null}
{"identifier": "-BA38x6Cf2|||3|||71", "text": "AFTER REBUTTAL", "label": null}
{"identifier": "-BA38x6Cf2|||3|||72", "text": "I read the rebuttal of the authors and the other reviews.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||73", "text": "I will increase the grade to a 5.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||74", "text": "The reasons are that I think that the authors improved significantly the paper with this revision.", "label": null}
{"identifier": "-BA38x6Cf2|||3|||75", "text": "However, I believe that the paper would benefit from experiments on a larger number of datasets, in order to better understand on which type of datasets their method shows better performance.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||0", "text": "This paper studies the universal approximation of robust networks called the abstract universal approximation.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||1", "text": "While the traditional universal approximation aims to approximate the single output corresponding to each input value, abstract universal approximation studies the output interval generated by the input interval (or box) and the interval value propagation.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||2", "text": "The main contribution of the paper is to extend the result of Baader et al., 2020 to networks using general squashable activation functions.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||3", "text": "I think that the main weakness of this paper is its novelty.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||4", "text": "The main idea of the proposed result seems very similar to that by Baader et al., 2020: approximate a target function using a summation of indicator functions.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||5", "text": "This idea and using a squashable (or sigmoid) activation function to approximate the indicator function have been widely used in the universal approximation literature.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||6", "text": "Hence, I think that the result of this paper (Theorem 4.2) can be viewed as a simple extension of the result by Baader et al. 2020.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||7", "text": "The authors additionally claim that their construction and analysis are simpler than those by Baader et al., 2020.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||8", "text": "However, I think that this is only a minor improvement as the main focus is to show the 'existence' of networks, approximating functions under constraints.", "label": null}
{"identifier": "-DRft_lKDqo|||0|||9", "text": "Due to these reasons, my decision tends to reject.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||0", "text": "This paper proposes to extend the techniques of Baader et al. [2020], demonstrating that interval analysis provable ReLU networks are universal approximators, to a larger class of activation functions, which they call squashable functions.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||1", "text": "Furthermore, they claim that their proof of this theorem is simpler due to using a bounded depth construction.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||2", "text": "Pros:", "label": null}
{"identifier": "-DRft_lKDqo|||1|||3", "text": "* Their theorem does reduce the depth of the necessary construction.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||4", "text": "* Expanding the theorem to more activation types is a potentially useful contribution.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||5", "text": "* Technically, the paper appears to be correct.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||6", "text": "Cons:", "label": null}
{"identifier": "-DRft_lKDqo|||1|||7", "text": "* The proposed method mostly follows the same proof technique ad Baader et al. [2020] and thus is not particularly novel.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||8", "text": "* The authors claim that their method uses the most commonly used activation functions, yet ReLU is by far the most relevant activation function and is already included in the original theorem.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||9", "text": "* The number of ReLUs they use for an indicator function (they might need to use intractably many indicator functions) is asymptotically identical to Baader et al [2020].", "label": null}
{"identifier": "-DRft_lKDqo|||1|||10", "text": "They have only reduced the number of neurons used per indicator by a constant.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||11", "text": "However, it is unclear whether this is an advantage, since the method appears to use more slices as well.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||12", "text": "How do the two methods compare in terms of number of neurons for the entire net when considering the same function, delta, and epsilon?", "label": null}
{"identifier": "-DRft_lKDqo|||1|||13", "text": "* The title \u201cGeneralized Universal Approximation for Certified Networks\u201d is claims a significant improvement in generality over the original paper, Universal Approximation with Certified Networks (Baader et al. [2020]), yet the addition of other activation functions is hardly a significant enough difference to warrant such a claim of generality.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||14", "text": "* Care is not taken when introducing concepts from prior work:  Section 3.2 discusses concepts introduced first by Cousot et al. [1992] and then brought to neural networks by Gehr et al. [2018], but is presented as if it were new.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||15", "text": "Definition 4.1 is an idea developed by Baader et al [2020] yet this is also not made clear.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||16", "text": "Section 6.2 proposes an idea that is very similar to Def 4.2 from Baader et al and uses concepts introduced as the nodal basis function in He et al [2018] but does not make this clear.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||17", "text": "Final Review:", "label": null}
{"identifier": "-DRft_lKDqo|||1|||18", "text": "In summary, while technically the paper does not appear to have flaws, its contributions are incremental, and its similarity to prior work is such that it is hard to recommend acceptance.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||19", "text": "I am thus giving this paper a rejection.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||20", "text": "[1] P. Cousot and R. Cousot.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||21", "text": "Abstract interpretation frameworks.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||22", "text": "Journal of Logic and Computation, 2(4):511\u2013547, 1992.\n[2]", "label": null}
{"identifier": "-DRft_lKDqo|||1|||23", "text": "Gehr, T., Mirman, M., Tsankov, P., Drachsler Cohen, D., Vechev, M., and Chaudhuri, S. Ai2: Safety and robustness certification of neural networks with abstract interpretation.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||24", "text": "In Symposium on Security and Privacy (SP), 2018.\n[3]", "label": null}
{"identifier": "-DRft_lKDqo|||1|||25", "text": "Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||26", "text": "ReLU Deep Neural Networks and Linear Finite Elements.", "label": null}
{"identifier": "-DRft_lKDqo|||1|||27", "text": "arXiv preprint arXiv:1807.03973, 2018.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||0", "text": "This work studies the task of universally approximating continuous functions by (certain classes of) neural networks.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||1", "text": "The paper shows that for a continuous function $f$ there exists a neural network $N$ such that for any box $B$ the range of outputs of $N$ is close (in a parameter $\\delta$) to the range of outputs of $f(x)$, $\\forall x \\in B$.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||2", "text": "This proof is constructive and applies to $N$ using ReLU, sigmoid, tanh or ELU activation functions.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||3", "text": "(The result actually includes an even larger class of activation functions, that the authors call _squashable_.)", "label": null}
{"identifier": "-DRft_lKDqo|||2|||4", "text": "A previous work, Baader et al., shows the same result when neural networks use ReLU activation functions.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||5", "text": "The authors of this paper provide a result applicable to a broader class of networks.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||6", "text": "They also improve the size of neural networks constructed in Baader et al. (as discussed in the last paragraph of Section 6), but at many points in the paper this result feels to be incremental.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||7", "text": "-- Minor comments about the paper:", "label": null}
{"identifier": "-DRft_lKDqo|||2|||8", "text": "Figure 1, which assumes Eq (1), could be moved to the top of Page 4, after Eq (1) is stated.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||9", "text": "Sections 3 and 4 have lots of text in bold.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||10", "text": "Many paragraphs are named, and then they have definitions with the same name.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||11", "text": "It reduces readability.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||12", "text": "The bottom of Figure 6 and the text are too close to each other.", "label": null}
{"identifier": "-DRft_lKDqo|||2|||13", "text": "**Updates**:", "label": null}
{"identifier": "-DRft_lKDqo|||2|||14", "text": "After carefully reading comments of the other reviewers as well as the authors' response, I change my score from 6 to 5.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||0", "text": "The paper shows an \"augmented\" universal approximation (UA) result for neural networks that the authors call Abstract UA (AUA for short) and the motivation comes from understanding expressivity and certifiability of NN.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||1", "text": "Their result holds for NN with a wide variety of activation units and this is the main point of the paper, which directly extends the same result for ReLU networks (Baader et al. 2020).", "label": null}
{"identifier": "-DRft_lKDqo|||3|||2", "text": "Typically, results in UA state that any continuous function f on a bounded domain can be expressed as a combination of activation units, sometimes even with only one hidden layer and one output layer, which is the classical result by Cybenko (1989), Hornik (1989) and more.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||3", "text": "Their augmented version asks what if we want to have a more robust version of those statements, where we want to certify that any given box in the domain of f is mapped through the NN in such a way so that the NN values always stay close to the values of f.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||4", "text": "More formally, for some given accuracy \\delta, we want to have |NN(x) - f(x)| < \\delta for all x in the box.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||5", "text": "Furthermore, we would like to be able to certify this property and to do so we can use the technique of interval propagation.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||6", "text": "(To put it differently, the image of any interval of f is basically \"sandwiched\" between two close-by surfaces that are output by the NN.)", "label": null}
{"identifier": "-DRft_lKDqo|||3|||7", "text": "The result is of theoretical nature and this is fine as expressivity and certifiability are not yet well-understood.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||8", "text": "The major issue with the paper is the lack of novelty.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||9", "text": "The most related paper in terms of techniques and ideas (Baader et al.) proved the exact same result when the activations are ReLUs instead of the wider variety of activation units considered in the paper.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||10", "text": "The units considered here, are basically monotone functions that have upper and lower cut-offs or can be phrased as such after a simple transformation.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||11", "text": "The main ideas for approximating functions via step functions and indicators on intervals or boxes has been used for decades (even starting with the folklore results in approximation theory) and there seems to be lack of substantially new ideas.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||12", "text": "The reviewer feels that given the previous work on ReLUs (Baader et al.), the proof for the more general activation units can be reverse-engineered.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||13", "text": "Finally, of secondary importance the authors state some simplifications over Baader et al.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||14", "text": "A minor issue that the reviewer is ok with, is that for a more practice-oriented person, it is hard to buy the motivation from adversarial robustness/certifiability as in many settings the adversary will not just choose an attack bounded in \\ell_infinity by some small parameter.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||15", "text": "It is nice to have certifiable NN and know their behaviour if the inputs slightly change, however the motivation also needs to address what happens when a few large perturbations are allowed like the single-pixel attack (\"One pixel attack for fooling deep neural networks\").", "label": null}
{"identifier": "-DRft_lKDqo|||3|||16", "text": "Overall, conditioned on the immediate prior work,  the result itself simply extends AUA from ReLU activations to more general units, and is not surprising.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||17", "text": "One quick suggestion:", "label": null}
{"identifier": "-DRft_lKDqo|||3|||18", "text": "Can the authors clarify how the lipschitzness of f or of the NN can affect the statements?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||19", "text": "Otherwise, the theorem may be misinterpreted as a consequence of continuity.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||20", "text": "Suggestions for future improving:", "label": null}
{"identifier": "-DRft_lKDqo|||3|||21", "text": "Did the authors consider proving something along the lines of depth separation results for such kinds of AUA?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||22", "text": "The reviewer believes this could give a new dimension to the AUA theorems and draw another connection with certifiability and depth/width tradeoffs.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||23", "text": "Depth is usually preferred in practice over width, and a formal way for proving this is to show the power of depth in representing functions: for example, relevant works here are Telgarsky's (Benefits of depth in neural networks) and later improvements (Better Depth-Width Trade-offs for Neural Networks through the lens of Dynamical Systems) that show exponential separations in depth vs width by constructing highly oscillatory functions.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||24", "text": "However these works do not consider certifiability which may be an opportunity for your techniques.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||25", "text": "Another quick question:", "label": null}
{"identifier": "-DRft_lKDqo|||3|||26", "text": "Is there a formal comparison of your squashable activation units to the semi-algebraic units considered in Telgarksy?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||27", "text": "Maybe your results can also capture such activations?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||28", "text": "Other comments:", "label": null}
{"identifier": "-DRft_lKDqo|||3|||29", "text": "Theorem 3.3:  Do the authors consider this one of the main contributions of the paper?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||30", "text": "Wouldn't it be better to be stated as a proposition, as it is quite straightforward for the activation units stated there?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||31", "text": "p1: \"Their theorem states that not only can neural networks approximate any continuous function f\n(universal approximation)\" ... the reviewer believe that this first part of the sentence is a bit misleading as universality has been known since the 80s.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||32", "text": "The sentence starting with \"but...\" is the extension presented in Baader et al. and that is the contribution, right?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||33", "text": "Fig.1: When one takes interval bounds and considers a superset of box B, shouldn't the output be superset of f(B)?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||34", "text": "Maybe the figure is a bit confusing?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||35", "text": "What does it mean:  \"(Right is adapted from Baader et al. (2020).)\"", "label": null}
{"identifier": "-DRft_lKDqo|||3|||36", "text": "p2: When Abstract universal approximation (AUA) theorem is stated: Again f is continuous so it should be clarified.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||37", "text": "Also: Line 5 of this paragraph: interval bounds are stated but if the parameters are not given (is it \\eps?) like it was done in the intro, then the result is not meaningful as it would be a consequence of the continuity of f and Lipschitzness of NN (please discuss).", "label": null}
{"identifier": "-DRft_lKDqo|||3|||38", "text": "Maybe another way to phrase the same thing, is to say that whenever the input is changed to B+\\eps, the network follows f(B)+\\delta for appropriate parameters?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||39", "text": "The point is just that the NN will not do anything \"too wild\" correct?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||40", "text": "In other words, there is a NN that will map intervals of f to not-much-wider intervals correct?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||41", "text": "Typos/Inaccuracies:\np2: the AUA theorem implies us that \np2: while controlling approximation error \u03b4\np2: a bounded number of layers --> just one hidden layer was proved in Cybenko.", "label": null}
{"identifier": "-DRft_lKDqo|||3|||42", "text": "What is the \"standard\" feedforward NN?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||43", "text": "p2: an universal -> a (several places)\np6: has rigid values --> rigid?", "label": null}
{"identifier": "-DRft_lKDqo|||3|||44", "text": "(meaning relu is fixed everywhere?)", "label": null}
{"identifier": "-GLNZeVDuik|||0|||0", "text": "### Summary:", "label": null}
{"identifier": "-GLNZeVDuik|||0|||1", "text": "This work uses the idea of variational inference to map categorical data to continuous space affording the use of normalizing flows.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||2", "text": "Authors use several ideas to increase their framework's applicability--factorized distribution assumption, use of multi-scale architecture for step-generation, and permutation invariant components\u2014achieving favorable results on several problems.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||3", "text": "The approach seems to be especially useful when data is non-sequential.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||4", "text": "### Strength:", "label": null}
{"identifier": "-GLNZeVDuik|||0|||5", "text": "I like the idea; it is simple and seems to be very useful in specific problems, especially those without a natural order on categorical variables.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||6", "text": "The authors also make an excellent effort to gather empirical evidence in favor of their method.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||7", "text": "The literature review is comprehensive, and the method is well-placed among the cited papers.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||8", "text": "Several sections are well written and have a nice natural flow.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||9", "text": "I find the application to graphs an excellent use of the multi-scale architectures in coupling-based flows and compliment the authors for the superb visualization in Figure 1.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||10", "text": "### Concerns:", "label": null}
{"identifier": "-GLNZeVDuik|||0|||11", "text": "I found the most critical section in the paper least clearly explained--section 2.2.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||12", "text": "I appreciate the overall idea of this work and believe I understand the work well, yet the use of ill-defined terms and lack of clarity hamper my confidence.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||13", "text": "In my current understanding of the paper, $p(z)$ is modeled using a large flow.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||14", "text": "Authors use the motivation that $KL[q||p]$ is negligible and approximate $p(x_i|z_i)$ using $q(z_i|x_i)$.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||15", "text": "With this, eq. 2 can be used to train; however, I am unsure if that is indeed the case; I will appreciate it if the rebuttal absolutely clarifies this.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||16", "text": "To make it easier for the authors, I effort to pinpoint the specific instances where they lose me in section 2.2.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||17", "text": "In the first paragraph, I find the last line: \"Specifically, as normalizing ..\" a bit hard to parse.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||18", "text": "In particular, the term \"one distribution\" is unclear.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||19", "text": "The next paragraph tries to explain why the decoder becomes almost deterministic; despite using words like \"therefore\" and \"as a consequence,\" I found the explanations insufficient.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||20", "text": "The term \"overall encoding distribution\" is unspecified in the paper,  and I am skeptical of the use of \"true decoder likelihood.\"", "label": null}
{"identifier": "-GLNZeVDuik|||0|||21", "text": "Maybe I am missing something obvious, but why does the expression for \"true decoder likelihood\" involve the variational approximation?", "label": null}
{"identifier": "-GLNZeVDuik|||0|||22", "text": "Is it because we work under the assumption that the KL divergence is negligible?", "label": null}
{"identifier": "-GLNZeVDuik|||0|||23", "text": "Also, I am unsure what the objective is when you have \"removed\" the ELBO (see the first line, last paragraph, section 2.2.).", "label": null}
{"identifier": "-GLNZeVDuik|||0|||24", "text": "#### Minor bits:\n\"... we do not have *an unknown ...\" --using an in place of a.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||25", "text": "### Updates after the rebuttal", "label": null}
{"identifier": "-GLNZeVDuik|||0|||26", "text": "I find the revised manuscript to be clear and more transparent.", "label": null}
{"identifier": "-GLNZeVDuik|||0|||27", "text": "After reading the reviews, the response, and the extended appendix, I am increasing my score and vote to accept this paper.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||0", "text": "# Summary #", "label": null}
{"identifier": "-GLNZeVDuik|||1|||1", "text": "This paper proposes a new normalizing flow model for categorial data, where the typical dequantization is not applicable.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||2", "text": "We assume a categorical sample x has S variables, and each attribute x_i is a categorical variable.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||3", "text": "We want to model the probability mass of the S variable categorical data, and devise an invertible map that can convert x into the continuous latent variable z.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||4", "text": "For simplicity, we assume that each attribute x_i has its own latent continuous probability distribution p(z_i).", "label": null}
{"identifier": "-GLNZeVDuik|||1|||5", "text": "We expect an encoder, q(z_i|x_i), map categorical x into a continuous space where all categories are well partitioned.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||6", "text": "For that purpose, the paper proposes to formulate q(z_i|x_i) by a mixture of logistic distributions.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||7", "text": "A graph generative flow model is proposed as an application.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||8", "text": "Existing flow models for graphs do not handle the categorical data in proper manners and are permutation-dependent.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||9", "text": "The proposed categorical flow can develop a permutaion-invariant graph generative flow model.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||10", "text": "The proposed model performs better than the existing graph flow models in the molecular graph generation experiments.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||11", "text": "Typical invalid generation examples include isolated nodes.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||12", "text": "If we only focus on the largest sub-graphs, the proposed model can almost perfect graph generations.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||13", "text": "The permutation-invariant nature of the proposed model results in a stable performance on the graph coloring problem, while the baseline RNN models are deeply affected by the order choices.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||14", "text": "# Comment", "label": null}
{"identifier": "-GLNZeVDuik|||1|||15", "text": "I found the mixture of logistic regression is a good idea.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||16", "text": "Figure 2 and 3 in appendix indicate that this formulation can pratitioin the latent space into categories.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||17", "text": "I have a few questions to confirm my understanding of the paper.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||18", "text": "Q1.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||19", "text": "The proposed categorical normalizing flow with K-logistic mixture provides an approximated invertible map for the true distribution of the categorical samples x.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||20", "text": "Is this correct?", "label": null}
{"identifier": "-GLNZeVDuik|||1|||21", "text": "Namely, there is a non-zero KL divergence between the evidence P(X) and the marginalized ``likelihood'' q(X)??", "label": null}
{"identifier": "-GLNZeVDuik|||1|||22", "text": "Q2.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||23", "text": "The paper says \"we do not have a unknown KL divergence between approximate and true posterior constituting an ELBO\".", "label": null}
{"identifier": "-GLNZeVDuik|||1|||24", "text": "Does this mean we can compute the KL(q||p) analytically for the categorical normalizing flow?", "label": null}
{"identifier": "-GLNZeVDuik|||1|||25", "text": "Concerning the Q2.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||26", "text": "it is better if the final objective function to maximize/minimize, and the actual procedure for model training is clearly written in the main manuscript or the appendix.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||27", "text": "Concerning the molecular graph generation experiments, I'm interested in how the latent representations of the graphs are distributed in the space of Z.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||28", "text": "It is preferable if the paper can provide a visualization of the latent space for the actual molecular graph generation experiments, not the simulated ones of Figure 2 and 3.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||29", "text": "Presentaions of the experimental results in the main manuscript totally rely on the tables.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||30", "text": "However, current tables are not much effective to tell the significance of the proposed method.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||31", "text": "Please consider visual presentations: we may use plots or bar graphs to compare several methods for example if the detailed numbers are not important.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||32", "text": "The actual numbers can be moved to the appendix.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||33", "text": "# Evaluation points", "label": null}
{"identifier": "-GLNZeVDuik|||1|||34", "text": "(+)", "label": null}
{"identifier": "-GLNZeVDuik|||1|||35", "text": "A new approach to apply the normalizing flow.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||36", "text": "(+)", "label": null}
{"identifier": "-GLNZeVDuik|||1|||37", "text": "Truly permutation-invariant NF for graph generation is great.", "label": null}
{"identifier": "-GLNZeVDuik|||1|||38", "text": "(-) insufficient explanations for the optimization procedure", "label": null}
{"identifier": "-GLNZeVDuik|||1|||39", "text": "(-) more visual results may improve impressions of the manuscript (especially for non-expert readers)", "label": null}
{"identifier": "-GLNZeVDuik|||2|||0", "text": "The paper investigates normalizing flows for the general case of categorical data.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||1", "text": "The authors propose continuous encodings in which different categories correspond to unique volumes in a continuous latent space.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||2", "text": "Using the proposed Categorical Normalizing Flows (CNF) the authors present GraphCNF, a permutation-invariant normalizing flow on graph generation.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||3", "text": "In particular the paper presents a new approach for normalizing flow on categorical data.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||4", "text": "Then a three step generation approach is presented for graph generation with CNF.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||5", "text": "The paper is well written and the proposed solution is clearly presented in sections 2 and 3.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||6", "text": "The first results on molecule generation are very good when compared to those obtained with other approaches.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||7", "text": "The second experiment on graph coloring shows the validity of the proposed CNF even on an NP-hard optimization problem.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||8", "text": "Even the results on language modeling are encouraging.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||9", "text": "A final experiment shows how CNF can model discrete distributions precisely.", "label": null}
{"identifier": "-GLNZeVDuik|||2|||10", "text": "An experiment could be done on real-world datasets usually used for density estimation in order to assess the validity of CNF in therms of likelihood.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||0", "text": "Summary:", "label": null}
{"identifier": "-GLNZeVDuik|||3|||1", "text": "The paper considers the problem of modeling discrete distributions with normalizing flows.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||2", "text": "Authors propose a novel framework \u201cCategorical Normalizing Flows\u201d, i.e CNF.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||3", "text": "By jointly modeling a mapping to continuous latent space, and the likelihood of flows CNF solves some of the bottlenecks in current algorithms.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||4", "text": "With experiments on some synthetic domains, and benchmarking tasks like Zinc250, the authors empirically demonstrate that CNF-based algorithms perform competitively and often improve significantly on related approaches like Latent NF, discrete flows.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||5", "text": "Reasons for the score:\nI vote for accepting the paper, with minor improvements.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||6", "text": "The problem is well motivated, and the proposed algorithm improves significantly on previous approaches.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||7", "text": "I would encourage the authors to include generated samples/visualizations which may reflect some pathological failure modes of the algorithm.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||8", "text": "This would help with the readability of the paper, and improve understanding.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||9", "text": "Strengths:\n+", "label": null}
{"identifier": "-GLNZeVDuik|||3|||10", "text": "The CNF framework helps scaling normalizing-flow based approaches, and improves the stability and performance on standard benchmarks.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||11", "text": "+ GraphCNF outperforms baselines like GraphAF/GraphNVP on Zinc250 molecule generation. \n+", "label": null}
{"identifier": "-GLNZeVDuik|||3|||12", "text": "On language modelling tasks, CNF improves on Latent NFs and works competitively as an LSTM baseline.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||13", "text": "Weaknesses:\n- Plots such as comparing the training loss of different approaches would help understanding the stability of the learning algorithm.", "label": null}
{"identifier": "-GLNZeVDuik|||3|||14", "text": "- Adding some visualizations/samples from the learned models would help with clarity of section5.", "label": null}
{"identifier": "-Hs_otp2RB|||0|||0", "text": "Summary:", "label": null}
{"identifier": "-Hs_otp2RB|||0|||1", "text": "This work builds on the vulnerability of VAEs to adversarial attacks to propose investigate how training with alternative losses may alleviate this problem, with a specific focus on disentanglement.", "label": null}
{"identifier": "-Hs_otp2RB|||0|||2", "text": "In particular it is found that disentanglement constraints may improve the robustness to adversarial attacks, to the detriment of the performance.", "label": null}
{"identifier": "-Hs_otp2RB|||0|||3", "text": "In order to get the best of both, the author(s) propose a more flexible (hierarchical) model, trained with the beta-TC penalization on the ELBO.", "label": null}
{"identifier": "-Hs_otp2RB|||0|||4", "text": "The algorithm, named Seatbelt-VAE, shows improvement over the beta-TC VAE in terms of reconstruction, as well as in term of adversarial robustness for several datasets (Chairs, 3D Faces, dSprites).", "label": null}
{"identifier": "-Hs_otp2RB|||0|||5", "text": "Comments: \n1. The paper is well-written and the underlying reasoning is easy to follow.", "label": null}
{"identifier": "-Hs_otp2RB|||0|||6", "text": "2. The experiments are sound and well documented (results are reported across latent space dimensions, and adversarial attack parameters)", "label": null}
{"identifier": "-Hs_otp2RB|||0|||7", "text": "Questions:\n1. I am wondering how the bias of estimating the TC term on Z^L in Eq (8) scales with L and the minibatch size, compared to the more simple TC estimator from Chen et al. (2018) and if the author(s) had any evidence from the experiments that it might be problematic?", "label": null}
{"identifier": "-Hs_otp2RB|||0|||8", "text": "Does the algorithm require even larger batch sizes?", "label": null}
{"identifier": "-Hs_otp2RB|||0|||9", "text": "2. Should this approach be compared as well to weight decay or other simple regularization on the weights?", "label": null}
{"identifier": "-Hs_otp2RB|||0|||10", "text": "Minor questions:\n3. I wish the paper would make a stronger connection between disentanglement and robustness.", "label": null}
{"identifier": "-Hs_otp2RB|||0|||11", "text": "The beta-TC VAE is only one choice among other possible to constrain the variational network.", "label": null}
{"identifier": "-Hs_otp2RB|||0|||12", "text": "Did the authors ever try anything else?", "label": null}
{"identifier": "-Hs_otp2RB|||0|||13", "text": "4. Is it possible that the TC-VAE is effective at providing a defense against adversarial attacks in this manuscript because of the nature of the attack used in this manuscript (Eq. (1))?", "label": null}
{"identifier": "-Hs_otp2RB|||0|||14", "text": "If the attack was not based on the Kullback-Leibler divergence, but based on another geometry, maybe another disentanglement constraints would be more performant?", "label": null}
{"identifier": "-Hs_otp2RB|||1|||0", "text": "The paper considers the regularization of latent space toward achieving adversarial robustness against latent space attack.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||1", "text": "The paper demonstrates the applicability of disentanglement promoting VAEs for achieving adversarial robustness and further enhancing such VAEs by considering their hierarchical counterparts.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||2", "text": "The paper demonstrates their results in the benchmark datasets considered in the disentanglement and computer vision literature.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||3", "text": "The overall research direction pursued by this paper is exciting.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||4", "text": "However, I have some concerns, which include:", "label": null}
{"identifier": "-Hs_otp2RB|||1|||5", "text": "1. The paper attempts to establish the connection between disentanglement and robustness.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||6", "text": "The linkage, however, is not clear.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||7", "text": "In section 3, the paper argues for the smoothness of the encoder mapping and the decoder mapping.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||8", "text": "Toward this, the paper postulates additional regularization to enforce \"simplicity\" or \"noiseness\".", "label": null}
{"identifier": "-Hs_otp2RB|||1|||9", "text": "First of all, it is unclear how disentangled latent space helps achieve \"simplicity\" in the \"encode-decode process\".", "label": null}
{"identifier": "-Hs_otp2RB|||1|||10", "text": "Secondly, regarding \"noiseness\", it is not explained what extra would disentangled version of VAEs (e.g., TCVAE) provide compared to the standard setup.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||11", "text": "2. In section 3.2, the paper empirically demonstrates the connection between disentanglement and adversarial robustness.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||12", "text": "However, the evaluation carried out are not explicit.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||13", "text": "Firstly, to demonstrate the connection, the paper uses the attacker's achieved loss \\delta (from Eqn 1) as the metric.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||14", "text": "Although the \\delta is shown across different \\beta values, it is still unclear if disentanglement is directly related to robustness.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||15", "text": "Can the authors point out some disentanglement metrics (e.g., MIG) for each beta and compare MIG vs. \\delta?", "label": null}
{"identifier": "-Hs_otp2RB|||1|||16", "text": "Also, the curves are combined for all the d_z.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||17", "text": "What is the motivation behind doing that?", "label": null}
{"identifier": "-Hs_otp2RB|||1|||18", "text": "Because it has been known that disentanglement behavior is related to the dimension of latent space.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||19", "text": "Also, authors could consider decomposing the first term of \\delta for all the latent space dimensions and analyze if the disentangled dimensions are robust compared to the entangled dimension.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||20", "text": "This could be more helpful to establish the linkage.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||21", "text": "Secondly, authors have picked TCVAE considering \"reconstruction quality\" compared to \\beta-VAE, but in Fig 2 (right), ELBO is compared.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||22", "text": "Can the authors compare the reconstruction error?", "label": null}
{"identifier": "-Hs_otp2RB|||1|||23", "text": "Also, for fig 3, I think it is natural to see the comparison with \\beta-VAE.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||24", "text": "Why is such a comparison not included?", "label": null}
{"identifier": "-Hs_otp2RB|||1|||25", "text": "3. In section 4, for the motivation for the hierarchical TC-penalised VAEs, the paper states that \"TC-penalisation in single layer VAEs comes at the expense of model reconstruction quality\".", "label": null}
{"identifier": "-Hs_otp2RB|||1|||26", "text": "However, this directly contradicts the use of TC-VAE in the previous section.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||27", "text": "Although the results presented afterward support the authors' statement, the motivation must be clear and well written.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||28", "text": "The same comments for section 3.2 apply here too.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||29", "text": "4. The experimental results demonstrating protection against downstream tasks is performed using a simple 2-lear MLPs.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||30", "text": "This is different from the regular CNN network commonly considered for these datasets.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||31", "text": "Although this was meant to demonstrate the proposed model's efficacy, it would be more clear if the experimental setup is consistent with the current literature setting.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||32", "text": "Also, can the authors point out the initial results for the models before the attack?", "label": null}
{"identifier": "-Hs_otp2RB|||1|||33", "text": "Minor comments:", "label": null}
{"identifier": "-Hs_otp2RB|||1|||34", "text": "- There are a lot of grammatical errors and hard-to-follow sentences.", "label": null}
{"identifier": "-Hs_otp2RB|||1|||35", "text": "Some examples:\n    - \".. are not only even more ..\"\n    - \".. attack the models using methods outlined ..\".", "label": null}
{"identifier": "-Hs_otp2RB|||1|||36", "text": "But Eq (1) refers to only one method, right?\n    - \"\u2026 then \\delta too is small ..\"", "label": null}
{"identifier": "-Hs_otp2RB|||1|||37", "text": "(Update):", "label": null}
{"identifier": "-Hs_otp2RB|||1|||38", "text": "The score has been updated after a rebuttal from the authors.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||0", "text": "--Summary:", "label": null}
{"identifier": "-Hs_otp2RB|||2|||1", "text": "They proposed a robust method for the adversarial attack on VAE using a hierarchical version of $\\beta$-TCVAE and conduct analysis on the relationship between disentanglement and robustness to support their choice of approach.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||2", "text": "The experimental results demonstrate the effectiveness of the proposed defense method.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||3", "text": "--Strongness:\n1. The paper is well organized.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||4", "text": "2. They provide extensive analysis of their approach and provide proof.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||5", "text": "3. They demonstrate that the proposed method is more robust to other VAE baselines for the attacks.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||6", "text": "--Weakness:\n1. The qualitative results are not quite convincing.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||7", "text": "See below question 1.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||8", "text": "--Questions:\n1. Question on qualitative results:\n- For Figure 1, the adversarial examples for the three approaches are largely different.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||9", "text": "For the baselines -- VAE and $\\beta$-TCVAE, the provided inputs look not like just applying a small noise/distortion (which is the setting of adversarial attack) but have a huge difference from the original input.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||10", "text": "Therefore, the reconstructions can be expected that it won't look similar to the original reconstruction.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||11", "text": "- In Figure 6(c), are you using similar adversarial examples / similar amount of distortions to generate the reconstruction images for $\\beta$-TCVAE and your approach?", "label": null}
{"identifier": "-Hs_otp2RB|||2|||12", "text": "I'm not an expert in the adversarial attack domain, but shouldn't the adversarial examples be similar across different baselines?", "label": null}
{"identifier": "-Hs_otp2RB|||2|||13", "text": "2. Figure 4(b) top-right adversarial example seems not to be distorted (almost the same as the input).", "label": null}
{"identifier": "-Hs_otp2RB|||2|||14", "text": "Is it the adversarial example derived by applying distortion?", "label": null}
{"identifier": "-Hs_otp2RB|||2|||15", "text": "Since the distortion is large, I don't know why the adversarial looks just the same as the input.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||16", "text": "--Recommendation", "label": null}
{"identifier": "-Hs_otp2RB|||2|||17", "text": "In sum, the paper is well organized and consists of extensive theoretical proofs and experiments.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||18", "text": "While I'm not very convinced by the results.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||19", "text": "I vote for a more neutral score for now.", "label": null}
{"identifier": "-Hs_otp2RB|||2|||20", "text": "The authors are couraged to address these issues.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||0", "text": "The paper considers the problem of training VAEs which are robust to adversarial attacks.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||1", "text": "It shows that learning disentangled representations improves the robustness of VAE.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||2", "text": "However, this hurts the reconstruction accuracy.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||3", "text": "The paper then shows that using hierarchical VAEs can ensure robustness without sacrificing reconstruction.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||4", "text": "Strengths:", "label": null}
{"identifier": "-Hs_otp2RB|||3|||5", "text": "1. The problem considered is interesting and relevant.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||6", "text": "There is very little work on training robust VAEs---though they have been found to not be robust, and are also used for training robust classifiers downstream.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||7", "text": "2. The paper uncovers some interesting phenomenon about VAEs such as links between disentangled representations and robustness, and tradeoffs between disentanglement and reconstruction accuracy.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||8", "text": "3. The method also provides some protection for downstream classification tasks.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||9", "text": "Weaknesses:", "label": null}
{"identifier": "-Hs_otp2RB|||3|||10", "text": "1. There is a lack of baselines to compare against, and the paper has not really stress-tested the algorithm to ensure it is in fact robust.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||11", "text": "Most of the literature on adversarial examples does significantly more extensive testing to attempt to break the model.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||12", "text": "2. I found the story and the experimental evaluation a bit incomplete.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||13", "text": "Though the experiments demonstrate that the method achieves success in practice, the paper does not seem to sufficiently explore \"why\" it seems to work.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||14", "text": "I think this is a bit important here considering the previous point regarding baselines and testing.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||15", "text": "At the moment, it seems like the paper combines these two ideas of learning disentangled representation and hierarchical VAEs in a somewhat ad-hoc manner which ends up providing some robustness, but it is a bit opaque why this is supposed to work.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||16", "text": "Some experiments or ablation studies which establish a more direct link between disentangled representations and robustness will be helpful.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||17", "text": "Overall, I think this is a decent paper but I don't feel that I can advocate strongly for it since the scientific contribution seems a bit limited.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||18", "text": "------Updates after author response------", "label": null}
{"identifier": "-Hs_otp2RB|||3|||19", "text": "I thank the authors for the response and the new experiments.", "label": null}
{"identifier": "-Hs_otp2RB|||3|||20", "text": "In light of the clarifications and additional evaluations, I have increased my score to 7.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||0", "text": "This paper studies the universal approximability of residual networks.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||1", "text": "The main contribution of the paper is to prove that residual networks of width 2n can approximate any continuous function from $\\mathbb R^n$ to $\\mathbb R^n$ in arbitrary uniform error, for activation functions satisfying some differential equation.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||2", "text": "It was interesting for me that only two values of weights suffice for the universal approximation of residual networks.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||3", "text": "I think that this result can be used to prove the expressive power of compressed/pruned networks.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||4", "text": "However, I think that there exists a critical weakness in the connection with the existing universal approximation results.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||5", "text": "As the authors mentioned in the related work section, (Kidger & Lyons; 2020) proved that feed-forward networks of width n+m+1 can approximate any continuous function from $\\mathbb R^n$ to $\\mathbb R^m$ in arbitrary uniform error, for general activation functions.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||6", "text": "In my opinion, this result can be easily extended to residual networks since residual networks can approximate by any feed-forward network by choosing some large s and small W in Eq. (3.1), i.e., ignore the residual connections.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||7", "text": "If this approximation is valid, the main contribution given by Theorem 4.4 is significantly weakened.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||8", "text": "Due to the weakness I mentioned, my decision tends toward rejection.", "label": null}
{"identifier": "-IXhmY16R3M|||0|||9", "text": "------------------------------------------------------------------------------------\nafter reading the authors' response and revision, I raise my score to 6.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||0", "text": "The authors consider residual networks.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||1", "text": "They interpret a residual network as a discrete analogue of some dynamical control system.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||2", "text": "Using the results of the control-theoretic problem of controllability they proved the universal approximation property of the deep neural networks.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||3", "text": "The universal approximation property is rather weak; it is usually true for wide classes of models and activation functions.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||4", "text": "For example, in the case of single-layer networks, it is sufficient that the activation function is not a polynomial, see Pinkus, Allan.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||5", "text": "\"Approximation theory of the MLP model in neural networks.\"", "label": null}
{"identifier": "-IXhmY16R3M|||1|||6", "text": "Acta numerica 8.1 (1999): 143-195.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||7", "text": "In this paper, the universal approximation property is proved for a very specific class of activations (satisfying a certain diffur).", "label": null}
{"identifier": "-IXhmY16R3M|||1|||8", "text": "It is not clear why this is a significant requirement.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||9", "text": "The authors mention that it is possible to extend the results to other activation functions, and explain how to do this for ReLU, but what to do with other activations is still unclear.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||10", "text": "Of course, approximation using a dynamic system is a more complex model than a conventional single-layer network, but this requirement looks exotic.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||11", "text": "So, the authors consider a special type of a complex nonlinear model, parametrized by an infinite-dimensional control, for which the natural and rather weak property is proved.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||12", "text": "The proof technology itself is quite interesting and is based on the ideas of Lie algebras.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||13", "text": "But this is all hidden in the Appendix; if a person reads only the main text, all these ideas are not visible, and the results are not particularly impressive themselves.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||14", "text": "Authors should somehow emphasize their new ideas in the main text, or at least make it more interesting.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||15", "text": "References to literature are quite relevant in the work and the related work section is OK.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||16", "text": "As a conclusion I can say that\n- the results itself is rather natural and weak.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||17", "text": "Nowadays for residual networks we have significantly stronger results about their approximation capabilities, see e.g. Dmitry Yarotsky.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||18", "text": "\u201cOptimal approximation of continuous functions by very deep ReLU networks.\u201d", "label": null}
{"identifier": "-IXhmY16R3M|||1|||19", "text": "https://arxiv.org/pdf/1802.03620.pdf", "label": null}
{"identifier": "-IXhmY16R3M|||1|||20", "text": "- possibly, the technique, used for the proof is a new interesting tool itself, so that it can be used to solve other similar problems.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||21", "text": "However, the authors did not mention anything about this in the main text of the paper, and for me it is difficult to assess the novelty as I am not the specialist in the control-theoretic problem of controllability, results of Agrachev et al., etc.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||22", "text": "Therefore, I think that in the current state the paper can not be accepted for a publication.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||23", "text": "=========", "label": null}
{"identifier": "-IXhmY16R3M|||1|||24", "text": "After reading authors comments.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||25", "text": "- Well, still the Point 1 is generally not true in my opinion.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||26", "text": "The authors cited the work of Kidger & Lyons \"Universal approximation with deep narrow networks\", in which universal approximation is proved for a wide class of activation functions and long narrow networks.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||27", "text": "Moreover, if you do not strive to make the network as narrow as possible, then it is easy to convert a shallow wide network into a deep narrow one (but not vice versa).", "label": null}
{"identifier": "-IXhmY16R3M|||1|||28", "text": "Point 2 seems quite possible.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||29", "text": "Point 3 coincides with what I mentioned --- the authors proved UAP for several examples of activations, although really quite important ones.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||30", "text": "- \"Results will be accessible without the need for the technicalities\".", "label": null}
{"identifier": "-IXhmY16R3M|||1|||31", "text": "Well, I personally do not understand this statement in the conference article.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||32", "text": "From the main text, the reader only sees that a relatively basic property (universal approximation) is fulfilled for a couple of previously uncovered activation functions (or for a specialized class), and it is not clear why, since possibly new ideas are not in the main text.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||33", "text": "- This work clearly has strengths and weaknesses, which, in principle, are more or less clear.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||34", "text": "On the one hand, the universality property for approximations by dynamical systems and a specific class of activation functions is proved.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||35", "text": "How important, new and interesting is it?", "label": null}
{"identifier": "-IXhmY16R3M|||1|||36", "text": "It doesn't seem to be super strong result, because this property is relatively weak, completely expected, and has already been proven many times before in various situations (wide shallow networks, narrow deep networks, dynamic systems with some classes of activation functions).", "label": null}
{"identifier": "-IXhmY16R3M|||1|||37", "text": "In approximation theory, people have been studying more advanced questions like approximation rates for a long time.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||38", "text": "I also do not see any significant practical use of the results.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||39", "text": "On the other hand, it is possible that the authors have developed some new method when proving their results.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||40", "text": "But in order to check this fact the reviewer should check the whole appendix.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||41", "text": "If an article is submitted to a journal, the reviewer is expected to read full article, including the Appendix.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||42", "text": "But for this conference work, as far as I understand, this is not the case, the reviewer is not required to read the Appendix.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||43", "text": "Accordingly, if the authors could not convince the reviewer of the importance of the work within the main text, then this is their problem.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||44", "text": "As for the individual points that the authors wrote about - as I have already written, part of what they wrote is quite fair, no one disputes that they have some new results.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||45", "text": "At the same time, for example, I find it difficult to agree with point 1, as I have already written.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||46", "text": "- Well, in principle, there are two ideas that the authors mentioned (about just two values for all weights and memorising data), which are relatively interesting indeed.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||47", "text": "This is not something that would be mega-important, but it looks like non-obvious facts, which is a plus.", "label": null}
{"identifier": "-IXhmY16R3M|||1|||48", "text": "In principle, I can increase the grade for one point.", "label": null}
{"identifier": "-IXhmY16R3M|||2|||0", "text": "Pros:\n  [1]", "label": null}
{"identifier": "-IXhmY16R3M|||2|||1", "text": "The paper is well-written, clear and organized well. \n  [2]", "label": null}
{"identifier": "-IXhmY16R3M|||2|||2", "text": "The approach of relating DNN universal approximation problem to controllability of ensemble of control systems is interesting. \n  [3]", "label": null}
{"identifier": "-IXhmY16R3M|||2|||3", "text": "The results are a bit more general than existing literature in the same topic.", "label": null}
{"identifier": "-IXhmY16R3M|||2|||4", "text": "Main concerns:", "label": null}
{"identifier": "-IXhmY16R3M|||2|||5", "text": "The paper contains some interesting ideas and new results, but it seems it is more an incremental work to existing literature and the contribution might be a bit limited and not significant enough.", "label": null}
{"identifier": "-IXhmY16R3M|||2|||6", "text": "From empirical sense, it has been well observed that such deep residual neural networks has very good approximation capabilities to almost arbitrary functions.", "label": null}
{"identifier": "-IXhmY16R3M|||2|||7", "text": "From theoretic sense, the existing relevant work by Li et al., 2019 has already established very similar results (also through dynamic system and control theory) for universal approximation capability of such deep residual neural networks, but just in the sense of L-p norm rather than L-infinity norm (where p can be any number between 1 and infinity).", "label": null}
{"identifier": "-IXhmY16R3M|||2|||8", "text": "Although the authors argue that L-infinity norm is more general than such L-p norm result, the contribution does seems quite incremental to me (especially considering that the p can be any arbitrary large number between 1 and infinity), and I'm not very convinced by the significance of the contribution, in both theoretic sense as well as practical sense.", "label": null}
{"identifier": "-IXhmY16R3M|||2|||9", "text": "To be clear, I do think that the approach in this paper is somewhat interesting and inspiring and it also contains some valuable and publishable results, but I just not convinced that the contribution is significant enough to be published as a regular paper in such a top conference like ICLR.", "label": null}
{"identifier": "-IXhmY16R3M|||2|||10", "text": "-------", "label": null}
{"identifier": "-IXhmY16R3M|||2|||11", "text": "After reading the author's responses, I'm ok to promote the rating from 5 to 6.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||0", "text": "The paper gives a fresh perspective to a standard question in neural networks concerning their power to represent or approximately represent continuous functions.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||1", "text": "Here they focus on  deep residual neural networks where the activation units satisfy certain conditions defined via a differential equation (most common units in practice abide by this property) and present L_\\infinity approximation results which is the strongest possible sense of approximating a function.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||2", "text": "The paper uses novel ideas and techniques stemming from dynamical systems and the crux of the argument relies on viewing residuals neural nets as a control system that one can influence via the weight assignments.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||3", "text": "Using the weights, one can try to obtain specific values as the network's output and hence the universal approximation property follows.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||4", "text": "Several ideas along those lines have been previously exploited in related work as the authors state.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||5", "text": "The reviewer believes there is technical novelty in the paper and also the result is quite elegant.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||6", "text": "Also, highlighting new connections to dynamical systems has recently been a fruitful direction.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||7", "text": "Parts of the techniques establish some \"expansion\" property of the associated control system implying that for any given point in R^d, the dynamical system can be made to pass through it given the appropriate feedback loop.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||8", "text": "This holds true even for networks with simple\narchitectures and if the network's parameters take only two distinct values.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||9", "text": "The paper overall offers nice theory for an important question although the practical implications seem limited.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||10", "text": "Other comments:\nStating the result for functions R^n to R^n isn't it a bit misleading?", "label": null}
{"identifier": "-IXhmY16R3M|||3|||11", "text": "Isn't it true that once the result is established for f: R^n to R, then it can directly extend to higher dimension as well?", "label": null}
{"identifier": "-IXhmY16R3M|||3|||12", "text": "In terms of presentation, there are several ways to improve: it would be better to define separately what the activation units should be (satisfying the differential equations) and then use it in all places where it is needed.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||13", "text": "Currently, in most statements for Corollaries and Theorems the authors devote 2-3 lines of repeating information which waste space and make the theorems look ugly.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||14", "text": "Also, the authors should clarify more what parts of the approach via Lie algebras is novel and how it is compared specifically with prior works.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||15", "text": "The reviewer (and potentially most people in ML) are not familiar with such techniques and currently it feels it may be an incremental result.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||16", "text": "Moreover, the paper uses ideas from dynamical systems to study approximation capabilities and related works missing from the discussion are https://arxiv.org/abs/2003.00777 and https://arxiv.org/abs/1912.04378 , where depth separations are provided.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||17", "text": "A similar property holds for those networks as the one described above where only two distinct parameters are used for the weights of the network and it would be interesting to see if your results also can be applied to obtain depth separations.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||18", "text": "The paper in https://arxiv.org/abs/1602.04485 also offers representation benefits via general activations called semi-algebraic.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||19", "text": "What is the connection among those gates and your differential equations?", "label": null}
{"identifier": "-IXhmY16R3M|||3|||20", "text": "A discussion would be appreciated.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||21", "text": "Concerning the Lie algebras presentation, some high-level discussion on how exactly it is used without delving into the technical details for non-experts would also be greatly appreciated and help others build on your work.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||22", "text": "Currently, several parts of the paper read off as a bit mysterious for non-experts.", "label": null}
{"identifier": "-IXhmY16R3M|||3|||23", "text": "Minor Typo:\np1: to best of our knowledge", "label": null}
{"identifier": "-J9xYzP2HD|||0|||0", "text": "Previous meta-learning approaches typically focus on tasks that share the same input types, e.g. images.", "label": null}
{"identifier": "-J9xYzP2HD|||0|||1", "text": "This paper addresses the problem of meta-learning weight initialization across tasks with different types of input features.", "label": null}
{"identifier": "-J9xYzP2HD|||0|||2", "text": "It proposes Chameleon model that learns to align input features from different tasks by learning a permutation matrix for each task, and shows that Chameleon can successfully learn good initialization.", "label": null}
{"identifier": "-J9xYzP2HD|||0|||3", "text": "Strength:\n-", "label": null}
{"identifier": "-J9xYzP2HD|||0|||4", "text": "It identifies and tackles a new important problem in meta-learning: meta-learning on tasks with different input features.", "label": null}
{"identifier": "-J9xYzP2HD|||0|||5", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||0|||6", "text": "The proposed approach is simple but shows improvements over the baseline method.", "label": null}
{"identifier": "-J9xYzP2HD|||0|||7", "text": "Weakeness:\n- Supervised training for the permutation matrix is necessary for the model to perform well.", "label": null}
{"identifier": "-J9xYzP2HD|||0|||8", "text": "- Experimental results section can be more detailed.", "label": null}
{"identifier": "-J9xYzP2HD|||0|||9", "text": "Given that Algorithm 2 is the major part of the method, how is the reordering training procedure constructed?", "label": null}
{"identifier": "-J9xYzP2HD|||0|||10", "text": "How is the target permutation matrix determined?", "label": null}
{"identifier": "-J9xYzP2HD|||0|||11", "text": "are there / what are the shared features between different tasks?", "label": null}
{"identifier": "-J9xYzP2HD|||0|||12", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||0|||13", "text": "Would be great if experiments are done on one or two more datasets to strengthen the result.", "label": null}
{"identifier": "-J9xYzP2HD|||0|||14", "text": "Additional Comments:\n-", "label": null}
{"identifier": "-J9xYzP2HD|||0|||15", "text": "How many features are used?", "label": null}
{"identifier": "-J9xYzP2HD|||0|||16", "text": "How would the performance change if there are more/fewer features?", "label": null}
{"identifier": "-J9xYzP2HD|||0|||17", "text": "- typo: Equation (9) is mentioned several times", "label": null}
{"identifier": "-J9xYzP2HD|||0|||18", "text": "I believe this paper proposed a new interesting problem in meta-learning and provided a simple effective model to address the problem.", "label": null}
{"identifier": "-J9xYzP2HD|||1|||0", "text": "Chameleon: Learning Model Initializations Across Tasks With Different Schemas", "label": null}
{"identifier": "-J9xYzP2HD|||1|||1", "text": "The paper provides an interesting direction in the few-shot classification field.", "label": null}
{"identifier": "-J9xYzP2HD|||1|||2", "text": "In particular, it proposes a model that learns to align different predictor schemas to a common representation.", "label": null}
{"identifier": "-J9xYzP2HD|||1|||3", "text": "The paper also demonstrates how current meta-learning approaches can successfully learn a model initialisation across tasks with different schemas as long as they share some variables with respect to their type or semantics.", "label": null}
{"identifier": "-J9xYzP2HD|||1|||4", "text": "The paper takes on an interesting facet of few-shot classification:", "label": null}
{"identifier": "-J9xYzP2HD|||1|||5", "text": "An encoder model that aligns to different predictor schemas to a common representation.", "label": null}
{"identifier": "-J9xYzP2HD|||1|||6", "text": "It tackles the problem by using 1D convolution (three of them) to transform the input features to the K-features target space and learning the alignment from the data itself.", "label": null}
{"identifier": "-J9xYzP2HD|||1|||7", "text": "Comprehensive experiments have been done with quantitative results and analysis, to show the effectiveness of the proposed approach and the results are convincing and the code is provided to determine the reproducibility of the results.", "label": null}
{"identifier": "-J9xYzP2HD|||1|||8", "text": "Overall performance is quite good however, it would be a good study to have an analysis of the different datasets as to how balanced/unbalanced they are, how it affects the performance, the nature of the features etc.", "label": null}
{"identifier": "-J9xYzP2HD|||1|||9", "text": "Also, I would like the author to discuss how suitable/adaptable this approach will be for multi-label tasks and what kind of modifications (if any) are to be made.", "label": null}
{"identifier": "-J9xYzP2HD|||1|||10", "text": "The idea of encoding different predictor schemas to a common representation is quite interesting and comprehensive experiments and supporting ablation study has been made.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||0", "text": "- Summary and contributions\n    -", "label": null}
{"identifier": "-J9xYzP2HD|||2|||1", "text": "In this work, the authors tried to solve the problem of ``heterogeneous'' meta-learning where each task resides in a different feature space from the other tasks.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||2", "text": "They introduced a feature transformation or re-ordering matrix to align the features.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||3", "text": "While I agree with the authors that this problem is of significance in the meta-learning community, the solution in this work, depending on the ground-truth of re-ordering matrix, is trivial and impractical.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||4", "text": "- Strengths: \n    -", "label": null}
{"identifier": "-J9xYzP2HD|||2|||5", "text": "The problem investigated in this paper, i.e., meta-learning tasks in heterogeneous feature spaces, is important to the field of meta-learning.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||6", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||2|||7", "text": "The paper is well written and easy to follow.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||8", "text": "- Weaknesses:\n    -", "label": null}
{"identifier": "-J9xYzP2HD|||2|||9", "text": "The primary concern about this paper is its technical contribution, being limited and impractical.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||10", "text": "To align tasks in incommensurable feature spaces, projecting them into a common feature space has been a common practice.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||11", "text": "Please kindly see related works on heterogeneous transfer learning.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||12", "text": "The major challenge lies in the supervision needed to train the alignment matrix or function.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||13", "text": "The ground-truth feature alignment matrix is almost impractical to collect, if the dimension of features is super large and we have no knowledge of the semantic correspondence between two features from two tasks.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||14", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||2|||15", "text": "The empirical results are also not convincing.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||16", "text": "- Why is only Glorot initialization compared in Figure 3?", "label": null}
{"identifier": "-J9xYzP2HD|||2|||17", "text": "What has been widely adopted is some better initialization strategies, including (He initialization).", "label": null}
{"identifier": "-J9xYzP2HD|||2|||18", "text": "- From both Figure 3 and Figure 4, and also the results in Appendix C, I see little improvement of the proposed over Frozen.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||19", "text": "This means that most benefits of the feature alignment come from the supervised training part where a ground-truth alignment matrix is required to train $\\Phi$, while the matrix is even infeasible to have in practical settings.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||20", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||2|||21", "text": "In Line 6 of the section \"Ablations\", the authors mentioned that features 2 and 3 are showing a strong correlation, but I cannot see why in Figure 6.", "label": null}
{"identifier": "-J9xYzP2HD|||2|||22", "text": "Maybe it is features 2 and 4?", "label": null}
{"identifier": "-J9xYzP2HD|||2|||23", "text": "- Minor:\n    Line 2 in Section 4: Equation (9) does not exist\u2026", "label": null}
{"identifier": "-J9xYzP2HD|||3|||0", "text": "Summary\n--------------", "label": null}
{"identifier": "-J9xYzP2HD|||3|||1", "text": "The paper proposes a trainable way to re-order or recover the ordering of features from sets of examples, and use it as a way to build a common feature space (or embedding) for a neural net, the (initial) parameters of which can be trained by Reptile.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||2", "text": "Experiments show that such initial parameters enable faster training (inside of an episode) than untrained weights.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||3", "text": "Pros\n------\n-", "label": null}
{"identifier": "-J9xYzP2HD|||3|||4", "text": "The paper shows it is possible to recover information about the identity of coordinates in the input space, through a learned transformation, on several unstructured datasets.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||5", "text": "The similarity between such representations of individual coordinates can help identify similar features, either in a given dataset or across datasets.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||6", "text": "Cons\n--------", "label": null}
{"identifier": "-J9xYzP2HD|||3|||7", "text": "The paper is overall really hard to follow, statements are often confusing or misleading.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||8", "text": "For instance:\n- The introduction suggests a multi-modal learning paradigm, where different tasks could have access to data in different input spaces, some of them common.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||9", "text": "However, the paper then seems to consider individual coordinates in the input space only, and focuses on mapping shuffled subsets of these coordinates back to their initial position.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||10", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||3|||11", "text": "There is confusion about the \"tasks\", which sometimes correspond to one of the OpenML datasets, and sometimes to individual few-shot episodes from one of these datasets.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||12", "text": "- Concepts like \"schema\" and \"predictors\" are never properly introduced or defined.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||13", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||3|||14", "text": "The description of the \"chameleon\" (alignment) component mentions \"order-invariant\" and \"permutation invariant\" several times, but it is quite unclear whether it refers to the the order of the examples within the data set (or episode) or the order in which the features are represented.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||15", "text": "The paper uses few-shot learning vocabulary and techniques, including Reptile, but the methodology seems completely different from the few-shot learning literature.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||16", "text": "In particular:\n- There does not appear to be a split between meta-training and meta-test classes within a dataset, or meta-training datasets and meta-testing ones, except for the EMNIST experiment.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||17", "text": "Even then, the pre-training of the \"chameleon\" alignment module seems to involve using examples of the meta-test classes.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||18", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||3|||19", "text": "The reported evaluation metric is really unusual: they report the improvement (and sometimes accuracy) after 3 steps of gradient descent from within an episode, which is somewhat related to the quality of the meta-learned weights, but no other metric that would be comparable to existing literature, which makes it especially hard to assess the results.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||20", "text": "The principle of the alignment module seems similar to (soft) attention mechanisms, in that there is a softmax trained to highlight which parts of an input vector should be emphasized (or selected) at a given point in the processing (here, in the aligned feature space).", "label": null}
{"identifier": "-J9xYzP2HD|||3|||21", "text": "However, the literature on attention is not reviewed.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||22", "text": "Many design choices are not addressed clearly, neither in how they were made, or the impact of these choices, especially regarding the architecture of the alignment module:\n- It is a linear transformation (before the softmax), though parameterized by 3 matrices.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||23", "text": "An alternative would have been a 3-layer neural net, similar to attention networks.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||24", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||3|||25", "text": "The parameterization of the first matrix makes the number of parameters depend on N, the number of examples in a given task.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||26", "text": "This could be quite limiting to be restrained to tasks of exactly N examples, especially if both the support (mini-train) and query (mini-test or valid) parts of an episode need to have exactly N examples.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||27", "text": "-", "label": null}
{"identifier": "-J9xYzP2HD|||3|||28", "text": "There is also no discussion of the  value or impact of or K, the size of the chosen embedding space).", "label": null}
{"identifier": "-J9xYzP2HD|||3|||29", "text": "Recommendation\n--------------------------", "label": null}
{"identifier": "-J9xYzP2HD|||3|||30", "text": "I recommend to reject this submission.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||31", "text": "Arguments\n------------------", "label": null}
{"identifier": "-J9xYzP2HD|||3|||32", "text": "The main idea in the paper, learning alignments of various input spaces into a common embedding space through an attention mechanism, has merit and may  work reasonably.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||33", "text": "However, both the algorithm and the experimental set up are described in a quite confused way, and not well justified or grounded.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||34", "text": "The reported results are not comparable with few-shot learning literature, nor multi-modal training or feature imputation, and do not make a convincing case.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||35", "text": "Questions", "label": null}
{"identifier": "-J9xYzP2HD|||3|||36", "text": "---------------", "label": null}
{"identifier": "-J9xYzP2HD|||3|||37", "text": "As I understand it, the \"Chameleon\" architecture itself simply consists in 3 matrix multiplications (Nx8, 8x16, 16xK), which would be equivalent to the length-1 1D convolutions, is that correct?", "label": null}
{"identifier": "-J9xYzP2HD|||3|||38", "text": "It may be more straightforward to explain that way, as $enc(X) = X M_1 M_2 M_3 X^T$.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||39", "text": "Also, should the 2nd and 3rd convolutions be labeled \"8x16x1\" and \"16xKx1\" respectively?", "label": null}
{"identifier": "-J9xYzP2HD|||3|||40", "text": "As far as I can tell, only the first Conv1D should have a dependency on N.", "label": null}
{"identifier": "-J9xYzP2HD|||3|||41", "text": "Additional feedback", "label": null}
{"identifier": "-J9xYzP2HD|||3|||42", "text": "---------------------------", "label": null}
{"identifier": "-J9xYzP2HD|||3|||43", "text": "In Figure 2, the \"reshape\" operation should be \"transpose\" instead.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||0", "text": "Summary:", "label": null}
{"identifier": "-J9xYzP2HD|||4|||1", "text": "This paper aims to perform meta-learning across tasks that have different input data types by learning separate task-specific encoders, and then aligning the features produced by these encoders before making predictions.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||2", "text": "Pros:\nSharing information across tasks with different input types is a relevant problem\nCons:\nPrecise problem statement and method very unclear\nExperiments are only on toy datasets", "label": null}
{"identifier": "-J9xYzP2HD|||4|||3", "text": "Detailed Comments:", "label": null}
{"identifier": "-J9xYzP2HD|||4|||4", "text": "It is not clear from the abstract / introduction what is meant by \u201cschema.\u201d", "label": null}
{"identifier": "-J9xYzP2HD|||4|||5", "text": "From the abstract: \u201cfor example, if the number of predictors varies across tasks, while they still share some variables.\u201d", "label": null}
{"identifier": "-J9xYzP2HD|||4|||6", "text": "Does this refer to the number of classes in a few-shot problem?", "label": null}
{"identifier": "-J9xYzP2HD|||4|||7", "text": "What variables are shared?", "label": null}
{"identifier": "-J9xYzP2HD|||4|||8", "text": "Classes, or input features?", "label": null}
{"identifier": "-J9xYzP2HD|||4|||9", "text": "Later in the intro: \u201ctraining a single model across different tasks is only feasible if all tasks share the same schema, meaning that all instances share one set of features in identical order.\u201d", "label": null}
{"identifier": "-J9xYzP2HD|||4|||10", "text": "These definitions of schema do not seem to be the same.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||11", "text": "Schema also does not seem to be defined in Section 3.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||12", "text": "At the beginning of that section it says, \u201cevery task has to share the same schema of common size K\u201d which seems to indicate \u201cschema\u201d is the number of features and then a few lines later, \u201c tasks with varying input schema and feature length F\u201d which seems to indicate \u201cschema\u201d is *not* the number of features.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||13", "text": "In the related work section, few-shot learning did not begin in 2017 as might be suggested by the citations.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||14", "text": "It would be good to recognize the earlier works in this area, such as \nFei-Fei, L. et al.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||15", "text": "A bayesian approach to unsupervised one-shot learning of object categories.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||16", "text": "2003\nA Bayesian framework for concept learning.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||17", "text": "PhD thesis, Massachusetts Institute of Technology, 1999.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||18", "text": "For few-shot learning with deep learning, Matching Networks should arguably be cited: Vinyals, Oriol, et al.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||19", "text": "Matching networks for one shot learning.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||20", "text": "2016.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||21", "text": "The original MAML paper actually proposed the first-order version of MAML, Nichol et al. was not the first to propose this.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||22", "text": "I don\u2019t understand how the method works when the features are learned and not given.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||23", "text": "For example, the encoder for EMNIST-Digits produces 32 features, while the encoder for EMNIST-Letters produces 64 features.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||24", "text": "If the meta-training tasks are drawn from only EMNIST-Digits, then how can the \u201cre-ordering\u201d matrix be learned from EMNIST-Digits such that it can re-order features from EMNIST-Letters?", "label": null}
{"identifier": "-J9xYzP2HD|||4|||25", "text": "At the most basic level, based on Figure 2, the matrix \\Pi would have to have different dimensionality for each dataset.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||26", "text": "Even if they were the same dimensionality, how is the feature ordering supervision performed in this case?", "label": null}
{"identifier": "-J9xYzP2HD|||4|||27", "text": "In the \u201cmain results\u201d, if you sub-sample features, how do you know that the sub-sampled features have enough information to perform the classification task?", "label": null}
{"identifier": "-J9xYzP2HD|||4|||28", "text": "It would be helpful to have an experiment on a less-toy dataset, both to demonstrate that the problem of \u201cmis-aligned features\u201d exists in more complex data, and that the method can address it.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||29", "text": "Overall, this paper is extremely confusing.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||30", "text": "I do not understand the problem statement or how the method is trained in the learned feature case.", "label": null}
{"identifier": "-J9xYzP2HD|||4|||31", "text": "In my view, the clarity of this paper needs to be significantly improved to consider acceptance.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||0", "text": "The paper presents a framework that combines 1) multi-view prediction for 3D reconstruction from a single image and 2) content-style disentangled representation learning using instance-norm-based auto-encoders.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||1", "text": "It aims at learning disentangled 3D representation of input images.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||2", "text": "The authors show that these methods can be applied to few-shot recognition, visual question-answering (VQA), view prediction, and image generation from texts.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||3", "text": "The paper is well-written.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||4", "text": "It's impressive that the authors were able to build such an integrated system with so many modules.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||5", "text": "The experiments are quite extensive.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||6", "text": "My concerns are mostly about experimental evaluation.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||7", "text": "There are some important studies missing, and other results not well-justified.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||8", "text": "First, some important ablation studies are missing, especially in Section 4.1.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||9", "text": "The authors have used three different techniques to improve the few-shot recognition performance: 1) multi-view predictive learning, 2) content-style disentanglement, and 3) optimization-based prototypical network (the authors used gradient descent to find the rotation of the observation w.r.t. the prototype).", "label": null}
{"identifier": "-Lr-u0b42he|||0|||10", "text": "However, the only ablation study available in Table 1 is the 3DP-Net, which uses only (1) multi-view predictive learning.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||11", "text": "Moreover, I suggest authors test the performance of other algorithms for few-shot recognition, for example, a very simple baseline algorithm from Chen (2020, https://arxiv.org/abs/2003.04390 ).", "label": null}
{"identifier": "-Lr-u0b42he|||0|||12", "text": "Is the proposed representation algorithm only working with prototypical networks?", "label": null}
{"identifier": "-Lr-u0b42he|||0|||13", "text": "Second, Table 2 also misses an important baseline.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||14", "text": "Looking at the in-domain test set, we have seen that NSCL-2D has a better performance than D3DP -- the authors explained that this is because the NSCL-2D uses ImageNet pretraining.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||15", "text": "What will be the performance of NSCL without ImageNet pretraining?", "label": null}
{"identifier": "-Lr-u0b42he|||0|||16", "text": "On the one-shot generalization test split: what will be the performance of D3DP without 3D shape primitive and without shape/style disentanglement?", "label": null}
{"identifier": "-Lr-u0b42he|||0|||17", "text": "It seems that even with one of these two features, the proposed D3DP still has worse performance than the 2D network.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||18", "text": "It'd be great to hear from the authors if there is any intuition on the result.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||19", "text": "I also have serious concerns about the generality and applicability of the claimed shape-style disentanglement.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||20", "text": "From the results in Table 1 (comparing D3DP and 3DP) and Table 2 (comparing D3DP w/. and w/o. style/content disentanglement), it seems that the shape-style disentanglement has shown great importance in few-shot learning settings.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||21", "text": "But, in \"style features\", how should materials and colors be disentangled; in \"content features\", how should object textures and shapes be disentangled?", "label": null}
{"identifier": "-Lr-u0b42he|||0|||22", "text": "The limitation on generality will greatly restrict the application of this framework in real-world images.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||23", "text": "This is also related to another minor comment about the setup of few-shot style recognition in Table 1.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||24", "text": "In CLEVR, there are 8 colors and 2 materials.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||25", "text": "So the \"16 style classes\" in the paper indicates that the authors are treating pairs of (color, material) as the label for objects.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||26", "text": "Such a design will not scale up wrt the type of attributes.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||27", "text": "I'd love to hear more justifications from authors.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||28", "text": "And this should be more clearly indicated in the main paper as well.", "label": null}
{"identifier": "-Lr-u0b42he|||0|||29", "text": "Update: I appreciate the response and have adjusted my score accordingly.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||0", "text": "This paper describes an approach that learns a disentangled shape and style representation of objects in a self-supervised way from RGB-D images.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||1", "text": "The approach is based on various components, like a 3D feature volume, a bounding box detector, and a disentanglement network.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||2", "text": "Neural rendering (e.g. recomposing the various disentangled parts into an image) is used as the learning signal for self-supervision.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||3", "text": "Various applications of this representation  are shown, examples are few-shot shape learning and Visual Question Answering.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||4", "text": "--- Strengths ---", "label": null}
{"identifier": "-Lr-u0b42he|||1|||5", "text": "Some applications that are enabled by the representations are very interesting.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||6", "text": "Disentangling style and shape for example allows to detect object independently of style (something easily done by humans) and generation of scenes from language utterances.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||7", "text": "Overall, it seems like a good direction to try to go for a full 3D(+style) representation to get more flexible and general models.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||8", "text": "The experiments indicate that the main concepts help in the down-stream tasks.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||9", "text": "--- Weaknesses ---", "label": null}
{"identifier": "-Lr-u0b42he|||1|||10", "text": "Only synthetic samples are shown.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||11", "text": "Since the approach is fully self-supervised, it should be possible to apply it to real images.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||12", "text": "The authors show quantitative results on one, apparently, real dataset \"Real veggies\", but no qualitative samples are shown to give the reader an indication on how realistic this dataset is.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||13", "text": "Is it realistic?", "label": null}
{"identifier": "-Lr-u0b42he|||1|||14", "text": "If not, what is preventing applying this approach to more realistic images and objects?", "label": null}
{"identifier": "-Lr-u0b42he|||1|||15", "text": "The other propose a very elaborate, modular pipeline.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||16", "text": "While this modularity does explicitely enable some of the applications that are shown, it also could be a weakness, as some modules present single points of failure which might be hard to recover from (for example when the 3D object detector fails).", "label": null}
{"identifier": "-Lr-u0b42he|||1|||17", "text": "This could be alleviated by either showing results on realistic data and/or a controlled studies of how robust the approach is the to failures of individual parts (e.g. what happens when the detector is inaccurate?).", "label": null}
{"identifier": "-Lr-u0b42he|||1|||18", "text": "The paper is a bit hard to read because it is crowded.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||19", "text": "I suspect that it due to he many concepts that are introduced.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||20", "text": "The  main contribution clearly is the self-supervised learning of the representation, but a sizeable part of the paper discusses down-stream applications (which are non-trivial and are thus not discussed in sufficient detail), while other parts (for example the feature selection) would require a bit more space to be easier to understand.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||21", "text": "--- Other ---", "label": null}
{"identifier": "-Lr-u0b42he|||1|||22", "text": "The example for one-shot scene generation in Figure 1 is exceptionally clean.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||23", "text": "Is this a real example that was rendered by the proposed approach?", "label": null}
{"identifier": "-Lr-u0b42he|||1|||24", "text": "Are you planning to release the code?", "label": null}
{"identifier": "-Lr-u0b42he|||1|||25", "text": "An elaborate system like this would benefit strongly from this.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||26", "text": "I suspect that the submission is severely over length.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||27", "text": "Starting from page 5, the font size is much smaller than the default font size.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||28", "text": "Typos: page 3: \"operation operation\"", "label": null}
{"identifier": "-Lr-u0b42he|||1|||29", "text": "-- Summary ---", "label": null}
{"identifier": "-Lr-u0b42he|||1|||30", "text": "The proposed representation is interesting, but the huge number of applications makes the paper inaccessible.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||31", "text": "I'd argue it would be beneficial to explore and understand the representation and pipeline on a more fundamental level (i.e. can it be learned on real images?", "label": null}
{"identifier": "-Lr-u0b42he|||1|||32", "text": "what about individual inaccuracies?", "label": null}
{"identifier": "-Lr-u0b42he|||1|||33", "text": "when does the approach fail?", "label": null}
{"identifier": "-Lr-u0b42he|||1|||34", "text": "what is the influence of the granularity of the 3D feature map?", "label": null}
{"identifier": "-Lr-u0b42he|||1|||35", "text": "....), than to jump right into elaborate higher level tasks.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||36", "text": "---", "label": null}
{"identifier": "-Lr-u0b42he|||1|||37", "text": "Post rebuttal ---\nI'd like to thank the authors for the response.", "label": null}
{"identifier": "-Lr-u0b42he|||1|||38", "text": "I've updated my score in light of these additional results.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||0", "text": "The paper claims that the main contribution is \"to identify the importance of using disentangled 3D feature representation for few-show learning\".", "label": null}
{"identifier": "-Lr-u0b42he|||2|||1", "text": "This is a great goal, but in my view the paper does not deliver on that front.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||2", "text": "There are several issues from my perspective", "label": null}
{"identifier": "-Lr-u0b42he|||2|||3", "text": "1) The model formulation aims to disentangle shape from style in a particular formulation.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||4", "text": "To achieve training of this disentanglement it requires large amounts of multi-view data of highly simplified (and in this case simulated) scenes of at most few objects at the time.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||5", "text": "I am not convinced that we will ever have such data except for highly specialized situations.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||6", "text": "Unfortunately, the paper does not discuss this major limitation and does not make any attempt to convince the reader that this is a sensible starting point for further work.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||7", "text": "The only attempt to use real data is a custom dataset called \"real world veggie\" that has never been used before and it's characteristics are very unclear (number of classes, number of images, number of scenes, etc - also no sample images are given)", "label": null}
{"identifier": "-Lr-u0b42he|||2|||8", "text": "2) In order to show the \"importance of using disentangled 3D feature representation\" one would expect a set of experiments that shows comparable models with and without some form of disentanglement.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||9", "text": "However, the paper does not seem to contain any such experiments as far as I understood.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||10", "text": "3) While there are several experiments given, it is unclear how valuable the results and comparisons are.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||11", "text": "It seems that most if not all of the explored settings have not been addressed in the other works that this paper compares to.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||12", "text": "E.g. the paper [Mao et al 2019] does not seem to report any of the results given in Table 2.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||13", "text": "Similarly, the referenced papers that I checked from table 1 did not seem to report those results or this respective setting.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||14", "text": "This makes it very hard to understand if the proposed approach really has any benefit when it is used only in non-published and non-standard settings", "label": null}
{"identifier": "-Lr-u0b42he|||2|||15", "text": "So while the model formulation is quite sensible and combines in a meaningful way previous ideas and approaches, it remains unclear what can be taken away from this paper beyond the rather specific experiments mostly on simulated data (and some unclear real world veggie experiments)", "label": null}
{"identifier": "-Lr-u0b42he|||2|||16", "text": "========= post rebuttal ========", "label": null}
{"identifier": "-Lr-u0b42he|||2|||17", "text": "Thanks for adding the experiments for Replica - these at least seem to suggest that the approach can work on more complex scenes than shown initially in the paper.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||18", "text": "I still think the training scheme and required data is relatively specific for this approach that it is hard to see that it will generalize beyond - but that is probably fine for a research paper.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||19", "text": "that addresses Q1", "label": null}
{"identifier": "-Lr-u0b42he|||2|||20", "text": "for Q2 - please include some information in the main paper - not everyone will check the supplement and the paper should be self-contained.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||21", "text": "I still find the comparisons and ablations weak as the particular training setup and model are the key contribution for the paper and thus without a proper ablation it is hard to know what exactly to take away.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||22", "text": "Therefore I still remain somewhat skeptical but have raised my score somewhat.", "label": null}
{"identifier": "-Lr-u0b42he|||2|||23", "text": "To me the paper is still borderline and thus somewhere between 5 and 6 really.", "label": null}
{"identifier": "-Lr-u0b42he|||3|||0", "text": "This paper presents a modular network architecture for few-shot concept learning.", "label": null}
{"identifier": "-Lr-u0b42he|||3|||1", "text": "The architecture consists of image-to-scene module that maps input RGBD images to 3D scene features and an object-centric disentangling auto-encoder that crops object features to generate shape and style codes, and finally a neural rendering module that put back the reconstructed object and background features to image domain.", "label": null}
{"identifier": "-Lr-u0b42he|||3|||2", "text": "The proposed network is verified in few-short recognition task and VQA task with comparisons to state-of-the-art methods.", "label": null}
{"identifier": "-Lr-u0b42he|||3|||3", "text": "+This paper is well-written and the core ideas and network design are well illustrated and explained in the paper and supp video.", "label": null}
{"identifier": "-Lr-u0b42he|||3|||4", "text": "This paper also presents comprehensive experiments including sufficient details in supp to support the claims, which also makes the paper more reproducible.", "label": null}
{"identifier": "-Lr-u0b42he|||3|||5", "text": "+State-of-the-art results on few-shot recognition, especially on one-shot recognition.", "label": null}
{"identifier": "-Lr-u0b42he|||3|||6", "text": "The object-centric disentangling and 3D shape prototype learning seem to play complementary roles for the results.", "label": null}
{"identifier": "-Lr-u0b42he|||3|||7", "text": "-The success of the proposed method seems to be limited to the data being considered in this paper, rigid 3D objects with different appearances.", "label": null}
{"identifier": "-Lr-u0b42he|||3|||8", "text": "It is not clear how the proposed methods, e.g. AdaIn disentangling and rotation invariant prototype can be easily generalized to other scenarios including deformation (pointed out in the paper) and part-based composition.", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||0", "text": "This paper is based on an interesting observation that previous data augmentation tricks cut&mix may select regions that do not contain useful information.", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||1", "text": "Instead, this paper use saliency models to detect the salient regions first and then cut and mix these salient regions in a source image.", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||2", "text": "This observation is interesting and worth trying.", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||3", "text": "However, the contribution might be too limited for a ICLR conference paper:\n1. as cut&mix cuts bigger blocks in an image, the target object is more likely to be selected.", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||4", "text": "Also for most image classification dataset, the images are quite iconic, so the improvement on classification tasks are limited (as shown in Tab. 1, C10+ and C100+).", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||5", "text": "This might help with detection as it may train models to focus on the most discriminative part of the image, but recent works show that there is no direct correlation between between the performance of the same backbone on detection tasks and classification tasks.", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||6", "text": "In addition, the author didn't provide analysis on what causes the 1.8% improvement on detection tasks (better on smaller objects?)", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||7", "text": "So it's not clear how helpful this trick is.", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||8", "text": "2. While the method is simple, I expect either some mathematic proof or this method works well on various tasks.", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||9", "text": "The paper didn't have any proof or statistical analysis.", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||10", "text": "This paper didn't either show if the proposed method will work on more tasks (for example segmentation or GAN?", "label": null}
{"identifier": "-M0QkvBGTTq|||0|||11", "text": "the detection provided in this paper is using the backbone initialized from classification, during training faster-rcnn it seems that the trick is not used).", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||0", "text": "This paper proposes a new augmentation method based on CutMix.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||1", "text": "The authors find out that randomly selecting may mix background textures and this will mislead the model.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||2", "text": "So, they propose to use saliency maps to control the selection of mixed patches, which is called SaliencyMix.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||3", "text": "This idea seems easy and reasonable, many experiments are conducted to prove the effectiveness of the proposed method.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||4", "text": "However, the experiments\u2019 results fail to show the ability of the method, and some explanation is missed.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||5", "text": "Positive:\n1.     The idea is simple and clear, the paper is well organized and easy to follow.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||6", "text": "2.     The experiments are comprehensive, including classification and transfer learning.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||7", "text": "Weakness:\n1.     The main concern is the effectiveness of the proposed method.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||8", "text": "According to the authors\u2019 experiments, the improvement over CutMix is very limited on all datasets.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||9", "text": "Especially on ResNet-101, the promotion over CutMix is only 0.08.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||10", "text": "2.     According to the authors\u2019 ablation study in sec.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||11", "text": "3.3, only using fast self-tuning background subtraction produces better results than CutMix.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||12", "text": "Why other methods even worse than CutMx?", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||13", "text": "What\u2019s the core reason for the improvement of using fast self-tuning background subtraction?", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||14", "text": "3.     The authors use batchsize=256, lr=0.1 for CIFAR training, while usually batchsize=128, lr=0.1 is used in previous works (Cutout).", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||15", "text": "And as described in [1], the learning rate should be increased linearly with batchsize.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||16", "text": "This change of hyperparameters may need further explanation.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||17", "text": "[1] Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||18", "text": "============Post", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||19", "text": "Rebuttal====================", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||20", "text": "After reading the feedback from authors, I still have my concerns.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||21", "text": "The novelty of this paper is too limited for ICLR.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||22", "text": "I really do not think a combination of CutMix with existing saliency detection method is a novel method.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||23", "text": "Moreover, the improvement over CutMix is diminishing.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||24", "text": "These main concerns are not addressed by the authors.", "label": null}
{"identifier": "-M0QkvBGTTq|||1|||25", "text": "So, my final recommendation is still rejection.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||0", "text": "The paper presents a method called SaliencyMix.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||1", "text": "They improve a method that augments images by adding random patches from other images.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||2", "text": "The innovation is that they select these patches using a saliency map.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||3", "text": "This paper has an excellent discussion and critique of previous work.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||4", "text": "They discuss the existing work with a nice summary and then discuss reasons why selecting random patches can have issues.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||5", "text": "There is a clear argument for their method over selecting patches randomly.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||6", "text": "They make clear claims that this approach improves performance over randomly selecting patches.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||7", "text": "The experiments support this with sufficient related work (Cutout, Cutmix) and exploration of other design decisions and aspects about the idea.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||8", "text": "The idea is relatively straightforward and is inline with existing literature.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||9", "text": "The paper is well executed so there is not much to complain about.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||10", "text": "The availability of the source code is not clear from the text.", "label": null}
{"identifier": "-M0QkvBGTTq|||2|||11", "text": "A potentially interesting analysis (but not required) is an analysis of the increased runtime in practice.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||0", "text": "*Summary and contributions:*", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||1", "text": "This paper proposes a new data augmentation strategy to train image classifiers and object detectors.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||2", "text": "The key insight is to use an image saliency signal to guide where to crop-and-paste images when mixing them.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||3", "text": "The paper includes an exploration of the design space of such approach, and multiple experimental results showing the empirical superiority of the proposed approach compared to existing data augmentation strategies.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||4", "text": "*Significance:*", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||5", "text": "The paper is interesting because it provides a new trick in the bag of tricks that is both simple to understand, reasonable to argue for, and (now) has good empirical support (for classification, detection, and adversarial attack robustness).", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||6", "text": "Originality: \nLimited.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||7", "text": "Although no previous work provides the experimental results presented here, the results are expected.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||8", "text": "This work is good A+B incremental work.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||9", "text": "*Strengths:*\n* Overall the paper read easily.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||10", "text": "The general argumentation, method description, and experiments are all reasonably well described.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||11", "text": "* Simple ideas that provide good results.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||12", "text": "In retrospectively it might seem obvious, yet not explored before.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||13", "text": "* Widely applicable for all methods using pre-trained image classifiers.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||14", "text": "*Weaknesses:*\n* Although the experimental section is good, some elements are missing.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||15", "text": "E.g. adding non-data-augmented as reference point in the plot, or considering CAM as a saliency strategy.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||16", "text": "*", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||17", "text": "Some sections of the text would benefit from revisiting the English.\n*", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||18", "text": "The method implicitly relies on having \u201csimple images\u201d with one dominant foreground object  like the ones in CIFAR and ImageNet.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||19", "text": "The saliency / \u201cobject of interest\u201d  detection method depends on these characteristics.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||20", "text": "Ideally the paper would be more upfront on these assumptions.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||21", "text": "Especially in the context of the \u201cRethinking ImageNet Pre-training\u201d, ICCV 2019 work.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||22", "text": "*", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||23", "text": "The related work section for saliency is very partial.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||24", "text": "*", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||25", "text": "Some of the saliency methods evaluated use training data, even the ones that do not have been tuned using additional data.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||26", "text": "The paper would benefit from a discussion of this additional information.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||27", "text": "*Correctness:* Paper seems correct.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||28", "text": "There are minor shortcomings in the experimental protocol, but nothing that would foreseen invalidating the main conclusions of the paper.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||29", "text": "*Clarity:* Paper presentation is clear.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||30", "text": "*Relation to prior work:*\nRelated work section has a reasonable extent.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||31", "text": "Regarding data augmentation, the paper compares with the main methods.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||32", "text": "The text mentions Lemley 2017, however I think it would be also worth mentioning AutoAugment Cubuk 2019; and justifying why it is not included in the results comparison.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||33", "text": "The saliency detection is very partial, and does not cover the main works in the area.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||34", "text": "For one the text does not clarify \u201cwhich kind\u201d of saliency is considered (where will a human look ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||35", "text": "which are the main objects of the scene ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||36", "text": "which is the main object of the scene ?).", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||37", "text": "Depending on which one, discussing the main performers in the related benchmarks (e.g. table in Qin 2019 paper) seem relevant.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||38", "text": "From what I grasp, the proposed method would actually want to have as input a weakly supervised class-conditional segmentation.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||39", "text": "And \u201csaliency\u201d is used as a proxy for this.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||40", "text": "Discussing the relation to (image-level labels) weakly supervised segmentation would also be welcome.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||41", "text": "(I would guess it could provide even better results, but at a much higher computational and system complexity cost).", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||42", "text": "In particular CAM is discussed in section 4.3.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||43", "text": "Would that not be a task-specific way to obtain the desired \u201csemantically important regions\u201d  ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||44", "text": "When preparing the camera ready, please consider discussing the concurrent work of https://arxiv.org/pdf/2009.06962.pdf which seems related (seems recent enough, ICML 2020, to give the benefit of presumed concurrency).", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||45", "text": "*Reproducibility:*\nThe overall algorithm is simple to understand and re-implement.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||46", "text": "The selected saliency method \u201cWand and Dudek, 2014\u201d seems to be a video saliency method.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||47", "text": "From a quick inspection of that paper it is not immediately clear to me how to transpose it for single image saliency.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||48", "text": "Since this method is used in most experiments, providing more details of the implementation and its parameters are necessary to reproduce the key results.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||49", "text": "*Specific per-section feedback:*", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||50", "text": "Section 1:\n- every field: is too broad of a statement, remove/rephrase the first sentence.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||51", "text": "- extremely complex -> complex\n- generalizability -> generalization\n- undesired to the CNN since -> undesired since\n- does not allow \u2026 to have any uniformative pixel: double negative, consider simplifying.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||52", "text": "- semantically important region: these are task dependent.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||53", "text": "How do you ensure the saliency to match the task / semantics ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||54", "text": "Section 2.1:\n- comes into account that aims: unclear, please rephrase.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||55", "text": "Section 2.2:\n- unable to \u2026 1 or 0: this is not true, please rephrase.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||56", "text": "- intermediate values: you mean closer to 0.5 ?\n-", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||57", "text": "Thereby, helps: unsure if this is proper English.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||58", "text": "Section 2.3: See comments above regarding related work.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||59", "text": "-", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||60", "text": "Which kind of saliency is considered here ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||61", "text": "Which are the relevant benchmarks ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||62", "text": "Which training data / evaluation data are these methods bringing (indirectly) into the system ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||63", "text": "- BAsNet for example, was trained on 10k images.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||64", "text": "Why not simply include these (and their mask) as part of the pretraining when considering some of the baselines ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||65", "text": "Section 3.1:\n- selected training image -> selected training (source) image.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||66", "text": "In general do give hints for the subscript meaning of I_s, I_vs, etc.\n-", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||67", "text": "Why only one pixel with maximum intensity ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||68", "text": "What happens if (due to quantization) two pixels have the same value ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||69", "text": "This is clarified later in the text, but some context would be welcome in the mention here.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||70", "text": "Figure 3:\n- Add non-augmented result bar as reference point.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||71", "text": "- Consider showing the five points per bar, or some hint for the variance in these results.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||72", "text": "From the plot it is left unclear if the fluctuations across methods are significant or minor (since not reference point, nor sense of the variance).", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||73", "text": "Section 3.3:\n- Tiny-Imagenet: saliency methods tend to _not_ be scale invariant (in particular trained methods like BasNet).", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||74", "text": "How is this handled ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||75", "text": "Why would one expect Tiny-Imagenet to provide conclusive data ?\n-", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||76", "text": "What about CAM, or any other class-conditioned saliency / weakly supervised segmentation method ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||77", "text": "Section 3.4:\n- found out -> consider\n-", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||78", "text": "What about \u201cSalient to Random\u201c ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||79", "text": "That seems a reasonable option too ?", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||80", "text": "- are identical -> are similar", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||81", "text": "Section 4.1:\n- SOTA top-1 error: there are 20 methods that claim better results in https://paperswithcode.com/sota/image-classification-on-cifar-10 and https://paperswithcode.com/sota/image-classification-on-cifar-100 that claim better results.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||82", "text": "Maybe temperate the \u201cSOTA\u201d claim, with phrasings like \u201cbest known results for model X\u201d  or similar.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||83", "text": "One specific model result, far from best known, does not constitute \u201cstate of the art\u201d  in my understanding.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||84", "text": "Section 4.2:\n- Because in ->", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||85", "text": "This is because in", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||86", "text": "*Updates after reviews and authors feedback:*", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||87", "text": "The updates from the author are appreciated and make the arguments of the paper clearer.", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||88", "text": "After reading the other reviews and discussions, I have downgraded my score to \"7: Good paper, accept\".", "label": null}
{"identifier": "-M0QkvBGTTq|||3|||89", "text": "(Please note that in the current pdf the table at the top of page 7 has formatting issues.)", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||0", "text": "This paper proposes an improvement on the cutmix strategy of data augmentation, where the source patch is selected not randomly but based on saliency.", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||1", "text": "Results show improvements w.r.t mixup and other related strategies on Imagenet, CIFAR 10/100 and also transfer to object detection", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||2", "text": "Pros:\n-", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||3", "text": "The approach is intuitive and makes sense (which is more than can be said for the baselines of CutMix and MixUp).", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||4", "text": "I think this approach probably starts to get to the heart of why these previous strategies work: they are probably less effective ways of doing what this paper suggests.", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||5", "text": "-", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||6", "text": "The results seem quite promising, and the improvements seem significant.", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||7", "text": "Cons:\n- I am a bit surprised that the best strategy is Sal + Corr.", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||8", "text": "I understand the author's reasoning, but I find it strange that it has that big of an effect.", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||9", "text": "If the author's reasoning is correct, I would assume that a random placement of the cut patch would be just as effective.", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||10", "text": "Could the authors try Sal + Random instead?\n-", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||11", "text": "It is well known in the saliency literature that saliency has a center bias.", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||12", "text": "This suggests a baseline where the source patch is always cut from the center.", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||13", "text": "I would suggest the authors add this baseline.\n- I am not sure about the point of using CAM visualizations on augmented images.", "label": null}
{"identifier": "-M0QkvBGTTq|||4|||14", "text": "Perhaps a better visualization might be CAM visualization of the models trained with each kind of visualization on the unaugmented images?", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||0", "text": "Considering a continuous time RNN with Lipschitz-continuous nonlinearity, the authors formulate sufficient conditions on the parameter matrices for the network to be globally stable, in the sense of a globally attracting fixed point.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||1", "text": "They provide a specific parameterization for the hidden-to-hidden weight matrices to control global stability and error gradients, consisting of a weighted combination of a symmetric and a skew-symmetric matrix (and some diagonal offset).", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||2", "text": "The authors discuss numerical integration by forward-Euler and RK2, and thoroughly benchmark their approach against a large set of other state-of-the-art RNNs on various tasks including versions of MNIST and TIMIT.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||3", "text": "Finally, they highlight improved stability of their RNN against parameter and input perturbations.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||4", "text": "What I like about this paper is that it provides a solid theoretical basis and a principled, insightful parameterization that let\u2019s one control separately the size of the real and the imaginary parts of the eigenvalues of A and W.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||5", "text": "The paper contains a number of interesting thoughts, and a really extensive comparison to other state-of-the-art models on several benchmarks.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||6", "text": "In general, I feel however that this work is relatively close to the 2019 ICLR paper by Chang et al.; in several ways it feels like a more or less straightforward extension of this previous work.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||7", "text": "It also remains a bit unclear to me how it\u2019s ensured in practice that the matrices obey to the required conditions in the training process.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||8", "text": "Sect. 5 is not really about training, but just about numerically solving the ODE.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||9", "text": "From Appendix", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||10", "text": "C it seems the scalar parameters $\\beta, \\gamma$ controlling the influence of the symmetric vs. skew-symmetric parts and the offset are not learned at all but just fixed after grid-search?", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||11", "text": "The component matrices B, C, on the other hand it seems are not restricted at all but just initialized such that the theoretical conditions are likely, but not necessarily, met?", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||12", "text": "This seems somewhat unsatisfying as there are in fact no guarantees that the global stability conditions will be met in practice, and tuning the model may require (potentially extensive) meta-parameter search?", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||13", "text": "Another drawback in my mind is that enforcing global fixed point dynamics is quite restrictive.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||14", "text": "For instance, this rules out cycles and many other interesting dynamics in the model\u2019s intrinsic behavior.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||15", "text": "Apparently, from the authors\u2019 empirical tests, this seems not to be required for solving this particular set of tasks.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||16", "text": "Which is somewhat puzzling to me as it appears this assumption should strongly curtail the model\u2019s expressiveness.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||17", "text": "In the tables and figures I missed statistics.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||18", "text": "No standard errors or confidence bands were provided, or how many runs were performed.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||19", "text": "If it\u2019s all from a single run, can I be certain the numbers are not just lucky draws?", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||20", "text": "Nevertheless, given the overall convincing and extensive empirical results, I\u2019m slightly leaning toward acceptance.", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||21", "text": "Minor issues:\n-", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||22", "text": "The authors sell the additional linear term in their RNN as a novelty, while in fact it\u2019s rather standard in continuous RNN (older papers by Barak Pearlmutter, Song & Wang 2016, arxiv.org/abs/2006.02427, arxiv.org/abs/1910.03471)", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||23", "text": "- RK2 is generally not sufficient for more involved (stiff) dynamical problems; so the reason it works well here may lie in the fact that the model\u2019s intrinsic dynamic is indeed very simple\n- What is an unstable unit?", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||24", "text": "I guess the authors mean that the RNN is not globally stable?", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||25", "text": "- Sect. 2,  dynamical systems inspired RNN:", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||26", "text": "It may be important to note that formulating a RNN as ODE does not solve the exploding/vanishing gradient or stability problem per se (nor is it immediately clear to me why it should actually make it easier).", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||27", "text": "- Theorem 1: The $\\sigma$ refer to the matrix eigenvalues in this case?\n-", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||28", "text": "What is a superset of skew-symmetric matrices?", "label": null}
{"identifier": "-N7PBXqOUJZ|||0|||29", "text": "- Second-to-last pg. of Sect. 7 was unclear to me, i.e. what exactly was done and evaluated here, maybe cos I\u2019m not familiar with some of the cited methods.", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||0", "text": "The authors proposed a new continuous-time RNNs which appears to be extremely similar to CT-RNN (Funahashi et al. 1993).", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||1", "text": "They then constraint the network representation to account for learning long-term dependencies.", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||2", "text": "Positive:", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||3", "text": "The formulation of the constraint is very clear and sound.", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||4", "text": "Positive:", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||5", "text": "The analysis of the stability of the model and other properties is rigor enough and to the best of my knowledge sound and correct.", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||6", "text": "Negative: From the experimental setting, reported values in tables, and code, it seems like the authors tuned the hyperparameters on the test set which I consider a bad practice that violates the code of conduct!", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||7", "text": "I suspected this, and therefore, ran the code myself.", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||8", "text": "With a few changes to the hyperparameters from the tuned one reported in the table the performance of the proposed model dropped significantly!", "label": null}
{"identifier": "-N7PBXqOUJZ|||1|||9", "text": "I would suggest the authors to create a fair testing scheme for all baselines and report the experimental results more accurately.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||0", "text": "I have increased my score to reflect the revisions", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||1", "text": "---------", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||2", "text": "This paper presents yet another architecture for fully connected RNNs or infinitely deep networks based on the integration of a continuous time dynamical system, where a projection of the weights is used to guarantee stability, hence a fixed point and a finite Lipschitz constant.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||3", "text": "Positives:", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||4", "text": "The maths presented in the paper is correct and their results are nicer than the considered (not exhaustive) baselines.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||5", "text": "Negatives:", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||6", "text": "1) This idea has been widely explored and exploited by now.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||7", "text": "Adding a linear term and using a different solver is not enough in my opinion to make an innovative contribution.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||8", "text": "If you wish to present this as an ablation study, then perhaps you need to benchmark against existing solutions.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||9", "text": "For instance, in this (missing) reference a very similar network is presented and analysed.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||10", "text": "@incollection{NIPS2018_7566,\ntitle = {NAIS-Net: Stable Deep Networks from Non-Autonomous  Differential Equations},\nauthor = {Ciccone, Marco and Gallieri, Marco and Masci, Jonathan and Osendorfer, Christian and Gomez, Faustino},\nbooktitle = {Advances in Neural Information Processing Systems 31},\npages = {3025--3035},\nyear = {2018},\npublisher = {Curran Associates, Inc.},\nurl = {http://papers.nips.cc/paper/7566-nais-net-stable-deep-networks-from-non-autonomous-differential-equations.pdf}\n}", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||11", "text": "I will refer to this as [1].", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||12", "text": "While this paper was about unrolling the stable RNN to generate a deep Lipschitz classifier, and was not used for sequence to sequence task, the architecture you propose and the claims are so much similar to [1] that this demands for a direct comparison.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||13", "text": "In the above paper, stability projections are presented for an architecture that is essentially the same, minus the additional linear component here.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||14", "text": "Paper [1] should be included as a baseline.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||15", "text": "You have it already implemented for fully connected layers.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||16", "text": "2) It is not very clear how this additional linear component would help in practise, as your architecture is fundamentally discretised with Euler which results in yet another generalised res-net.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||17", "text": "It has been shown that ResNet works much better than their predecessor,  Highway networks, because of the direct skip connection and better gradient flow.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||18", "text": "While the missing reference [1] (NAIS-Net) preserves that connection,", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||19", "text": "It feels like your linear term would get rid of the skip connection and prevent the technique from being used in very deep networks or very long sequences due to vanishing gradients.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||20", "text": "3) What is the motivation for using a continuos-time approach?", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||21", "text": "Your results are invalidated by the forward Euler, unless a very small hyperparameter epsilon is introduced.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||22", "text": "Why not just compute the projection in discrete time as done in [1].", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||23", "text": "Your stability will not hold for sparse in time data-points, because the Euler step would become too big and this is effectively an RNN that cannot handle different sampling time while preserving stability.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||24", "text": "4) It seems strange to compare to NODEs as they are meant to be used for something else.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||25", "text": "In particular, NODEs are designed to work without inputs but just by estimating the initial condition for the ODE and then \"unrolling\".", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||26", "text": "If you add input signals to the NODE, which I guess is what you mean by \"NODE RNN\", how do you train it with the adjoint method?", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||27", "text": "This should be made very clear in the paper.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||28", "text": "5) Your results are limited to a fully connected architecture, while [1] has shown a method to have a Lipschitz RNN for convolutional layers.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||29", "text": "Can you generalize to that as well?", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||30", "text": "I don't feel the contribution here is relevant enough to be included in the conference.", "label": null}
{"identifier": "-N7PBXqOUJZ|||2|||31", "text": "If the above points are clarified in a convincing way and both the theoretical justification and the ablations performed with respect to [1], then I could consider improving my score.", "label": null}
{"identifier": "-N7PBXqOUJZ|||3|||0", "text": "The paper presents some novel contributions regarding recurrent neural networks.", "label": null}
{"identifier": "-N7PBXqOUJZ|||3|||1", "text": "Building on the work of Chang et al. (2019), \u00a0the authors provide a global convergence result for the hidden representation of a family of recurrent neural networks using standard techniques from the Lyapunov analysis of dynamical systems.", "label": null}
{"identifier": "-N7PBXqOUJZ|||3|||2", "text": "The requirements of the theorem are met \u00a0(within the limits of discretization) by their proposed algorithmic scheme.", "label": null}
{"identifier": "-N7PBXqOUJZ|||3|||3", "text": "Numerical evaluation on a variety of benchmarks shows that the proposed algorithm yields systematic improvement over other RNN approaches.", "label": null}
{"identifier": "-N7PBXqOUJZ|||3|||4", "text": "For all of the above reasons, I recommend the acceptance of the paper.", "label": null}
{"identifier": "-N7PBXqOUJZ|||3|||5", "text": "Some concerns to be addressed:\n- The connection between stability and trainability or refer to Chang et al. (2019) if their analysis applies here.", "label": null}
{"identifier": "-N7PBXqOUJZ|||3|||6", "text": "- specify the functions \\sigma_min and \\sigma_max used in Theorem 1\n- specify the meaning of the one-arg function f(h^*) as opposed to the 2-arg f(h,t) appearing in Definition 1.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||0", "text": "The paper found several methods to improve log likelihood of diffusion models while maintain their sample quality, including cosine instead of linear noise schedule, using a hybrid objective to learn parameters of the covariance function, and using importance sampling to improve the gradient noise.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||1", "text": "The authors also explore how sample quality and log likelihood scale with the number of diffusion steps and model capacity.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||2", "text": "Experiments on 64x64 ImageNet dataset show competitive llh while keeping the sample quality.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||3", "text": "Clarity: as the denoising diffusion model, especially its success in generating high-quality image samples, is still quite new, it would be beneficial if the paper could describe the technical background of it and the existing variational training methods in more details.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||4", "text": "Section 2 kind of serves this purpose, but it misses many important steps and focuses more on defining terms used in later sections.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||5", "text": "Significance of this work:", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||6", "text": "The necessity for having larger log likelihood for the diffusion model is not very well motivated.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||7", "text": "If the model is mostly used to generate high-quality samples and we have known how to train it to do so, why does the LLH values still matter?", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||8", "text": "Originality: using cosine noise schedule, new parameterization and hybrid objective seems effective for training, but doesn't seem to be very innovative.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||9", "text": "But I'm not very familiar with the denoising diffusion model and could be wrong.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||10", "text": "The results on how the model scales with computation seems trivial and may have been known already.", "label": null}
{"identifier": "-NEXDKk8gZ|||0|||11", "text": "Finally, it'll be beneficial if the authors can verify findings got in this paper can apply to other dataset or types of data more broadly.", "label": null}
{"identifier": "-NEXDKk8gZ|||1|||0", "text": "Denoising diffusion probabilistic models have been proved to produce excellent samples in the image and audio domains.", "label": null}
{"identifier": "-NEXDKk8gZ|||1|||1", "text": "However, it has yet to be shown that they can achieve competitive log-likelihoods.", "label": null}
{"identifier": "-NEXDKk8gZ|||1|||2", "text": "This paper shows that with several small modifications, diffusion models can achieve competitive log-likelihoods in the image domain while maintaining high sample quality.", "label": null}
{"identifier": "-NEXDKk8gZ|||1|||3", "text": "This paper is well-written and good-organized.", "label": null}
{"identifier": "-NEXDKk8gZ|||1|||4", "text": "However, I have the following concerns.", "label": null}
{"identifier": "-NEXDKk8gZ|||1|||5", "text": "1.\tThe authors claim that the noise schedule used in Ho et al. (2020) was, experimentally, sub-optimal for ImageNet $64 \\times 64$, which lacks theoretical guarantees.", "label": null}
{"identifier": "-NEXDKk8gZ|||1|||6", "text": "2.\tThis manuscript is mainly based on the previous work Ho et al. (2020).", "label": null}
{"identifier": "-NEXDKk8gZ|||1|||7", "text": "The novelty seems to be too limited.", "label": null}
{"identifier": "-NEXDKk8gZ|||1|||8", "text": "3.\tI am not convinced that only one dataset (ImageNet $64 \\times 64$) is sufficient to demonstrate the performance of the proposed strategy.", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||0", "text": "The paper talks builds upon the recent work from Ho (2020) about generative models that use noise diffusion.", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||1", "text": "The authors suggest that the proposal in Ho can not only be used in good quality sample generation (as already shown by Ho), but also leads to reasonable improvements in likelihood.", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||2", "text": "Overall, some of the ideas presented in the paper are interesting and useful; but the paper overall needs work.", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||3", "text": "Other questions/concerns:\n1. Firstly, from an application point of view, what does achieving a high log-likelihood mean, if the samples are already good enough or high quality?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||4", "text": "2. How do we interpret the bits/dim metric here?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||5", "text": "Its rather hard to rationalize that a change in 0.01 makes sense in this metric?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||6", "text": "And more generally, what are we aiming for in terms of a reasonable change?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||7", "text": "3. In section 3.1; how did we end up needing to tune big-sigma_theta (x_t, t) while arguing that fixing small-sig_t^2 is ok?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||8", "text": "This is in section 3.1 second paragraph; Either I am missing something of the argument here is that we need to tune noise variance and cannot fix it?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||9", "text": "4. What is the intuition behind expecting the (squared) cosine schedule to work?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||10", "text": "It is interesting to think that a periodic decay noising schedule is better than a linear one?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||11", "text": "5. And related to that, do not understand this weird value of 0.008 for s?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||12", "text": "The whole point here is some small non-zero s is ok; why specifically 0.008?!", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||13", "text": "6. One of the main conclusions in section 3.4 is kind of confusing --- based on the summary, if we are not interested in sample quality but only interested in maxing of likelihood, then the proposal of this work is not good, and working with L_vlb suffices?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||14", "text": "Is this correct?", "label": null}
{"identifier": "-NEXDKk8gZ|||2|||15", "text": "Based on the motivation, it seems the opposite was being claimed i.e., L_hybrid is important for maxing of likelihood (third para in introduction)?", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||0", "text": "**Summary**", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||1", "text": "This paper presents rich discussions and various practical techniques to improve the training of probabilistic diffusion models, which include a hybrid objective to learn the variance for improving log-likelihood performance, a different noise schedule tailored for ImageNet 64x64 and importance sampling to reduce gradient noise.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||2", "text": "Experiments on ImageNet 64x64 and various ablation study provided interesting insights and empirically justified the claims.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||3", "text": "**Pros**", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||4", "text": "The paper is well-written and develops some useful practical techniques for improving a recently proposed deep generative model (a modified version of the diffusion probabilistic model in (Jascha, et al 2015)).", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||5", "text": "Specifically, the paper managed to improve the log-likelihood performance by identifying the issue of the simplified objective and proposed to learn the variance using a hybrid objective.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||6", "text": "I think this technique along with others are useful practical techniques to improve the training of diffusion models.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||7", "text": "**Questions & Concerns**\n-", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||8", "text": "No results on CIFAR: Most recent papers in this field considers CIFAR-10 as the standard benchmark to report generative performance, including the original DDPM paper [1] and the score matching paper [2].", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||9", "text": "Although ImageNet 64x64 is a larger dataset with more complicated structure and diversity, outperforming pervious strong baselines in CIFAR-10 is still challenging and non-trivial.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||10", "text": "Thus the empirical study will also be more convincing to demonstrate that the proposed method can indeed achieve much better log-likelihood without sacrificing sample quality too much.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||11", "text": "Otherwise, it's hard to get a sense of how much improvement has been actually achieved by directly looking at the numbers in this paper and the ones in previous papers.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||12", "text": "- As the major contribution, the variance parametrization (Eq 16) needs more insightful discussions.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||13", "text": "For example, why this is the case: \"Since Figure 1a shows that the reasonable range for\u0012$\\Sigma(x_t; t)$ is very small, it is clear that we should not use a neural network to predict\u0012$\\Sigma(x_t; t)$ directly.\".", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||14", "text": "Can we predict the log of the variance with a neural network directly?", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||15", "text": "The proposed one is only an interpolation between $\\beta_t$ and $\\tilde{\\beta}_t$ - is this expressive enough?", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||16", "text": "- About the noise schedule: is this a generally better noise schedule, or it is only tailored for ImageNet 64x64.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||17", "text": "In the latter case, I think it is only a trick that overfits a specific dataset.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||18", "text": "To improve the training of DDPM generally, is there any advice on how to find a good noise schedule?", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||19", "text": "I will consider raising my score if the above concerns can be addressed.", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||20", "text": "[1] Denoising Diffusion Probabilistic Models", "label": null}
{"identifier": "-NEXDKk8gZ|||3|||21", "text": "[2] Generative modeling by estimating gradients of the data distribution", "label": null}
{"identifier": "-ODN6SbiUU|||0|||0", "text": "This paper is in defense of simple semi-supervised learning (SSL) with pseudo-labeling (PL): authors demonstrate with experiments on 4 vision datasets (CIFAR-10, CIFAR-100, Pascal VOC and UCF-101) that pseudo-labeling can perform on par with consistency regularization methods.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||1", "text": "Authors argue that PL doesn't work well because of poor network calibration: because of that the high confident predictions are wrong leading to noisy training and poor generalization.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||2", "text": "The main contribution of the paper is the usage of prediction uncertainty selection in addition to the confidence-based selection which provides high accuracy of PL used in the further training.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||3", "text": "Besides this PL is generalized to create negative labels: with this authors perform and show effectiveness of negative learning and multi-label classification.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||4", "text": "Proposed approach performs in the same ballpark as state-of-the-art methods on CIFAR-10 and CIFAR-100, while it achieves the new state-of-the-art results on video dataset and multi-label task.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||5", "text": "It is worth to notice that proposed approach is independent from the domain while consistency regularization methods extensively are based on the specific augmentation techniques for the vision/datasets.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||6", "text": "Pros:\n- Cool idea on predictions uncertainty based selection for pseudo-labeling, no any dependence on domain-specific augmentations for the method\n- Analysis of correlation between model calibration and prediction uncertainty\n- Analysis of UPS compared to the conventional PL and confidence-based PL\n- Ablation study on each component of the method, and dependence on the network architecture\n- Well-designed (fair comparison) extensive experiments and comparisons on 4 dataset for multiclass and multi-label classification with better performance than other methods", "label": null}
{"identifier": "-ODN6SbiUU|||0|||7", "text": "Cons:\n- Absent of large-scale experiments with ImageNet", "label": null}
{"identifier": "-ODN6SbiUU|||0|||8", "text": "Comments:\n- typo page 3 \"would lead to binary psuedo-labels\" -> \"would lead to binary pseudo-labels\"\n- \"For the multi-label case, $\\gamma = 0.5$ would lead to binary pseudo-labels, in which multiple classes can be present in one sample.\" - this sentence is not clear.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||9", "text": "If $\\gamma = 0.5$ it could be only one or two classes presented in the pseudo-label vector.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||10", "text": "- Eq. (2), it is obvious but still please specify that $\\tau_p >= \\tau_n$\n- Figure 1 (b) and (c) - on which data is this analysis done?\n-", "label": null}
{"identifier": "-ODN6SbiUU|||0|||11", "text": "Do authors use the same network $f_{\\theta, k}$ on each PL iteration $k$ and just randomly reinitialize it?\n-", "label": null}
{"identifier": "-ODN6SbiUU|||0|||12", "text": "What is $\\gamma$ value in experiments?", "label": null}
{"identifier": "-ODN6SbiUU|||0|||13", "text": "- For Table 1 would be good to have a clarification on baselines: which are consistency regularization based, which are PL.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||14", "text": "- typo in footnote 2 on page 7: add dot at the end of sentence.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||15", "text": "- typo page 7 \"experimental set-ups.\" -> \"experimental setups.\", \"labeled samples Both\" -> \"labeled samples.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||16", "text": "Both\"\n- Did authors try experiments on ImageNet too?", "label": null}
{"identifier": "-ODN6SbiUU|||0|||17", "text": "-", "label": null}
{"identifier": "-ODN6SbiUU|||0|||18", "text": "On which data is study in Fig.2 performed?", "label": null}
{"identifier": "-ODN6SbiUU|||0|||19", "text": "- typo page 8 \"unique in that it can be easily\" -> \"unique in that: it can be easily\" (or any punctuation here)\n- For ECE computation is percentile binning used?", "label": null}
{"identifier": "-ODN6SbiUU|||0|||20", "text": "- some possible relevant works: https://arxiv.org/pdf/2003.03773.pdf, https://arxiv.org/abs/2006.07733, simCLR v2, https://arxiv.org/abs/2006.09882", "label": null}
{"identifier": "-ODN6SbiUU|||0|||21", "text": "It is very well written paper with extensive experiments and ablations (except large-scale experiment), which prove the method efficiency and generalization.", "label": null}
{"identifier": "-ODN6SbiUU|||0|||22", "text": "Hope, this will push the study of simple SSL approach, pseudo-labeling, with the new competitive results not only in vision but in other domains too.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||0", "text": "# Summary", "label": null}
{"identifier": "-ODN6SbiUU|||1|||1", "text": "This paper proposes uncertainty aware pseudo-labelling for semi-supervised learning, extending previously known methods by negative labels.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||2", "text": "# Score justification", "label": null}
{"identifier": "-ODN6SbiUU|||1|||3", "text": "Well written paper and with extensive experiments with some points of improvement.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||4", "text": "The method should be positioned against others using confidence filtering and the role of calibration needs to be studied further.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||5", "text": "# Strong and weak points\n## Pros\nExtensive experiments on different datasets and domains.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||6", "text": "Broadly applicable method, that aims to improve conventional pseudo-labelling.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||7", "text": "Method is independent of uncertainty estimate and data augmentation.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||8", "text": "Combining uncertainty estimates and confidence estimates, as well as adding negative learning are interesting approaches.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||9", "text": "## Cons\nConfidence filtering for pseudo-labels has already been suggested, see e.g. suggestions given in detailed comments.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||10", "text": "I am unfortunately not convinced by the role of calibration, details given below.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||11", "text": "The authors could enhance the ablations studies with and without calibration and -importantly- adjusted thresholds, to clarify that point.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||12", "text": "# Questions to the authors", "label": null}
{"identifier": "-ODN6SbiUU|||1|||13", "text": "The authors should consider e.g.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||14", "text": "https://arxiv.org/pdf/2002.02705 and https://www.microsoft.com/en-us/research/uploads/prod/2020/06/uncertainty_self_training_neurips_2020.pdf and position their work against those.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||15", "text": "\"Learning with UPS\" have you treated pseudo-labels and original labels equally when training $f_{\\theta,1}$, if so, how about fine-tuning on the original labels?", "label": null}
{"identifier": "-ODN6SbiUU|||1|||16", "text": "Please elaborate on the effect of different thresholds and how they were chosen.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||17", "text": "Especially with regards to calibration.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||18", "text": "# Detailed comments", "label": null}
{"identifier": "-ODN6SbiUU|||1|||19", "text": "The authors should consider e.g.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||20", "text": "https://arxiv.org/pdf/2002.02705 and https://www.microsoft.com/en-us/research/uploads/prod/2020/06/uncertainty_self_training_neurips_2020.pdf", "label": null}
{"identifier": "-ODN6SbiUU|||1|||21", "text": "Augmentations for other domains are only not effective, if they are not suitable for that domain.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||22", "text": "Augmentations should use domain specific invariances.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||23", "text": "From the paper it becomes clear, you did use data augmentation, not sure why you argue so strongly against it.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||24", "text": "I am not sure if calibration plays a role here.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||25", "text": "Calibration only moves the distribution in shape, by setting a confidence threshold $\\tau$ the threshold would be changed, but a suitable threshold could be found before and after calibration.", "label": null}
{"identifier": "-ODN6SbiUU|||1|||26", "text": "Section 3.1 small type: \"psuedo\" --> \"pseudo\"", "label": null}
{"identifier": "-ODN6SbiUU|||2|||0", "text": "Strength:", "label": null}
{"identifier": "-ODN6SbiUU|||2|||1", "text": "The paper notices the problem in pseudo-labeling methods of semi-supervised learning, which is the erroneous prediction of pseudo-labeling.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||2", "text": "Pseudo-labeling is an easy-to-implement method for semi-supervised learning does not require constraints needed by consistency regularization methods, so improving pseudo-labeling methods can promote the practical use of semi-supervised learning.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||3", "text": "The paper is well-written and easy to follow.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||4", "text": "The experimental results on datasets of different domains such as image, video show that the proposed method outperforms previous semi-supervised learning methods.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||5", "text": "Constraint:", "label": null}
{"identifier": "-ODN6SbiUU|||2|||6", "text": "The paper proposes a new method to predict pseudo-labels of unlabeled data by confident threshold.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||7", "text": "However, confidence threshold is a widely-used method to decide pseudo-labels.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||8", "text": "And the threshold of confidence is hard to decide since different backbone network and different datasets have different confidence levels.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||9", "text": "Furthermore, using both positive and negative labels for classification is also used in (Kim et al., 2019).", "label": null}
{"identifier": "-ODN6SbiUU|||2|||10", "text": "The authors fails to discuss the difference of the usage of positive and negative labels between their method and (Kim et al., 2019).", "label": null}
{"identifier": "-ODN6SbiUU|||2|||11", "text": "The authors also need to discuss what is the special challenge of using positive and negative labels in pseudo-labeling for semi-supervised learning.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||12", "text": "The paper argues that the calibration error is greatly reduced with more certain predictions.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||13", "text": "However, the paper only empirically shows the relation but fails to demonstrate the claims or give intuition on the relation.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||14", "text": "Also, confidence itself is also an uncertainty measure, but the authors do not use the confidence for uncertainty but use a new uncertainty measure.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||15", "text": "Could the authors explain what uncertainty measurement do they use and why using the new measurement instead of the confidence?", "label": null}
{"identifier": "-ODN6SbiUU|||2|||16", "text": "The paper uses a fixed set of threshold hyper-parameters for all the experiments.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||17", "text": "However, the confidence-level for different datasets should be different.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||18", "text": "For example, the confidence for a real difficult dataset should be much lower than an easy dataset.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||19", "text": "The authors need to show that why using a set of hyper-parameters is enough and how to select the hyper-parameters.", "label": null}
{"identifier": "-ODN6SbiUU|||2|||20", "text": "For example, showing the relation between the accuracy of pseudo-labels and the confidence.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||0", "text": "As noted in (Guo et al., 2017), modern neural networks are often miscalibrated.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||1", "text": "Pseudo-labeling based Semi-Supervised learning schemes are predicated on high confidence predictions from these neural networks.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||2", "text": "This paper posits that this miscalibration may lead to inferior results in confidence-based pseudo-labeling approaches.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||3", "text": "By taking into account the uncertainty of models and only using pseudo labels from high-confidence instances with low uncertainty this work presents a model that significant improves on other PL strategies and is competitive with consistency-based regularization strategies that comprise the current state-of-the-art.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||4", "text": "Not only does this approach generate competitive results from with pseudo labeling strategy (which is compelling due to the history of PL approaches), but it is also less reliant on domain-specific augmentations that current consistency-based regularization approaches rely on.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||5", "text": "This is important for extending approach beyond domains where specific types of augmentation have been extensively studied.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||6", "text": "The results on the video domain offer some evidence of the usefulness in under-explored domains.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||7", "text": "For the CIFAR (10/100) results presented it is unclear what data augmentations where used to produce the UPS results and how important these augmentations are to presented performance.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||8", "text": "As a key claim in this paper is de-emphasis of domain-specific augmentations would be beneficial to see a highlighted result strengthening this claim.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||9", "text": "Also, it seems the model is re-initialized and trained to convergence after each round of pseudo labeling.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||10", "text": "This would extremely compute intensive for large-scale semi-supervised learning tasks, which is a motivating use-case for semi-supervised learning.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||11", "text": "Providing results and comparisons on the compute requirements for this approach and its comparisons with competitors would be beneficial.", "label": null}
{"identifier": "-ODN6SbiUU|||3|||12", "text": "As the re-initialization, is likely a major component in compute resources would also be beneficial for community to understand how essential this is to the improved performance numbers presented in the paper.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||0", "text": "Authors applied this algorithm into several datasets such as CIFAR 10, CIFAR 100 and TinyImagenet, they argued the image classification accuracies onto these datasets were comparable.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||1", "text": "The experiments were sufficient but not well designed.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||2", "text": "The reported experimental results show some novelty and good performance.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||3", "text": "In the early stage of the spiking neural networks, the encoding methods are very important, especially for the training process.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||4", "text": "As the authors said that the rated based encoding method brings much more time latency which is time consuming.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||5", "text": "Therefore, the topic of this work is critical.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||6", "text": "However I have some worries about the proposed methods, from my point of view, this work is just combing the DCT and ANN-SNN method, the novelty is significantly limited, but the idea is interesting.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||7", "text": "Then, the experimental results reported by this paper were not well designed.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||8", "text": "The authors argued that the rated based encoding methods are time consuming, but they just did not compare the much more temporal encoding methods in Table 3.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||9", "text": "And for me, CIFAR10, CIFAR 100 and MNIST are in the same quantity level, which means that you used a VGG net is waste of resource (VGG is too deep).", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||10", "text": "I even did not know what parts of the final results works, the deep CNN based network architecture?", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||11", "text": "The DCT encoding method?", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||12", "text": "Or the ANN-SNN methods.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||13", "text": "Actually, ANN-SNN method is not a typical bio-inspired way to construct a SNN, I prefer to see you adopt the proposed method into a Tempotron or STDP based learning rule not a surrogate-gradient based rule.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||14", "text": "The computational efficiency is nice; especially the authors calculated the spike rate of each single layer, but if you just argued the proposed the method is energy consumption, you should at least consider the ANN training process, it is not a single trade-off between inference accuracy and latency.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||15", "text": "I have run the code from authors provided, the reproducibility is reliable.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||16", "text": "Also there are some writing errors, such as thy->they, -s, etc. and reference missing, such as these important works:\n1.\tAn FPGA Implementation of Deep Spiking Neural Networks for Low-Power and Fast Classification.", "label": null}
{"identifier": "-Qaj4_O3cO|||0|||17", "text": "2.\tDeep CovDenseSNN: A hierarchical event-driven dynamic framework with spiking neurons in noisy environment", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||0", "text": "This paper proposes an encoding method based on the Discrete Cosine Transform (DCT) for Spiking Neural Network (SNN).", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||1", "text": "The key idea is to decompose an image into different frequency components and feed them to the SNN sequentially.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||2", "text": "Compared to the Poisson coding method used in most SNN studies, the proposed encoding method significantly decreases the latency that the SNN needs for image classification while having minimal accuracy decease.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||3", "text": "Highlights:", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||4", "text": "1. The idea of using DCT for input spike encoding is novel and has great potential.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||5", "text": "One of the problems that prevent the SNN from using fewer inference timesteps is the ineffectiveness of encoding input information.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||6", "text": "Using DCT, the method can potentially filter out less important information and more effectively encode the information in limited timesteps (as shown in Fig. 6 and Fig. 8 in the paper).", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||7", "text": "2. The paper doesn't directly learn in the frequency domain generated from DCT.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||8", "text": "Instead, it reverse transforms the DCT result back to the spatial domain and spreads it into different timesteps of the SNN.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||9", "text": "By doing so, the spike encoding gives more importance to the low-frequency information in the image.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||10", "text": "This is desirable because low-frequency information is more important than high-frequency information in the image for classification.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||11", "text": "Concerns:", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||12", "text": "1. The paper lacks experiments to show that DCT directly contributes to the decrease of timesteps for classification.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||13", "text": "Although comparisons with earlier SNN works that use Poisson encoding are shown, there is a lack of comparison with any SNN methods that directly convert pixel values into spikes using IF neurons and threshold selection.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||14", "text": "Thus, the existing experiments are not sufficient to exclude the possibility that the latency decrease is not due to DCT.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||15", "text": "The reviewer suggests conducting additional experiments for this.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||16", "text": "2. While the proposed method only focuses on the input encoding of SNN, many recent papers target new training methods (such as [Jibin Wu et al, 2019], [Sen Lu et al, 2020]) that also result in significant latency decrease of SNN for image classification.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||17", "text": "The paper lacks experiments to compare the performance with these more recent results.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||18", "text": "The reviewer suggests conducting additional experiments for this.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||19", "text": "3. The paper claims that the proposed method has better performance than ANNs trained on DCT coefficients.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||20", "text": "However, this is not a fair comparison since their encodings are different.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||21", "text": "The ANNs trained on DCT coefficients (such as [Max Ehrlich et al, 2019]) follow the same procedure as JPEG compression.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||22", "text": "The encoding uses non-overlapping 8x8 blocks, and the ANNs directly learn from the JPEG transformed domain.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||23", "text": "If the paper wants to compare with these ANNs, it needs experiments using the same encoding input.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||24", "text": "4. The example in Fig. 1 for the reverse transformation is not the same as the source code (spike_model_vgg9_submit.py: Line 233).", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||25", "text": "In section 3.1, the paper performs inverse transform by doing an element-wise multiplication between the transformed vector Y and each frequency basis in the transformation matrix, and claims the same method generalizes to the 2D case.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||26", "text": "However, the source code performs inverse transformation using the particular element in the transformed matrix Y (it's now a matrix but not a vector) corresponding to the specific frequency basis.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||27", "text": "Thus, the explanation in the paper contradicts the implementation.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||28", "text": "The reviewer thinks the example given in Fig. 1 is mathematically incorrect.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||29", "text": "5. The proposed method spreads the reverse transformed image into different timesteps, and each timestep corresponds to a particular frequency.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||30", "text": "However, the paper doesn't explore other possible approaches for spreading the information.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||31", "text": "For example, the encoding method can use multiple subsequent timesteps for a particular frequency or only present intermittent frequencies.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||32", "text": "The lack of in-depth analysis of the proposed method possibly prevents the paper from fully exploring the potential of the use of DCT encoding for SNN.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||33", "text": "Minor Comments:", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||34", "text": "1. What is the meaning of \"ov\" in Fig.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||35", "text": "4? The reviewer thinks it means \"overlap\".", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||36", "text": "However, it needs to be explained in the text or figure caption.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||37", "text": "2. In Table 2, it's not clear whether DNN-d uses DCT coefficients or the reverse transformed image.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||38", "text": "If the DNN-d uses the DCT coefficient, is there any change to the ConvNet since the DCT destroys the block's spatial relationships?", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||39", "text": "3. In Table 2, the SNN-d results for TinyImageNet are missing.", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||40", "text": "Is there any reason for that?", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||41", "text": "Jibin", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||42", "text": "Wu et al, 2019, A Tandem Learning Rule for Effective Training and Rapid Inference of Deep Spiking Neural Networks", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||43", "text": "Sen Lu et al, 2020, Exploring the Connection Between Binary and Spiking Neural Networks", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||44", "text": "Max Ehrlich et al, 2019, Deep residual learning in the jpeg transform domain", "label": null}
{"identifier": "-Qaj4_O3cO|||1|||45", "text": "Since most of my primary concerns are resolved, I have updated my rating based on the revised version.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||0", "text": "The scheme proposed breaks down the information in a block of an image into orthogonal basis functions (DCT is used) to make a progressively better reconstruction of the original image block with the addition of more basis functions used (like an nth order Taylor expansion).", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||1", "text": "The increasing spatial frequency components are known to be perceptually less sensitive (they need to include this) in images, so the low freq components can be presented first.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||2", "text": "Each freq component is encoded into spikes sequentially, thereby staging the more perceptually important information first, with less important info coming later.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||3", "text": "This reorders the presentation of information to allow a tradeoff of image quality with time/latency.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||4", "text": "I think the solution proposed is well-founded and will indeed mitigate the latency problem for spiking neural networks.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||5", "text": "However, this feels a bit more like an engineering solution to a specific problem rather than a new concept.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||6", "text": "I do like the injection of methods from other fields like image/video compression; it often feels that the deep learning field rediscovers things that have been uncovered years ago in other fields.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||7", "text": "I see that as the main value of the paper in addition to helping to make spiking neural networks a POSSIBLE viable solution to edge deployment.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||8", "text": "Section 1: I don\u2019t think it\u2019s a strongly supported claim that deep learning architectures are unsuitable for edge deployment.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||9", "text": "There are plenty in deployment and there are new processors (Movidius, Mythic, etc) that can handle these computations for real-time applications.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||10", "text": "I\u2019d suggest a softer language there.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||11", "text": "This does weaken the motivation for the paper though.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||12", "text": "Section 1, second paragraph:  typo: Thy ->", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||13", "text": "The", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||14", "text": "Section 3.2: On constraints for the transforms.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||15", "text": "Did the authors consider Integer Transform (IT)?", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||16", "text": "This is used in MPEG/AVC.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||17", "text": "It is a reversible transform that is an integer simplification of the DCT.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||18", "text": "Given that the point of the paper is to decrease latency and computing requirements for edge deployments, this could help.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||19", "text": "Section 3.2: The authors do a good job of sweeping performance for different block sizes.", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||20", "text": "Figure 5:  Isn\u2019t it an obvious result that more time steps are required for Poisson vs DCT?", "label": null}
{"identifier": "-Qaj4_O3cO|||2|||21", "text": "There simply aren\u2019t enough bins to sum over to have a result until a certain point.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||0", "text": "Pros:\n1.\tA novel coding scheme is proposed based on Discrete Cosine Transform (DCT) for efficient information expression in place of conventional Poisson distribution method.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||1", "text": "The required time-steps are 2-14x reduced compared with other conversion-SNNs or hybrid trained SNNs.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||2", "text": "2.\tDCT is data-independent while performing at par with PCA.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||3", "text": "Cons:\n1.\tThe experimental results are not convincing enough.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||4", "text": "2.\tCorrectness Problem.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||5", "text": "I am afraid that the descriptions about the reconstruction of input image is wrong.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||6", "text": "The reverse transform should be a matrix multiplication instead of Hadamard product of the coefficients and the basis.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||7", "text": "For example, in Fig 1., X=Y_1*T_1+Y_2*T_2+\u2026+Y_5*T_5.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||8", "text": "It results in conceptual errors in Fig1 & Fig2.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||9", "text": "3.\tThe specific equation of DCT should be with the discussion of the desirable properties and constraints in the section of encoding scheme to provide a clear picture of the method.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||10", "text": "About the experimental results:\n1.\tCould the authors try to provide some explanations for why DCT is able to outperform Poisson method or directly exposing the original image to the input spike neuron, since DCT\u2019s reverse transform is a reconstruction of the original image over time?", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||11", "text": "2.\tFewer time-steps with relatively lower accuracy is kind of confusing.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||12", "text": "I would like that the authors could further show the required time-steps for reaching strictly equal (or better) accuracy results with other SNN works (especially those directly trained).", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||13", "text": "It is because that the trade-off between accuracy and the number of time-steps is natural in SNNs.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||14", "text": "For example, Rathi et al. (2020) could increase their VGG16\u2019s performance from 91.13% to 92.02% by adding 100 time-steps on CIFAR-10.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||15", "text": "Rathi, Nitin, et al.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||16", "text": "\"Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation.\" arXiv preprint arXiv:2005.01807 (2020).", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||17", "text": "3.\tI notice that you cite Wu's article in related works, but there is a lack of comparison to it in later experiment part.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||18", "text": "In my opinion, the results of the paper and its sequel on time-steps for training SNNs from scratch are worth-noticing.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||19", "text": "I would like a more comprehensive and fair comparison of results.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||20", "text": "Wu, Yujie, et al.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||21", "text": "\"Spatio-temporal backpropagation for training high-performance spiking neural networks.\"", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||22", "text": "Frontiers in neuroscience 12 (2018): 331.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||23", "text": "Wu, Yujie, et al.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||24", "text": "\"Direct training for spiking neural networks: Faster, larger, better.\"", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||25", "text": "Proceedings of the AAAI Conference on Artificial Intelligence.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||26", "text": "Vol. 33. 2019.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||27", "text": "Clarity:", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||28", "text": "The paper is fairly well-written.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||29", "text": "Originality: DCT is a widely used transformation technique in signal processing and data compression, but this paper creatively explores it as a coding scheme in SNNs.", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||30", "text": "Significance:", "label": null}
{"identifier": "-Qaj4_O3cO|||3|||31", "text": "The proposed coding scheme might provide a new resolution to the high inference latency bottleneck in SNNs.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||0", "text": "### Summary:", "label": null}
{"identifier": "-QxT4mJdijq|||0|||1", "text": "The paper presents a meta-learning algorithm to learn/encode equivariance into deep nets.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||2", "text": "The main idea is to decompose the model parameters into two parts, a spatial sharing pattern, and the trainable weights.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||3", "text": "When transferring to a new task, the sharing pattern is fixed and only the remaining trainable weights are tuned.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||4", "text": "They authors motivated this approach stating that data augmentation may not be practical for robotics application which requires training in the real-world.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||5", "text": "For experiments, they consider a synthetic dataset where they can recover the equivariance and also k-shot classification tasks on datasets augmented with crops, rotations, and reflections.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||6", "text": "### Decision:\nI recommend a borderline reject for this paper.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||7", "text": "I have some questions with the claimed contribution, and details in the experimental section that should be answered before I can recommend an acceptance.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||8", "text": "### Supporting Arguments:\n1.\tOverall, I think the paper is interesting and relevant to the community.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||9", "text": "However, there are some questions that should be addressed.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||10", "text": "2.\tThis paper demonstrates that through their reparametrization, parameter-sharing, the network can be \"equivariant to any finite symmetry group\".", "label": null}
{"identifier": "-QxT4mJdijq|||0|||11", "text": "This result is similar to the work by Ravanbakhsh et al., (2017), which they also show the sharing-patterns encode equivariance properties.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||12", "text": "Due to its relevance, I believe a more careful discussion should be included and not just cited as \u201ctheoretical work has characterized the nature of equivariant layers for various symmetry groups\u201d.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||13", "text": "3.\tFor the experiments, I think a necessary to have a baseline that treats both U,v as trainable parameters.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||14", "text": "I wonder if it is necessary to train U and v separately on train and val;", "label": null}
{"identifier": "-QxT4mJdijq|||0|||15", "text": "Maybe the performance gain comes just from the fact that U is trainable, i.e., this model architecture benefits learning.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||16", "text": "4.\tWhat are the reported +/- in the Tables?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||17", "text": "Is it the standard deviation over several random initialization runs?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||18", "text": "5.\tJust to confirm, \u201ctest-sets\u201d in Aug-Omniglot and Aug-MiniImagenet are augmented as well?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||19", "text": "If yes, are there still performance gain without the augmentation?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||20", "text": "6.\tAre the baselines in Table 3 trained with data-augmentation as well?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||21", "text": "The paper states \u201cbenchmarks are identical to prior work (Finn et al., 2017)\u201d; does that mean without data augmentation?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||22", "text": "Also, what happens if both train and val sets are augmented for Alg.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||23", "text": "2?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||24", "text": "7.\tThere seem to be some learning rate, step sizes, and architecture tuning as described in supplementary materials.", "label": null}
{"identifier": "-QxT4mJdijq|||0|||25", "text": "How are these hyperparameters searched?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||26", "text": "What metric is being used to pick them and what ranges were considered?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||27", "text": "### Additional feedback:\n- Figure 2. Is a little bit blurry?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||28", "text": "- Maybe also mention the issue of  data augmentation with real-world data in the introduction?", "label": null}
{"identifier": "-QxT4mJdijq|||0|||29", "text": "It wasn\u2019t clear to me the challenges with data augmentation until much later in the paper.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||0", "text": "In this paper, the authors propose MSR, a parametrization of convolutional kernels that allows for meta-learning symmetries shared between several tasks.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||1", "text": "Each kernel is represented as a product of a structure matrix and a vector of the kernel weights.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||2", "text": "The kernel weights are updated during the inner loop.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||3", "text": "The structure matrix is updated during the outer loop.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||4", "text": "Strengths\n1.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||5", "text": "The paper is interesting and is easy to read.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||6", "text": "The figures help a lot in understanding the discussed ideas.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||7", "text": "2. The authors demonstrate that the proposed method outperforms the baseline meta-learning models.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||8", "text": "They empirically prove that MSR indeed learns valuable symmetries from the set of tasks and the provided data.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||9", "text": "3. The related work as well as the experimental part allow for a clear positioning of the proposed approach.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||10", "text": "It demonstrates a valuable connection between meta-learning and building equivariant models.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||11", "text": "I did not find any major weaknesses in the presented paper.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||12", "text": "Questions\n1. A matrix $W$ of size $8\\times8$ can be reparametrized in several ways.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||13", "text": "The corresponding vector $v$ can be of size $1, 2, 4, \\dots 64$.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||14", "text": "Do you consider the size of the vector as a hyperparameter?", "label": null}
{"identifier": "-QxT4mJdijq|||1|||15", "text": "If so, how to choose it?", "label": null}
{"identifier": "-QxT4mJdijq|||1|||16", "text": "2. If we consider the case of the exact flip symmetry, then the length of $\\text{vec}(W)$ must be even.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||17", "text": "One half encodes the original weight and the other half encodes the flipped weight.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||18", "text": "So we end up with a constraint between the shape of the matrix and the structure of the symmetry group.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||19", "text": "The same argument applies to all other symmetry groups.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||20", "text": "What happens if the constraint is not satisfied?", "label": null}
{"identifier": "-QxT4mJdijq|||1|||21", "text": "Can we learn a flip symmetry for $W$ of size $7 \\times 2$?", "label": null}
{"identifier": "-QxT4mJdijq|||1|||22", "text": "How $U$ will look in this case?", "label": null}
{"identifier": "-QxT4mJdijq|||1|||23", "text": "I enjoyed reading the paper.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||24", "text": "It is insightful, well-written, and demonstrates several valuable results both theoretical and experimental.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||25", "text": "### Decision", "label": null}
{"identifier": "-QxT4mJdijq|||1|||26", "text": "The authors answered all my questions.", "label": null}
{"identifier": "-QxT4mJdijq|||1|||27", "text": "My decision stays the same.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||0", "text": "OVERVIEW:", "label": null}
{"identifier": "-QxT4mJdijq|||2|||1", "text": "The authors present a meta-learning approach for network equivariance where the key idea is that equivariance to a finite group of transformations can be achieved by identifying the sharing pattern of weights.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||2", "text": "Their proposition claims that a fully connected layer $\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ with weights $W$ can be factorized into a symmetry matrix $U$ and filter parameters $v$ where the symmetry matrix encodes desired group-convolutions: $\\text{vec}(W) = U v, \\hspace{1em} v \\in \\mathbb{R}^k, U \\in \\mathbb{R}^{mn \\times k}$.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||3", "text": "Within the meta-learning framework, they learn both $U$ and $v$ as part of the outer and inner steps respectively.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||4", "text": "They demonstrate that they are able to recover the translational equivariance baked into traditional convolutions using this approach including the expected symmetry pattern (shown in Fig. 4).", "label": null}
{"identifier": "-QxT4mJdijq|||2|||5", "text": "They are also able to demonstrate this two more scenarios: (i) for a group of translation, discrete $45^\\circ$ rotation and reflection, and (ii) for Augmented-Omniglot and Augmented-MiniImagenet, providing empirical evidence that their proposed approach learns meaningful information for network equivariance.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||6", "text": "PROS:\n- I really liked the idea of equivariance captured as a symmetry pattern (or weight sharing) and being able to learn it from data using meta-learning.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||7", "text": "This is very interesting and exciting with the ability to \"learn\" rather than \"hope\" that equivariance is learned from appropriately augmented data. \n-", "label": null}
{"identifier": "-QxT4mJdijq|||2|||8", "text": "A proof of Proposition 1 is presented in the Appendix which is a nice contribution in my opinion.\n- I liked the visualizations of the symmetry pattern for translation equivariance and the filters for discrete rotation equivariance (in appendix).", "label": null}
{"identifier": "-QxT4mJdijq|||2|||9", "text": "They are visual evidence of achieving what is expected from the proposed approach.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||10", "text": "-", "label": null}
{"identifier": "-QxT4mJdijq|||2|||11", "text": "The experiment in Section 5.2 is helpful in backing up the claim of \"hybrid\" equivariance where you are able to learn translations + discrete rotations.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||12", "text": "This is achievable by appropriate filter design for simpler groups (mostly 2D) like in Harmonic-Nets [Worrall et al] but can get complicated for interesting groups like SO(3) & SE(3).", "label": null}
{"identifier": "-QxT4mJdijq|||2|||13", "text": "CONS:\n- The biggest concern for me are that the experiments are largely on synthetic data which has been randomly generated (Sections 5.1 and 5.2).", "label": null}
{"identifier": "-QxT4mJdijq|||2|||14", "text": "The only real data experiment is on Aug-Omniglot and Aug-MiniImagenet for a few-shot classification task.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||15", "text": "Equivariance has been demonstrated to be effective on real data in two setups that I am aware of: (a) 3D Model classification with spherical convolutions like in Cohen & Welling, [Esteves et al](https://arxiv.org/abs/1711.06721), (b) Azimuth and scale estimation on Google Earth images [Henriques and Vedaldi](http://proceedings.mlr.press/v70/henriques17a.html).", "label": null}
{"identifier": "-QxT4mJdijq|||2|||16", "text": "Applying the proposed method to one of these experimental setups will be very helpful in convincing the readers of their use on real data.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||17", "text": "-", "label": null}
{"identifier": "-QxT4mJdijq|||2|||18", "text": "Is the proposed approach learning local equivariance (like with harmonic filters) or global equivariance (like with warped convolutions where transforming the image into polar coordinates gives it rotation and scale equivariance)?\n-", "label": null}
{"identifier": "-QxT4mJdijq|||2|||19", "text": "From the proof of Proposition 1, it becomes clear why the restriction to a finite group.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||20", "text": "The question I have is what is needed to move into more general (continuous) groups like SO(2), SO(3), SE(3)?", "label": null}
{"identifier": "-QxT4mJdijq|||2|||21", "text": "Will an approximation (into some finite pattern with tiling) be good enough?", "label": null}
{"identifier": "-QxT4mJdijq|||2|||22", "text": "I think that it is a possible future research direction but I am interested in knowing your thoughts about it.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||23", "text": "-", "label": null}
{"identifier": "-QxT4mJdijq|||2|||24", "text": "The methods the authors compare against are meta-learning methods which makes sense given the broader framework the work is in.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||25", "text": "However, I would like a comparision with other network equivariance via filter design algorithms.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||26", "text": "I don't expect better results but some discussion of comparable performance without the handcrafted design or the ability to learn equivariance for groups without a handcrafted filter available will be a huge plus.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||27", "text": "REASON FOR RATING:\nI like the paper and it makes a very good contribution in an important area.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||28", "text": "It is however held back by the lack of experiments on real data and comparision with other established equivariance works leading to my current rating.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||29", "text": "UPDATE:\nI have read the author feedback and other reviews/discussions.", "label": null}
{"identifier": "-QxT4mJdijq|||2|||30", "text": "I have updated my rating to 8 from 7 reflect it.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||0", "text": "This work (i) meta-learns equivariances in neural networks by reparameterizing fully connected layers into a symmetry matrix and filter parameters, \nand (ii) meta-learns invariances from augmented data.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||1", "text": "Strengths:\n+", "label": null}
{"identifier": "-QxT4mJdijq|||3|||2", "text": "The work implements (i) by a layer and custom inner loop optimizer using the higher-order optimization library [1]\nand (ii) by augmenting benchmark datasets [2].", "label": null}
{"identifier": "-QxT4mJdijq|||3|||3", "text": "+", "label": null}
{"identifier": "-QxT4mJdijq|||3|||4", "text": "The work learns partial translational symmetry, euqivariance to rotation and flips\n+", "label": null}
{"identifier": "-QxT4mJdijq|||3|||5", "text": "The work performs important ablation studies, such as testing reparameterization with and without meta-learning by using multi-task learning instead.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||6", "text": "Weaknesses:\n- Mostly toy examples", "label": null}
{"identifier": "-QxT4mJdijq|||3|||7", "text": "- Rather than baking-in inductive biases into the network by encoding equivarainces, \nother approaches such as the vision Transformer [3] learn inductive biases from data:\nlearning local, medium, and long range connections, discovering architectures which supersede CNNs, learning filters, \nand demonstrating that CNNs are a curve on an attention distance vs. network depth plane.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||8", "text": "Adding a reference to this line of work may improve the introduction.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||9", "text": "- Minor changes:\nFigures 1,2, and 3 may be improved.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||10", "text": "Algorithm 2 may be sufficiently described in words.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||11", "text": "Typos on page 14 may be fixed.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||12", "text": "lines 9-10 should read \"chosen to have\" and \"theoretically meta-learned\"\nline 32 should read \"each $\\pi(i)$ translates the filter\"", "label": null}
{"identifier": "-QxT4mJdijq|||3|||13", "text": "[1] Generalized inner loop meta-learning, Grefenstette et al, 2019.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||14", "text": "https://github.com/facebookresearch/higher", "label": null}
{"identifier": "-QxT4mJdijq|||3|||15", "text": "[2] Torchmeta: A meta-learning library for PyTorch, Wurfl et al, 2019.", "label": null}
{"identifier": "-QxT4mJdijq|||3|||16", "text": "https://github.com/tristandeleu/pytorch-meta", "label": null}
{"identifier": "-QxT4mJdijq|||3|||17", "text": "[3]", "label": null}
{"identifier": "-QxT4mJdijq|||3|||18", "text": "An image is worth 16x16 words: Transformers for images recognition at scale, Dosovitskiy et al, 2020\nhttps://arxiv.org/pdf/2010.11929.pdf", "label": null}
{"identifier": "-RQVWPX73VP|||0|||0", "text": "### **Summary and Contributions of Paper**", "label": null}
{"identifier": "-RQVWPX73VP|||0|||1", "text": "This paper proposes to improve the K-shot RL meta-learning problem by using an LSTM, whose repeated inputs are (s,a,s') state-action transitions, and whose step-wise outputs are context vectors.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||2", "text": "The policy, during the K-shot phase, then additionally observes the current context vector along with the standard state, in order to produce an action.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||3", "text": "In order to train both the policy and the LSTM in an end-to-end fashion, the paper modifies the MAML objective appropriately, since the LSTM's context vector is used throughout the entire K-shot process.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||4", "text": "Experiments are performed on the Nav-2D 4-corner exploration task (proposed by ProMP, 2018) and a few standard Mujoco meta-learning tasks.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||5", "text": "### **Strengths**\n-", "label": null}
{"identifier": "-RQVWPX73VP|||0|||6", "text": "Seems to be novel in the sense of allowing an RNN to control how the policy explores the task during the K-shot phase.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||7", "text": "This is in contrast with previous works which used manual methods for the policy exploration.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||8", "text": "- Once I understood Figure 3 (past the presentation issues), the LSTM's context vector seems to signal to the policy which areas to explore next (rather than entropy-dependent exploratory movement in the default MAML algorithm) in the K-shot phase, which is good exploration behavior.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||9", "text": "### **Weaknesses**\n-", "label": null}
{"identifier": "-RQVWPX73VP|||0|||10", "text": "One of the most obvious issues are the numerous grammar issues/awkward phrasings.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||11", "text": "The writing of the paper can be significantly improved.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||12", "text": "This also includes the mathematical notation, as variables are sometimes used abruptly without introduction.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||13", "text": "I do think this paper has promise, but really needs to be cleaned up in its presentation.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||14", "text": "- Experiments can be more comprehensive.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||15", "text": "So far, there are only 3 Mujoco tasks used, whereas there are 8+ to choose from (see T-MAML, 2019).", "label": null}
{"identifier": "-RQVWPX73VP|||0|||16", "text": "More Mujoco benchmarks will make the results convincing.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||17", "text": "Furthermore, the authors only plotted the recorded scores from previous papers (as horizontal lines) rather than run the baselines themselves.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||18", "text": "One issue here is that in many cases, there are multiple task definitions/tweakings that are not the same across papers.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||19", "text": "For instance, the original MAML paper and T-MAML used completely different scalings for the alive bonus, movement reward, and the energy expenditure penalties for the task \"ForwardBackwardAnt.\"", "label": null}
{"identifier": "-RQVWPX73VP|||0|||20", "text": "The authors need to run the baselines themselves in order to have rigorous metrics.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||21", "text": "This should not be difficult, as there are already very clean implementations, e.g. see (https://github.com/lhao499/taming-maml).", "label": null}
{"identifier": "-RQVWPX73VP|||0|||22", "text": "I think the paper proposes some promising ideas.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||23", "text": "However, due to the weaknesses mentioned, I think the paper needs to be cleaned up a lot more before being ready for submission, and thus I propose rejection.", "label": null}
{"identifier": "-RQVWPX73VP|||0|||24", "text": "### **Clarity Questions**\n-", "label": null}
{"identifier": "-RQVWPX73VP|||0|||25", "text": "Can you provide some explanations for pros/cons against PEARL (which also uses a context vector approach, but uses a feed-forward VAE-like network for the context vector?)", "label": null}
{"identifier": "-RQVWPX73VP|||0|||26", "text": "PEARL seems to be the most direct competitor to the proposed paper's approach, as both are using context vectors.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||0", "text": "Authors introduce a new meta-RL algorithm based on SAC.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||1", "text": "It uses a context variable $c$ that they condition the Q-function on and the adaptation mechanism which is based on the values of the value function (ie. $\\mathbb{E_a} Q(\\dot, a)$) instead of the true returns.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||2", "text": "Authors claim their method reduces variance and bias of the meta-gradient estimation, is closer to human learning, encourages the agent to learn to explore, is more data-efficient in test-time and has competitive performance among gradient-based algorithms.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||3", "text": "I found many of the claims of the paper are unjustified:\n1. Authors claim \"we reformulate and propose the K-shot meta-RL problem to simulate the real world environment\".", "label": null}
{"identifier": "-RQVWPX73VP|||1|||4", "text": "The formulation that follows is the standard one (cf. MAML (Finn et al. 2017): \"In K-shot reinforcement learning, K rollouts from $f_\\theta$ and task $T_i$ (...) may be used for adaptation on a new task $T_i$\".", "label": null}
{"identifier": "-RQVWPX73VP|||1|||5", "text": "It is unclear how does the authors' definition relate to the real world.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||6", "text": "2. They suggest their method is able to \"learn where to explore\", but nothing in the method specifically addresses this part (compared to eg.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||7", "text": "Learning to reinforcement learn (Wang et al. 2016), which also has a learned context that influences the policy).", "label": null}
{"identifier": "-RQVWPX73VP|||1|||8", "text": "3. They suggest their method learns more \"human-like\", which is understood as \"not using batched sampling\" (as, arguably, humans learn more sequentially).", "label": null}
{"identifier": "-RQVWPX73VP|||1|||9", "text": "However, their method also uses batched sampling (see eq. (17)).", "label": null}
{"identifier": "-RQVWPX73VP|||1|||10", "text": "In another part of the paper human-like learning is associated with the usage of LSTM, which is also neither novel (see Wang et al. (2016) again) nor grounded.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||11", "text": "4. They claim their method decreases bias and variance.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||12", "text": "Gradient estimation in a typical meta-RL method is unbiased, so that's hard to decrease.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||13", "text": "On the other hand, I believe the variance of the proposed method is increased compared to E-MAML and others due to approximating rewards with V, which, due to the fixed-capacity of the model, will make their adaptation procedure inherently biased and thus estimating gradients in incorrect places.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||14", "text": "5. Their method \"makes the meta-policy more interpretable\", yet no interpretation attempt was presented.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||15", "text": "6. \"Agent can learn how to explore environment and how to utilize the transition data, which is a more structured learning scheme\".", "label": null}
{"identifier": "-RQVWPX73VP|||1|||16", "text": "The learning scheme is basically the same as in PEARL (Rakelly et al. 2019).", "label": null}
{"identifier": "-RQVWPX73VP|||1|||17", "text": "It's not clear what exactly makes the learning scheme \"more structured\" nor what it means.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||18", "text": "7.  \"Each trail we sampled 200 steps for total 2 trails, the data used to do adaptation in our algorithm is 10% of in PEARL and 5% in ProMP.\".", "label": null}
{"identifier": "-RQVWPX73VP|||1|||19", "text": "According to Rothfuss et al. (2018), app. D.2, they used a single trajectory of 200 steps for adaptation for HalfCheetahFwdBack, which is half of what the authors' method used.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||20", "text": "On the other hand, PEARL achieves much better results than the authors' method, what is not mentioned in the paper.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||21", "text": "Details of the method are not very clear, I assume they follow SAC with online/target networks, but am not really sure nor what $\\mu$, $\\phi$ and $\\eta$ mean.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||22", "text": "Mechanics of meta-testing also weren't described in enough detail.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||23", "text": "The writing of the paper is terrible, to the point it's not always clear what the authors mean.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||24", "text": "Most of the problems are simple grammar errors, which are easily solvable by a tool like grammarly or google docs correction.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||25", "text": "As such, I consider sending the paper to the review in its current state a disrespect for the reviewers who have to spend the time to decipher the writeup.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||26", "text": "Grammar problems (first page only):\n1. However, in many real world tasks\n2. Derive a new policy **which** maximizes\n3.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||27", "text": "The experiment results suggest that\n4.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||28", "text": "Utilize sampled data **to** some extent.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||29", "text": "5. **While** obtaining environment data\n6. Human gradually understand**s** where to sample data\n7.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||30", "text": "Stochastic trajectories to do p**o**licy adaptation\n8. \"This causes agent (...) and only learns how to utilize data\" - not clear what was meant\n9.", "label": null}
{"identifier": "-RQVWPX73VP|||1|||31", "text": "That have high cost o**f** obtaining data", "label": null}
{"identifier": "-RQVWPX73VP|||2|||0", "text": "The paper presents a new meta reinforcement learning algorithm that uses trajectory information to create a contextual embedding for each task.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||1", "text": "The context is used as a given condition for learning the Q function and policy using soft actor critic algorithm.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||2", "text": "Suggestions to improve the quality of the paper:\n- The title states \"interpretable\" but there is no interpretability discussed anywhere in the paper.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||3", "text": "A more relevant title would be appropriate.\n-", "label": null}
{"identifier": "-RQVWPX73VP|||2|||4", "text": "The idea of contextual embedding using trajectory and LSTMs have been explored in multiple papers (e.g. PEARL (Rarely et al. 2019), MQL (Fakoor et al., 2019).", "label": null}
{"identifier": "-RQVWPX73VP|||2|||5", "text": "It is not clear how the proposed method is different from these.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||6", "text": "-", "label": null}
{"identifier": "-RQVWPX73VP|||2|||7", "text": "It is unclear how adding context information to the Q function leads to structured exploration.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||8", "text": "SAC does not use Q function to explore, it uses the probability distribution of the policy network output for exploration.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||9", "text": "Prior work has shown that SAC exploration can be improved with dual Q function and upper confidence bounds, but that is not being used here: https://papers.nips.cc/paper/8455-better-exploration-with-optimistic-actor-critic", "label": null}
{"identifier": "-RQVWPX73VP|||2|||10", "text": "-", "label": null}
{"identifier": "-RQVWPX73VP|||2|||11", "text": "It is unclear how the K-shot version of the meta-RL algorithm is different from other meta-RL algorithm.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||12", "text": "The proposed algorithm also samples from the tasks uniformly, same as prior algorithms.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||13", "text": "-", "label": null}
{"identifier": "-RQVWPX73VP|||2|||14", "text": "There are numerous typographical and grammatical errors throughout the paper.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||15", "text": "These can be easily fixed with modern text editors.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||16", "text": "Some examples - trails, data in some extent, where to sampling data, plicy, meta-gradien, want perform, availble, help performing, subistitue, contrains, etc. \n- Figure 3 and 4 are very difficult to read, even on a computer.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||17", "text": "The graph title, axes labels, axes tick marks, legend are all ineligible.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||18", "text": "The quality of these graphs need to be improved for readability.", "label": null}
{"identifier": "-RQVWPX73VP|||2|||19", "text": "-", "label": null}
{"identifier": "-RQVWPX73VP|||2|||20", "text": "The evaluation results are over two trials, need more of them for a convincing argument that the proposed method is better than prior methods.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||0", "text": "Summary:", "label": null}
{"identifier": "-RQVWPX73VP|||3|||1", "text": "This paper proposes a new approach for meta-RL.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||2", "text": "The paper claims that the proposed method reduces variance and bias of the meta-gradient estimation by only a few samples.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||3", "text": "In addition, this paper claims that their method is more interpretable.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||4", "text": "My comments:", "label": null}
{"identifier": "-RQVWPX73VP|||3|||5", "text": "There are lots of unknown, unwarranted claims about this paper in addition to no thorough experiments and comparison with previous works :", "label": null}
{"identifier": "-RQVWPX73VP|||3|||6", "text": "1. Paper claimed that their proposed method not only reduces the variance of meta-gradient but also reduces bias.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||7", "text": "Reading through this method and experiments, I don't see any theoretical or empirical justification why that should be the case.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||8", "text": "2. The method claims this method has a more interpretable meta-gradient.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||9", "text": "Again, there is nothing in this paper that verifies this claim.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||10", "text": "3. Paper proposed to use fewer samples for the adaptation phase.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||11", "text": "I don't understand why this can help at all.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||12", "text": "4. Experiments are incomplete and there is no rigorous evaluation to analyze this method.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||13", "text": "5. The proposed context in this paper has been already proposed in previous work, not sure what is new here.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||14", "text": "6. There are lots of things that need to defined like trail, few-shot, etc.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||15", "text": "In summary, this paper is not ready at all and needs lots of works.", "label": null}
{"identifier": "-RQVWPX73VP|||3|||16", "text": "Hence, I'd recommend this paper to be rejected.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||0", "text": "Summary\n----------", "label": null}
{"identifier": "-RQVWPX73VP|||4|||1", "text": "This paper presents an approach to meta-RL based on combining gradient-based updating with recurrence-based meta-learning.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||2", "text": "The approach is based on combining gradient-based policy updating with recurrence-based updating of the value function.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||3", "text": "The authors evaluate the method of simple standard meta-RL benchmarks.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||4", "text": "Comments", "label": null}
{"identifier": "-RQVWPX73VP|||4|||5", "text": "----------", "label": null}
{"identifier": "-RQVWPX73VP|||4|||6", "text": "Overall, the direction of the paper is interesting but the paper has numerous shortcomings.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||7", "text": "First, the proposed method is a fairly straightforward combination of existing techniques.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||8", "text": "In particular, the approach consists of a fairly simple combination of gradient-based and recurrence-based methods.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||9", "text": "While this is not necessarily a limitation, it necessitates a thorough set of experiments to justify the combination of approaches.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||10", "text": "This is the second limitation of the paper: the experimental evaluation is very limited.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||11", "text": "The authors test of very simple benchmark problems compared to recent work in meta-RL.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||12", "text": "Moreover, there are few comparisons to baseline methods (especially PEARL) and the authors should include ablation experiments in which they examine the performance of their method relative to strictly gradient-based and strictly recurrence-based methods, using the SAC algorithm as an underlying algorithm.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||13", "text": "Finally, the writing of the paper is extremely sloppy.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||14", "text": "Trials is misspelled as trails throughout the paper.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||15", "text": "There are also numerous typos, such as \"gardient\", \"meta-reinforce\", \"Substitue\", \"apdated\", etc.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||16", "text": "The presentation of the algorithm is quite unclear and the discussion of related work is quite limited.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||17", "text": "Overall, the paper should be tested on a wider range of environments and against more competitive baselines to warrant acceptance.", "label": null}
{"identifier": "-RQVWPX73VP|||4|||18", "text": "The authors should also improve the presentation of the paper to improve clarity.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||0", "text": "** Summary", "label": null}
{"identifier": "-TwO99rbVRu|||0|||1", "text": "This work addresses the task of semi-supervised learning (SSL) in semantic segmentation.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||2", "text": "Following recent SOTAs in SSL, this work also advocates for the use of pseudo-labels on unlabeled data and heavy data augmentation.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||3", "text": "The main novelty of this work is the novel way to construct higher-quality pseudo-labels: besides the pixel-wise classifier's probabilistic outputs, the authors leverage as well CAM-based activation maps, named as SGC, as an additional pseudo-label source.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||4", "text": "The final set of pseudo-labels is determined by linear combining the two soft pseudo-label sources with temperature adjustment.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||5", "text": "The authors conducted extensive experiments with lots of ablation studies to validate the proposed framework.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||6", "text": "** Strengths:\n-", "label": null}
{"identifier": "-TwO99rbVRu|||0|||7", "text": "The paper is well-written, easy to follow\n- Extensive experiments with adequate discussions\n- Improvements over SOTAs on the addressed benchmarks.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||8", "text": "** Weakness/concerns:\n - Does \"distribution sharpen operation\" always use temperature < 1?", "label": null}
{"identifier": "-TwO99rbVRu|||0|||9", "text": "If yes, what is the reason?\n -", "label": null}
{"identifier": "-TwO99rbVRu|||0|||10", "text": "How is the temperature $T$ is chosen?", "label": null}
{"identifier": "-TwO99rbVRu|||0|||11", "text": "May the authors produce a performance analysis over T?\n - In Sec 3.4, it's not clear to me the advantage of proposed method on boundaries.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||12", "text": "CAM-based activations mostly focus on most discriminative areas (usually inner areas).", "label": null}
{"identifier": "-TwO99rbVRu|||0|||13", "text": "So hardly SGC can find pseudo-labels on boundaries.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||14", "text": "Why does the proposed method have an advantage there?", "label": null}
{"identifier": "-TwO99rbVRu|||0|||15", "text": "-", "label": null}
{"identifier": "-TwO99rbVRu|||0|||16", "text": "More and more segmentation works report results in urban datasets like cityscapes or camvid.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||17", "text": "It would be interesting to see results on those datasets.", "label": null}
{"identifier": "-TwO99rbVRu|||0|||18", "text": "One interesting aspect in urban datasets is the natural long-tail class distributions, which severely damages performance on minor classes, especially in low-data regime.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||0", "text": "Summary:", "label": null}
{"identifier": "-TwO99rbVRu|||1|||1", "text": "This paper focuses on the problem of semi-supervised semantic segmentation, where less pixel-level annotations are used to train the network.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||2", "text": "A new one-stage training framework is proposed to include the process of localization cue generation, pseudo label refinement and training of semantic segmentation.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||3", "text": "Inspire by recent success in the semi-supervised learning (SSL), a novel calibrated fusion strategy is proposed to incorporate the concept of consistency training with data augmentation into the framework.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||4", "text": "Experiments on PASCAL VOC and MSCOCO benchmarks validate the effectiveness of the proposed method.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||5", "text": "Pro:\n+", "label": null}
{"identifier": "-TwO99rbVRu|||1|||6", "text": "The proposed one-stage training framework is elegant compared with two stage methods in this area which include one step for pseudo-label generation and another step for refinement then semantic segmentation training.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||7", "text": "+", "label": null}
{"identifier": "-TwO99rbVRu|||1|||8", "text": "The new designed calibrated fusion strategy well incorporate the concept of consistency training with data augmentation into the same framework.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||9", "text": "+", "label": null}
{"identifier": "-TwO99rbVRu|||1|||10", "text": "Achieve a new state-of-the-art on both PASCAL VOC and MSCOCO benchmarks compared with recent semi-supervised semantic segmentation methods.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||11", "text": "Questions:\n- CCT (Ouali et al., 2020) includes the consistency training with perturbations which can be treated as a kind of data augmentation on features.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||12", "text": "I'm wondering if authors can provide some insights about why the proposed method can achieve better performance than CCT when they both include the consistency training and data augmentation in the designs.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||13", "text": "-", "label": null}
{"identifier": "-TwO99rbVRu|||1|||14", "text": "In table 3, I suggest to include the segmentation framework used by each method in the table.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||15", "text": "In early works, old version of deeplab is usually treated as the standard.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||16", "text": "I understand using deeplab v3 is a fair comparison with CCT.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||17", "text": "It would be good to make this information clear in the table.", "label": null}
{"identifier": "-TwO99rbVRu|||1|||18", "text": "-", "label": null}
{"identifier": "-TwO99rbVRu|||1|||19", "text": "It is also suggested to report the performance on PASCAL VOC test set as it is a common practice in this area (although CCT does not do so).", "label": null}
{"identifier": "-TwO99rbVRu|||1|||20", "text": "-", "label": null}
{"identifier": "-TwO99rbVRu|||1|||21", "text": "Sine the unlabeled data training branch does not rely on any pixel-level annotations, I'm wondering if the proposed method can also work under weakly-supervised setting, where no pixel-level annotations are available during the training.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||0", "text": "**Summary:**", "label": null}
{"identifier": "-TwO99rbVRu|||2|||1", "text": "This paper introduces a model to improve semantic segmentation by using a limited amount of pixel-labeled data and unlabeled data or image-level labeled data.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||2", "text": "The authors use a Self-attention Grad-CAM (SGC) and segmenter to generate the pseudo-labels during training.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||3", "text": "The approach shows good results on Pascal VOC and COCO datasets and is well analyzed.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||4", "text": "**Reasons for score:**", "label": null}
{"identifier": "-TwO99rbVRu|||2|||5", "text": "I do not think the technical contribution is strong enough for ICLR.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||6", "text": "The paper is incremental and the proposed approach is a combination of a lot of existing approaches.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||7", "text": "But I also want to highlight that the experimental section is strong and detailed.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||8", "text": "**Pros:**", "label": null}
{"identifier": "-TwO99rbVRu|||2|||9", "text": "- The idea of using pseudo-labels is interesting because it allows to build larger dataset without increasing the annotation cost.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||10", "text": "-", "label": null}
{"identifier": "-TwO99rbVRu|||2|||11", "text": "The approach is evaluated in the settings of using unlabeled data and using image-level labeled data.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||12", "text": "-", "label": null}
{"identifier": "-TwO99rbVRu|||2|||13", "text": "The ablation study section gives a lot of details about the model.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||14", "text": "The authors analyzed a lot of things: expected calibration error, hypercolumn feature, soft vs hard label, temperature sharpening, color jittering strength, backbone architecture.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||15", "text": "-", "label": null}
{"identifier": "-TwO99rbVRu|||2|||16", "text": "The approach shows good results on Pascal VOC and COCO datasets.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||17", "text": "-", "label": null}
{"identifier": "-TwO99rbVRu|||2|||18", "text": "The proposed method achieves good performance in the low-data regime", "label": null}
{"identifier": "-TwO99rbVRu|||2|||19", "text": "**Cons:**\n-", "label": null}
{"identifier": "-TwO99rbVRu|||2|||20", "text": "The overall approach seems incremental because it is a combination of a lot of existing approaches and there is not a strong technical contribution.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||21", "text": "For instance, the model uses several loss functions and all the losses are jointly optimized.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||22", "text": "- I think the related work section should be in the main paper instead of the supplementary.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||23", "text": "- I feel some parts are a bit difficult to read because of some misleading information.", "label": null}
{"identifier": "-TwO99rbVRu|||2|||24", "text": "For example, the title of section 3.1 is \u201cExperiments using unlabeled data\u201d but the model still uses some labeled data.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||0", "text": "Overview:", "label": null}
{"identifier": "-WwaX9vKKt|||0|||1", "text": "This submission focuses on the problem of visual question generation and proposes to use two hints: answer and visual regions.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||2", "text": "With the help of these two hints, a graph representation is built and then a GCN-based graph2sequence generator is applied to generate questions based on given inputs.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||3", "text": "Experiments on two datasets show its efficiency on question generation quality.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||4", "text": "Strengths:\n+", "label": null}
{"identifier": "-WwaX9vKKt|||0|||5", "text": "The description of the method (model) is easy to follow.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||6", "text": "+", "label": null}
{"identifier": "-WwaX9vKKt|||0|||7", "text": "Improvements of the question generation quality (according to the BLEU and Cider score) is big.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||8", "text": "+", "label": null}
{"identifier": "-WwaX9vKKt|||0|||9", "text": "Good ablation studies and human study.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||10", "text": "Weakness:\n- The motivation of VQG: I understand VQG is a recently raised task in vision-and-language research, but I can not get any ideas that why this is an important task, at least from this paper.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||11", "text": "According to my understanding, one reason that we need VQG is that we can use it as a data-augmentation tool to train a better VQA system (which is a more important task).", "label": null}
{"identifier": "-WwaX9vKKt|||0|||12", "text": "Another reason is that it may lead to a better  (goal-oriented) visual dialogue system, such as GuessWhat?!.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||13", "text": "This paper mentioned these two directions very briefly but failed to explain it well and there are no experiments presented in this paper to show whether their question generation model can benefit these two areas.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||14", "text": "I don't think a better BLEU score can show its potential since it only means this model can generate similar questions as the training data.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||15", "text": "It is worth to check whether these augmented questions can boost VQA performance.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||16", "text": "- The motivation of two hints:", "label": null}
{"identifier": "-WwaX9vKKt|||0|||17", "text": "It looks like many previous works only use answer types as the hint.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||18", "text": "I think this is more reasonable since it gives more freedom to the model to generate questions.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||19", "text": "And I don't think it is an issue that 'one answer/image can be potentially mapped to many different questions'.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||20", "text": "Instead, I think this provides a diversity of generated questions and further improves the VQA generalisation ability, from a data augmentation view.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||21", "text": "In this submission, authors choose to prove the answer directly as a hint.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||22", "text": "There might be two issues: at first, it seems not fair since it requires more annotation.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||23", "text": "Secondly, how can you get the answer during the inference?", "label": null}
{"identifier": "-WwaX9vKKt|||0|||24", "text": "Say if one downloads an image from the Internet  (not from a well-annotated VQA dataset) and there are three bears in the image, it might be easy if the answer is 'bear' since you can use a detector, but how can one know the answer is 'three' if we want to generate a question about 'how many?'.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||25", "text": "In this case, the answer type is more practical.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||26", "text": "According to the visual hints, if I am right, from section 4.1, they are mined from the questions and answers directly, for both training and testing.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||27", "text": "This suggests you (authors) already got some info from the target questions (even in the testing!).", "label": null}
{"identifier": "-WwaX9vKKt|||0|||28", "text": "I can't say it is a fair setting, even it happens in pre-processing step.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||29", "text": "Please fix me if I am wrong.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||30", "text": "- The 3.2.1 is basically an attention mechanism?", "label": null}
{"identifier": "-WwaX9vKKt|||0|||31", "text": "- What is the accuracy of the tasks listed in 3.2.2?", "label": null}
{"identifier": "-WwaX9vKKt|||0|||32", "text": "- In section 3.3, the \\epsilon seems quite important.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||33", "text": "It decides the sparsity of the graph.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||34", "text": "Any ablation studies?", "label": null}
{"identifier": "-WwaX9vKKt|||0|||35", "text": "- In section 4.1, another \\epsilon, which decides the quality of the visual hints.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||36", "text": "Any ablation studies?", "label": null}
{"identifier": "-WwaX9vKKt|||0|||37", "text": "---------------------------", "label": null}
{"identifier": "-WwaX9vKKt|||0|||38", "text": "After Rebuttal", "label": null}
{"identifier": "-WwaX9vKKt|||0|||39", "text": "--------------------------------", "label": null}
{"identifier": "-WwaX9vKKt|||0|||40", "text": "The authors did a good job in the rebuttal.", "label": null}
{"identifier": "-WwaX9vKKt|||0|||41", "text": "Most of my concerns have been addressed so I am happy to raise my score to 6.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||0", "text": "In this paper, the authors aim at generating the right questions based on the textual answers and corresponding visual regions of interest (ROIs).", "label": null}
{"identifier": "-WwaX9vKKt|||1|||1", "text": "The core innovation of the proposed method is leveraging the region information to supervise the question generation, which helps to mitigate the ambiguity of the answers.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||2", "text": "Correspondingly, a simple method is designed to generate the noisy annotations of the ROIs using the pretrained Masked-RCNN and the questions.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||3", "text": "This core part is mainly divided into two components: 1) aligning the object features with the answer word embeddings using attention mechanism; 2) constructing the object graphs and getting the GCN-refined object features; 3) refining the question embeddings by attending the refined object features and image ones.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||4", "text": "Experimental results show that the proposed method outperforms the existing approaches significantly.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||5", "text": "In addition, ablation studies prove the effectiveness of the proposed components.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||6", "text": "Although the visual regions of interest can help to guide the question generation, the one-to-many mapping issues still exist when generating the visual regions of interest (no additional information is given), which may lead to the same problem in the question generation stage.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||7", "text": "Nevertheless, the proposed approach is still valuable.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||8", "text": "Although it helps little in mitigating the one-to-many issues, it helps to learn a better question generation network, as the additional region information can mitigate the issue during the training stage, which will help to avoid learning a generator that prefers to give general questions.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||9", "text": "Therefore, I think authors should make that clear in the paper that it helps in learning a better question generator instead of generating the specific question because the ROIs are also selected based on the answers only.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||10", "text": "Although great improvements are achieved, the proposed method is not novel.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||11", "text": "The main novelty is leveraging the object features in generating questions.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||12", "text": "However, neither the feature alignment nor the GCN part is new.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||13", "text": "In addition, the graph construction part does not make sense to me, and no ablation study show that pruning the non-hints objects can help to improve the performance.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||14", "text": "In addition, the confusing equations and poor writing of the paper cannot make this paper an unaccepted one:\n* In Eq. (2), \\beta X^{a} seems like a matrix, which cannot be concatenated with a vector.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||15", "text": "* In Eq. (3), \\beta_{j} should be \\beta_{i, j}.\n* In Eq. (3), what I_{j} means?", "label": null}
{"identifier": "-WwaX9vKKt|||1|||16", "text": "Why the image feature has a subscript?", "label": null}
{"identifier": "-WwaX9vKKt|||1|||17", "text": "* In Eq. (3), what does the operation \u201c*\u201d means?", "label": null}
{"identifier": "-WwaX9vKKt|||1|||18", "text": "The product signs are different in (2) (3) and (4)\n* In Sec. 3.4, it lacks detailed explanations of how the model will attend the image and graph to get a better representation of the question embedding between the two LSTMs, which is one of the core components of the proposed methods.", "label": null}
{"identifier": "-WwaX9vKKt|||1|||19", "text": "* Typos like \u201csize information\u201d should be \u201cside information\u201d in the top paragraph on Page 2, \u201cquestion tokens\u201d should be \u201cword tokens\u201d in the bottom paragraph on Page 3.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||0", "text": "Summary:", "label": null}
{"identifier": "-WwaX9vKKt|||2|||1", "text": "The paper proposes a model for the task of Visual Question Generation (VQG) which uses the answer as well as object regions to generate the question.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||2", "text": "The paper models interactions between various visual entities and the answer tokens using a graph and then use it to generate the question.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||3", "text": "The proposed approach outperforms existing methods on the VQA and COCO-QA task.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||4", "text": "Strengths: \n-", "label": null}
{"identifier": "-WwaX9vKKt|||2|||5", "text": "The paper emphasizes on using both image-regions for the VQG task.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||6", "text": "While some of the proposed techniques borrows from existing works, they showed how to combine it to improve over exisitng methods.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||7", "text": "- On the VQA and COCO benchmarks, the models outperforms existing methods.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||8", "text": "On the small scale human study, the proposed approach was rated higher than existing approaches.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||9", "text": "Weaknesses: \n- Overall, the novely of the paper is low.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||10", "text": "The paper is a collection of already popular ideas (use of attented region features for V+L tasks (Shah et al.), use of position embeddings to model spatial relations (ViLBERT, VL-BERT etc).", "label": null}
{"identifier": "-WwaX9vKKt|||2|||11", "text": "Because, the paper is written as a collection of ideas, the take-away message isn't clear.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||12", "text": "- The ablation tables has very subtle performance changes across design choices which are hard to understand.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||13", "text": "The ablation table suggests that all choices in isolation leads to a drop when compared to the full model.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||14", "text": "But what happens when you combine these choices one by one.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||15", "text": "More importantly, what are the most critical components of the proposed method?", "label": null}
{"identifier": "-WwaX9vKKt|||2|||16", "text": "- Some of the statements made in the paper are not backed by citations or lack explanation.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||17", "text": "For instance, the authors claim that the \"amount of objects which are visual hints are much smaller than ones not\" and \"when humans ask questions from the image, we will infer whether the object is important clues by looking around in the image\".", "label": null}
{"identifier": "-WwaX9vKKt|||2|||18", "text": "- The paper was hard to follow and it seems like there are a lot of moving parts.", "label": null}
{"identifier": "-WwaX9vKKt|||2|||19", "text": "In general, this makes reproducing the paper and adopting ideas from the paper difficult.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||0", "text": "The paper introduces a new model on the task of Visual Question Generation.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||1", "text": "The model uses cross-modal alignment between the object features, position features and answer hints to find the right subset of relevant visual hints to be used to generate the relevant question.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||2", "text": "The model also ensures that the latent space features capture the answer and position information by predicting them back from it.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||3", "text": "Informed by the visual hints, the object and image features are passed to a GCN network that is used to get the final hidden state which is passed to an attention and language lstm similar to BUTD model to generate the final question.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||4", "text": "The cycle consistency used for answer and position features seems to provide grounded question generation for the answer and image.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||5", "text": "The model is tested on VQA2.0 and COCOQA and achieves better performance compared to the baseline model on automated metrics as well as human evaluation.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||6", "text": "Overall, the paper is strong and provides a solid foundation for the intuition and framework behind the model supported with detailed ablation analysis and case studies.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||7", "text": "What I find missing, is the actual test of how good these questions actually are by using them to train on the actual VQA task.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||8", "text": "Understanding that and how it performs as extra data on VQA 2.0 would give us a better understanding on how good the generation actually is where it matters.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||9", "text": "Some other questions:\n- Is there any specific reason why BERT wasn\u2019t used instead of GloVe for word embeddings?", "label": null}
{"identifier": "-WwaX9vKKt|||3|||10", "text": "- Please add citation for VQA 2.0", "label": null}
{"identifier": "-WwaX9vKKt|||3|||11", "text": "Overall, I would like to recommend the paper for acceptance but it is hard to understand the actual value of this work without downstream application on the task of VQA 2.0.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||12", "text": "Edit after rebuttal: I have read the author response and I thank the authors for their valuable insights and answers to my questions.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||13", "text": "It is exciting that this does help in improving the performance on downstream VQA2.0 task though I would have expected the results to be conducted on one of the recent state-of-the-art models instead of very old BUTD model where achieving performance gains is trivial.", "label": null}
{"identifier": "-WwaX9vKKt|||3|||14", "text": "I would like to keep my rating as it is.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||0", "text": "The paper proposes an Bayesian-symbolic physics (BSP), an intuitive physics model that jointly infers symbolic force laws and object properties (mass, friction coefficient).", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||1", "text": "The inductive bias is force summation, F=ma, and a grammar of force laws to express object interactions.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||2", "text": "The inference is done via an EM method that alternates between object property estimation (E-step) and force law induction (M-step), using techniques like symbolic regression and Hamiltonian Monte Carlo (HMC).", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||3", "text": "Some preliminary experiments are shown for the method's effectiveness and data efficiency.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||4", "text": "**Strength**:", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||5", "text": "The paper fills an important missing position in the spectrum of intuitive physics models, as Figure 1 argues.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||6", "text": "The force law grammar, to my knowledge, is something novel in this area, and represents a reasonable inductive bias that balances expressivity and physical plausibility (with two further physical constraints: dimensional analysis and reference invariance).", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||7", "text": "The grammar also helps improve data efficiency.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||8", "text": "From the inference side, the proposed EM approach is also reasonable.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||9", "text": "Symbolic regression for force law inference can be interesting for future research.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||10", "text": "**Weakness**:\nExperiments seem a bit weak for now, and I have some technical concerns about the inference.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||11", "text": "**Questions/suggestions**:\n1. Learning with unobserved properties (Figure 6) is the key experiment setting according to the paper's selling point, but is obviously lacking.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||12", "text": "It'd be great to see more scenarios (than graviton) with more diverse settings (e.g. object mass, initial position), some quantitive numbers, and comparison with some baseline (if possible).", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||13", "text": "2. For some qualitative sense, would also be nice to show true trajectories vs. predicted trajectories in the main paper, or if the learned symbolic law force is correct (for Figure 5).", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||14", "text": "Would be interesting to see if any symbolic laws are predicted wrong and how they look like.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||15", "text": "3. For object property inference, why use Monte Carlo methods over variational/gradient-based methods?", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||16", "text": "Also, is it possible to use MCMC for formula inference?", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||17", "text": "Some explanations or experiments would solidify design choices for the inference part.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||18", "text": "4. Is there an ambiguity when force constants and object properties are jointly inferred?", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||19", "text": "How tricky is designing priors for these?", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||20", "text": "(I see some tricks are used in the paper to handle small constants, for example)\n5.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||21", "text": "The paper currently only compares with one side of the spectrum, i.e. more neural approaches.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||22", "text": "Comparing BSP with more symbolic approaches like (Smith et al., 2019), the strength is supposed to be the ability to handle non-standard object interactions outside fixed physics engine but within the force grammar.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||23", "text": "Is it possible to generate some random force laws and show BSP still works while baselines from BOTH sides of the spectrum may fail?", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||24", "text": "It could be the most powerful experiment to support this position in the spectrum.", "label": null}
{"identifier": "-YCAwPdyPKw|||0|||25", "text": "In general I like the ideas and hope to see more experiments and justifications for design choices.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||0", "text": "This paper proposes a fully Bayesian approach to learn an intuitive physics model by combining symbolic regression and statistical learning (MCMC).", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||1", "text": "Pros:", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||2", "text": "+ Personally, I like this paper as it approaches the learning of the intuitive physics model in the right direction, highlighted in figure 1.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||3", "text": "It is a nice combination of symbolic and learning-based approach.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||4", "text": "Cons:", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||5", "text": "- The related work section is too brief to see the main difference between the present work and prior work.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||6", "text": "The current related work section only focuses on the machine learning-based intuitive physics model but does not cover symbolic regression in general.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||7", "text": "Using symbolic regression has a long history, especially in material science, soft robotics, and machine learning in general.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||8", "text": "Prior work has demonstrated that SR can indeed learn the physical law in a much more complex setting [1-2].", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||9", "text": "- For researchers who are opposed to the idea of intuitive physics, they would ask where the prior knowledge comes from.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||10", "text": "Some cognitive research has shown that they are innate for humans and some animals.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||11", "text": "But this rule cannot be applied for a machine:", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||12", "text": "If we give them all the rules, why not just directly use the physics engine?", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||13", "text": "In the experiments, the results demonstrated here are too simple so that most of the naive physics-based simulator can produce similar results at a probably faster speed.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||14", "text": "The question is, what the benefit of learning?", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||15", "text": "One may argue that if I know some properties individually, I might be able to transfer this knowledge to unseen scenarios.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||16", "text": "The authors do demonstrate some capability in 3.3.2 and 4.2, but it seems to come naturally with SR based on the given prior, not something new or surprise.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||17", "text": "Such capabilities have been demonstrated in [1-2].", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||18", "text": "- I would like to see if the authors would be able to demonstrate the learned physics can be generalized to scenarios beyond the training datasets.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||19", "text": "For instance, can the learned model from MAT dataset be transferred to simple scenarios created by bullet-like engine with a similar friction-based interaction, but not identical?", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||20", "text": "The learned physical knowledge should be general enough.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||21", "text": "Even not as the perfectly correct Newtonian physics, it should be able to generalize to unseen scenarios.", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||22", "text": "[1] Distilling free-form natural laws from experimental data, Science 2009\n[2]", "label": null}
{"identifier": "-YCAwPdyPKw|||1|||23", "text": "AI Feynman: A physics-inspired method for symbolic regression, Science Advance 2020", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||0", "text": "The paper proposes a fully Bayesian approach to symbolic intuitive physics that, by combining symbolic learning of physical force laws and statistical learning of unobserved properties of objects.", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||1", "text": "The paper proposes an EM-based method where in E-step object properties distribution are sampled using the current best estimated force laws and in M-step symbolic regression is used to update those laws.", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||2", "text": "The paper is clearly-written with clear explanation of the proposed EM-style method.", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||3", "text": "However, one of the main claim that the method \"enjoys the sample efficiency of symbolic methods with the ac- curacy and generalization of data-driven learned approaches\" is not well-supported in the experiments.", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||4", "text": "While in Sec. 4.1 it shows the proposed method is more data efficient than the neural baseline, it's not clear how the method generalized to complete different scenarios.", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||5", "text": "Ideally the symbolic physical laws are universal thus can be applied to all physical interactions.", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||6", "text": "Some other questions / comments:\n- One of the contribution states \"Through empirical evaluations, we demonstrate that the BSP approach reaches human-like sample efficiency, often just requiring 1 to 5 observations to learn the exact force laws \u2013 usually more than 10x fewer than that of the closest neural alternatives.\"", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||7", "text": "This (10x more data-efficient) is hard to tell from figure 5 as it doesn't show when the neural baseline reaches the same performance.", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||8", "text": "-", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||9", "text": "The error bars in figure 5 is somewhat not very indicative of the stability of the method and some of them for the neural baseline are extremely large.", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||10", "text": "It states that  the values are \"out of five experiments with different shuffling of the training set\".", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||11", "text": "How many different shuffling is there?", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||12", "text": "- Sec 3.3.1 \"In practice, as the cross-entropy method itself is sensitive to random initializations, in order to ro- bustify the M-step, we repeat it for r runs and pick the best optimizer.\"", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||13", "text": "Is 'm' supposed to be r in Algorithm 2? \n- For figure 6, if it converges after iteration 3, can we show the expression tree for F3 instead F5?", "label": null}
{"identifier": "-YCAwPdyPKw|||2|||14", "text": "How much do they differ?", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||0", "text": "The paper addresses the problem of sample-efficient inference for symbolic physical rules.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||1", "text": "In the literature, there exists neural-network based models for learning a physical engine which have good predictive accuracy but poor sample efficiency, as well as symbolic models which are highly sensitive to deviations from their fixed physics engine.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||2", "text": "To be able to overcome issues as such, authors propose a generative model along with a symbolic regression framework, in which forces are produced from a probabilistic context free grammar that is designed to mimic simple Newtonian physics.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||3", "text": "This particular grammar is parameterized by a few latent variables related to unobserved properties of the physical environment, such as mass and charge.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||4", "text": "Finally, they develop an Expectation-Maximization algorithm, in order for estimating these latent variables as well as inferring the underlying physical laws of the system.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||5", "text": "The methodology described in the paper enables prior physical knowledge to be incorporated into the statistical machine learning models in the form of a probabilistic context free grammar, which in return makes inference via EM applicable.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||6", "text": "The paper is well written in general, i.e. the main idea of the paper is easy to grasp and all the related technical concepts are explained in a very simple way.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||7", "text": "Mathematical notation is clear and consistent.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||8", "text": "Proposed methodology is novel and sound.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||9", "text": "Claims of the authors are supported by the experimental results on simulated datasets, and there seems to be no fallacy in empirical evaluations.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||10", "text": "Nevertheless, experimental section can be extended by large-scale applications or real world datasets.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||11", "text": "In other words, learning the underlying physical rules from a data set which is collected through real-world sensors would provide further evidence to the impact of such a framework.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||12", "text": "In addition, I think it\u2019s necessary to include more results from symbolic and neural-network based learned models as a reference.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||13", "text": "Although relation to prior work is clearly addressed in the paper, authors compare their results to only OGN.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||14", "text": "In my opinion, learning symbolic rules for physical reasoning and prediction in a sample-efficient manner will certainly be of interest to the ICLR community.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||15", "text": "I believe that the proposed methodology is clearly worth exploring, and the presented experimental results are promising.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||16", "text": "On the other hand, it is no surprise that a carefully constructed grammar for physics is sample-efficient.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||17", "text": "Authors chose to restrict the grammar with a small set of production rules, in order to narrow down the search space for deriving force terms.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||18", "text": "However, this can be also seen as a compromise from the generality of the model, since such a predefined grammar might be too restrictive and it might necessitate adding new rules for each new physical phenomena.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||19", "text": "For instance, in the fields related to optics or thermodynamics, to be able to model phenomena such as refraction-diffraction of light or heat transfer, one might need to modify the grammar manually.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||20", "text": "Another drawback of the methodology could be enforcing position and velocity vectors to reside in the Cartesian coordinate system.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||21", "text": "Such a choice limits the expressiveness of the model, for instance allowing a general coordinate system could make it easier to model harmonic movement via angular coordinates.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||22", "text": "In short, my major concern is that enforcing too many constraints inspired by the physics rules we already know introduces a bias, which in return affects the generalization of the model.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||23", "text": "My final remark is about the EM algorithm in the paper.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||24", "text": "I think that the details about the EM should be written more clearly and thoroughly.", "label": null}
{"identifier": "-YCAwPdyPKw|||3|||25", "text": "I recommend writing the generative model explicitly which could improve the clarity of the paper in general, as well as allowing different inference algorithms to be applicable.", "label": null}
{"identifier": "-_Zp7r2-cGK|||0|||0", "text": "The innovative part of the work consists in the trick (8), which allows for a kernel-based generalisation of Gaussian mixture models viewed under a discriminative perspective.", "label": null}
{"identifier": "-_Zp7r2-cGK|||0|||1", "text": "The model is trained via maximum a posteriori estimation; due to the analytically intractable form of the posterior expectation of the log-likelihood, the authors resort to a second-order Taylor approximation around the mode (Laplace approximation).", "label": null}
{"identifier": "-_Zp7r2-cGK|||0|||2", "text": "That's an old and not so fine approximation, but easy and widely-known.", "label": null}
{"identifier": "-_Zp7r2-cGK|||0|||3", "text": "The experiments are performed on standard benchmarks, and comparisons are provided against some natural competitors of the method.", "label": null}
{"identifier": "-_Zp7r2-cGK|||0|||4", "text": "In essence, the authors replace the penultimate softmax layer of a deep net classifier with their model and train end-to-end.", "label": null}
{"identifier": "-_Zp7r2-cGK|||0|||5", "text": "The experiments yield a marginal improvement over s.o.t.a., as so many papers do nowadays.", "label": null}
{"identifier": "-_Zp7r2-cGK|||0|||6", "text": "An aspect that is missing is how they initialised the models.", "label": null}
{"identifier": "-_Zp7r2-cGK|||0|||7", "text": "Initialisation may  play a crucial role in such approaches, especially when it comes to ensuring reproducibility of the results.", "label": null}
{"identifier": "-_Zp7r2-cGK|||0|||8", "text": "In summary, a slightly novel paper with some interesting insights and some pretty standard nowadays, yet marginally impressive experimental results.", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||0", "text": "* quality", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||1", "text": "The paper presents an interesting idea that uses sparse Gaussian mixtures, but it lacks theoretical guarantees.", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||2", "text": "Although the method is Bayesian, can we also give frequentist non-asymptotic bounds?", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||3", "text": "* clarity", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||4", "text": "The paper is well written.", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||5", "text": "* originality", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||6", "text": "The paper's ideas seem original, but they're very straightforward, making its contributions marginally incremental.", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||7", "text": "* significance", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||8", "text": "If the paper had more theoretical guarantees, its results would be more significant.", "label": null}
{"identifier": "-_Zp7r2-cGK|||1|||9", "text": "The current version is a bit weak.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||0", "text": "The paper proposes an interesting extension to both discriminative GMMs and CNNs with a softmax outuput.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||1", "text": "The key innovation is the introduction of the sparsity in discriminative, multimodal settings.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||2", "text": "This novelty and the clear experiments merit this work to be published at ICLR 21.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||3", "text": "However, the testing and evaluation could be significantly improved.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||4", "text": "There are a few areas where the paper could be improved.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||5", "text": "1. The language could use another review.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||6", "text": "Often, the writing slips into the use of informal language.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||7", "text": "2. Appendices were missing in my version; the paper makes an explicit reference to at least appendix A.\n3.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||8", "text": "The connection to Bayesian methods is week.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||9", "text": "4. The table describing Algorithm 1 appears abruptly and references formulae that have not been introduced.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||10", "text": "It is best positioned at the end of Section 3 to improve readability.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||11", "text": "5. Most of the Bayesian approaches used in the comparison are dated.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||12", "text": "6. Section 4.2 references high computational costs, but it is unclear which steps of the algorithm make this approach computationally prohibitive.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||13", "text": "7. It is unclear from Table 1 that SDGM is significantly better than RVM.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||14", "text": "A deep dive is warranted to better understand their relative performances.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||15", "text": "8. It would be better if Section 4.2.1 also include large datasets or more challenging domains.", "label": null}
{"identifier": "-_Zp7r2-cGK|||2|||16", "text": "The relative error reduction described in Table 2 on a couple of datasets are marginal.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||0", "text": "The paper proposes a sparse classifier via  discriminative GMM.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||1", "text": "This model is trained based on sparse Bayesian learning.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||2", "text": "The sparsity constraint removes redundant Gaussian components which results in  reducing the number of parameters and improving the generalization.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||3", "text": "This framework can potentially be embedded into the deep models and trained in an end-to-end fashion.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||4", "text": "The main motivation is that the proposed model (i.e., SDGM,)  can consider multimodal data while conventional softmax classifiers only assume unimodality for each class.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||5", "text": "Experimental results show the superiority of the SDGM over existing softmax-based discriminative models.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||6", "text": "The paper is well-written and easy to follow.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||7", "text": "And the paper precisely places the proposed method among the related work.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||8", "text": "However, there are some concerns:", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||9", "text": "1- Sparsity constraint is supposed to improve the performance and generalization, but experimental results do not support this.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||10", "text": "What is the motivation of employing sparsity learning in this framework?", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||11", "text": "2- Conventional softmax classifiers in deep architecture have already provided promising results on real world datasets such as ImageNet which contains classes with multimodal data.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||12", "text": "However, in this paper, experiments are performed on the small datasets, so was the proposed method evaluated on ImageNet as well?", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||13", "text": "3- There are many versions of softmax-classifiers such as \u201clarge-margin softmax\u201d, \u201cangular softmax\u201d, and \u201cadditive margin softmax\u201d , and  [1 ,2 ] that address conventional softmax-classifiers\u2019 issues.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||14", "text": "Have you compared your models with them?", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||15", "text": "[1] Liu W, Wen Y, Yu Z, Yang M. Large-margin softmax loss for convolutional neural networks.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||16", "text": "InICML 2016 Jun 19 (Vol. 2, No. 3, p. 7).", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||17", "text": "[2] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||18", "text": "SphereFace: Deep hypersphere embedding for face recognition.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||19", "text": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 212\u2013220, 2017", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||20", "text": "[3] Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||21", "text": "Additive margin softmax for face verification.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||22", "text": "IEEE Signal Processing Letters, 25(7):926\u2013930, 2018.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||23", "text": "[4] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||24", "text": "In Advances in Neural Information Processing Systems, pp. 1567\u20131578, 2019.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||25", "text": "[5] Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Mingsheng Long, and Han Hu. Negative margin matters: Understanding margin in few-shot classification.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||26", "text": "arXiv preprint arXiv:2003.12060, 2020.", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||27", "text": "4-  Finally, I could not find any guarantee for the convergence of the learning algorithm in the paper?", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||28", "text": "What is the time-complexity?", "label": null}
{"identifier": "-_Zp7r2-cGK|||3|||29", "text": "I think training with softmax would be much faster, easier and also provides promising classification results in practice.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||0", "text": "The paper establishes a relationship between self-supervised learning (SSL) and supervised few-shot learning (FSL) method and shows that when both are equivalent.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||1", "text": "The whole analysis and proof are based upon the two main assumptions: mean classifier and balanced class training data.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||2", "text": "The paper shows that if we have a too large number of classes in the SSL, then it is equivalent to the supervised learning scenario and model enjoy the same generalization ability.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||3", "text": "Always supervised loss is the upper bound by the SSL loss.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||4", "text": "Comment:\n1: The paper theoretically connects the SSL and FSL and shows when both will be equivalent.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||5", "text": "Theorem-1 shows that the supervised loss is upper bound by SSL loss by a linear relation (mostly scale+shift) when |C|-->infinity then both loss is equivalent.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||6", "text": "It seems that Theorem-1 is trivial since it is obvious that for the large class there will be very less chance of the negative pair is incorrect (i.e. false negative).", "label": null}
{"identifier": "-aThAo4b1zn|||0|||7", "text": "If all the negative pair is correct, then it is same as we know the class label and we make the negative pair using the class information of all samples.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||8", "text": "I believe this theorem provides less useful information for a practical perspective.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||9", "text": "2: Theorem 2 provides the underlying factor between the L_sup and L_U, and shows that L_sup loss is upper bound by the loss of the true-negative and the intraclass variance.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||10", "text": "For the small variance, we can reduce the gap between the supervised loss and SSL loss.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||11", "text": "Once a trivial solution is when |C|--> infinity.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||12", "text": "This theorem shows then when |C| is not large still we can still focus on reducing the intraclass variance and reduce the gap.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||13", "text": "3: It is clear that if we have large number of class, we can reduce the gap between the supervised loss and self-supervised loss, but why the large batch size help in to get a practically better result?", "label": null}
{"identifier": "-aThAo4b1zn|||0|||14", "text": "In this case, the probability of the false-negative samples is the same, and it does not depend on the batch size.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||15", "text": "Could you please explain that?", "label": null}
{"identifier": "-aThAo4b1zn|||0|||16", "text": "It is written that \"We can increase N by increasing the total negative samples N_k\", is true but in the total negative samples the probability of the false-negative will be same, and it depends on the number of class only.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||17", "text": "Then how large batch size help?", "label": null}
{"identifier": "-aThAo4b1zn|||0|||18", "text": "4: In the N-way and M-shot, it is intuitive that when M increase the model performance will increase, but why with the increase of the N model performance will increase?", "label": null}
{"identifier": "-aThAo4b1zn|||0|||19", "text": "5: Omniglot dataset has 1623 classes, while in the paper it is written that \"Omniglot involves up to 4800 classes\" please check that.", "label": null}
{"identifier": "-aThAo4b1zn|||0|||20", "text": "https://github.com/brendenlake/omniglot", "label": null}
{"identifier": "-aThAo4b1zn|||1|||0", "text": "The paper proposes to theoretically analyze whether self-supervised learning can help FSL.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||1", "text": "Under simplified assumptions (a simple mean classifier is used; training data is balanced; and a particular form of loss is used), the main result in Theorem 1 shows that self-supervised training loss is an upper bound of the supervised metric loss function.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||2", "text": "The idea is interesting and inspiring.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||3", "text": "However, the analysis is less satisfactory.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||4", "text": "The main concern is that Theorem 1 and 2 are quite loose.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||5", "text": "They only apply for the so-called  supervised metric loss function.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||6", "text": "Is it work for any fk and fq?", "label": null}
{"identifier": "-aThAo4b1zn|||1|||7", "text": "Can you provide more strict error bound to quantify the difference?", "label": null}
{"identifier": "-aThAo4b1zn|||1|||8", "text": "As said in the paper, \"\u03b30, \u03b4 are constants depending on the class distribution \u03c1\", then how to estimate \u03b30, \u03b4?", "label": null}
{"identifier": "-aThAo4b1zn|||1|||9", "text": "If they cannot be estimated, why we need this theory?", "label": null}
{"identifier": "-aThAo4b1zn|||1|||10", "text": "How to link this theory to the success of self-supervised learning in solving FSL problem?", "label": null}
{"identifier": "-aThAo4b1zn|||1|||11", "text": "Or can this theory be validated empirically?", "label": null}
{"identifier": "-aThAo4b1zn|||1|||12", "text": "I think this paper indeed proposes an interesting direction to explore.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||13", "text": "But without answering the above questions, the current version is not complete enough to be published.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||14", "text": "===", "label": null}
{"identifier": "-aThAo4b1zn|||1|||15", "text": "During discussion period, I noticed import missing references of this paper as written by Nikunj Saunshi.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||16", "text": "Besides, the authors do not respond to any of the reviewers' questions.", "label": null}
{"identifier": "-aThAo4b1zn|||1|||17", "text": "Hence I change my score to strong rejection.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||0", "text": "*** Key idea justification ***", "label": null}
{"identifier": "-aThAo4b1zn|||2|||1", "text": "This work shows that contrastive loss (for self-supervised learning) is an upper bound of cross-entropy loss (for supervised learning) and leads to a conclusion that this is the underlying reason why self-supervised learning can help supervised learning in FSL.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||2", "text": "This reasoning makes little sense with little logic.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||3", "text": "Concretely, there exist a number of to-be-answered questions before connecting the two things and making theoretical conclusion:", "label": null}
{"identifier": "-aThAo4b1zn|||2|||4", "text": "1) Why we need to know the upper bound of supervised learning loss given that we already have label data with the training data?", "label": null}
{"identifier": "-aThAo4b1zn|||2|||5", "text": "2) Decreasing SSL loss does not necessarily mean that supervised learning loss is also decreased, as it is just an upper bound.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||6", "text": "No guarantee there.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||7", "text": "3) Assume SSL helps decrease the supervised learning loss, then why is this needed when we can simply use class labels to minimize it?", "label": null}
{"identifier": "-aThAo4b1zn|||2|||8", "text": "Intuitively, the two are overlapping and SSL should be not useful.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||9", "text": "Besides, this paper only considers the case of contrastive loss which involves false negative samples.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||10", "text": "What if applying other SSL loss function, for example rotation?", "label": null}
{"identifier": "-aThAo4b1zn|||2|||11", "text": "I do see the same analysis applies to that.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||12", "text": "In conclusion, the proposed theory makes little sense and is also over-claimed.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||13", "text": "The whole study is neither theoretical nor logical.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||14", "text": "*** Presentation clarity ***", "label": null}
{"identifier": "-aThAo4b1zn|||2|||15", "text": "1) In general, the presentation of this paper is poor.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||16", "text": "One reason is using odd/strange terminologies and equation expressions.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||17", "text": "For example, contrastive loss (Eq 1) and cross-entropy loss (Eq 3) both are not given in their common expression.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||18", "text": "Other examples are \"Supervised Metric for Representations\" and \"Self-Supervised Metric (SSM) for Representations\", \"a metric loss\", etc.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||19", "text": "2) Quite a few equations are hard to read and understand.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||20", "text": "First, Eq (1) and (3) are not expressed in a standard way.", "label": null}
{"identifier": "-aThAo4b1zn|||2|||21", "text": "How are they derived?", "label": null}
{"identifier": "-aThAo4b1zn|||2|||22", "text": "3) What is the difference between a class-wise prototype pc and an episodic mean of support samples (At the end of Sec 3).", "label": null}
{"identifier": "-aThAo4b1zn|||2|||23", "text": "4) What means by \"the class distribution \u03c1 is uniform\" in the proof of Theorem 2?", "label": null}
{"identifier": "-aThAo4b1zn|||2|||24", "text": "5) What is implied by the last sentence of Sec 4: Theoretically, if given an unsupervised set with infinite classes and data, the performance achieved by SSM can be very close to that by supervised training?", "label": null}
{"identifier": "-aThAo4b1zn|||2|||25", "text": "*** Grammatical errors ***\n1) a episodic -> an episodic", "label": null}
{"identifier": "-aThAo4b1zn|||3|||0", "text": "This paper performs theoretical analysis of the relationship between supervised learning (SL) and self-supervised learning (SSL) in the context of few-shot learning (FSL).", "label": null}
{"identifier": "-aThAo4b1zn|||3|||1", "text": "It aims to quantify the gap in training loss between SL and contrastive SSL on FSL tasks by casting SSL as an SL problem.", "label": null}
{"identifier": "-aThAo4b1zn|||3|||2", "text": "Using this formulation, the authors show that the self-supervised training loss is an upper bound of the supervised metric loss function, implying that if you reduce the self-supervision loss to be small enough, you can control the model\u2019s supervision loss on the training data, and thus improve results on the downstream FSL tasks.", "label": null}
{"identifier": "-aThAo4b1zn|||3|||3", "text": "The theoretical formulation also provides guidelines for the optimal values for the queue size in contrastive SSL, which the authors evaluate on omniglot and miniImageNet datasets, showing that the test performance varies with queue size.", "label": null}
{"identifier": "-aThAo4b1zn|||3|||4", "text": "Strengths:", "label": null}
{"identifier": "-aThAo4b1zn|||3|||5", "text": "The motivation to perform theoretical analysis on the utility of SSL for few-shot learning is a good one.", "label": null}
{"identifier": "-aThAo4b1zn|||3|||6", "text": "While I could not check the proofs thoroughly, they seem to provide a nice framework for explaining why SSL might provide good performance on few-shot learning.", "label": null}
{"identifier": "-aThAo4b1zn|||3|||7", "text": "Weaknesses and suggestions: 1. The paper is very difficult to follow.", "label": null}
{"identifier": "-aThAo4b1zn|||3|||8", "text": "While the theory section (Section 4) is reasonably well-written, the rest of the paper needs a substantial rewrite to improve clarity and accessibility.", "label": null}
{"identifier": "-aThAo4b1zn|||3|||9", "text": "Unfortunately the writing quality makes it difficult  to make a strong case for the paper.", "label": null}
{"identifier": "-aThAo4b1zn|||3|||10", "text": "2. The experiments only touch upon one aspect of theory discussed in the paper -- the impact of N and M on test performance.", "label": null}
{"identifier": "-aThAo4b1zn|||3|||11", "text": "A more  thorough comparison with SL based few-shot learning and the impact of other factors like number of classes and class imbalance on test performance would make the paper stronger.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||0", "text": "#### Summary\n-", "label": null}
{"identifier": "-aThAo4b1zn|||4|||1", "text": "The authors analyze a self-supervised learning framework for downstream (supervised) few-shot classification.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||2", "text": "The self-supervised stage is a simplified version of MoCo (He et al. 2019) and relies on class-invariant augmentation of unlabeled data to produce samples for a contrastive loss.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||3", "text": "This produces two encoder networks that are used in the subsequent few-shot learning stage via a distance-based classification scheme similar to that used by Snell et al. (2017), [1], [2], and Chen et al. (2019).", "label": null}
{"identifier": "-aThAo4b1zn|||4|||4", "text": "- The authors show that the method minimizes an upper bound on an oracle supervised distance-based classification loss.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||5", "text": "They then further analyze the looseness by decomposing the self-supervised loss into contributions from false-negative and true-negative samples.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||6", "text": "They relate these quantities to key methodological considerations, such as the level of diversity in the meta-training/base data and the number of negative samples to use during contrastive learning.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||7", "text": "- The authors assess this method on the Omniglot and miniImageNet few-shot datasets, following the setup proposed by Hsu et al. (2019) in which the meta-training (aka base) split is treated as unlabeled.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||8", "text": "The results are strong, though are curiously relegated entirely to the Appendix.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||9", "text": "#### Strengths\n-", "label": null}
{"identifier": "-aThAo4b1zn|||4|||10", "text": "The overall pipeline is to my knowledge novel, even though the authors are careful to state that the method is not a core contribution as it draws heavily from prior methods.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||11", "text": "Unlike previous works that consider unsupervised/self-supervised pre-training for few-shot learning, this work provides some theoretical justification for its method.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||12", "text": "- Due to the judicious choice of considering contrastive learning and distance-based classification, the resulting analysis is relatively straightforward.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||13", "text": "#### Weaknesses\n-", "label": null}
{"identifier": "-aThAo4b1zn|||4|||14", "text": "This submission is overall poorly written.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||15", "text": "It was very difficult to parse due to a copious number of grammatical errors.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||16", "text": "In numerous instances, I can't quite discern what the authors mean.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||17", "text": "Aside from this, there are many vague statements unsupported by reference or argument.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||18", "text": "- The organization leaves much to be desired.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||19", "text": "For example, results of an ablation take center stage in the main text, while key experimental exposition and benchmark results are left entirely to the Appendix.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||20", "text": "- Comparison to CACTUs (Hsu et al., 2019) is not entirely fair as the method (like most modern contrastive learning methods) requires the specification of instance transformations that are class-invariant for test tasks.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||21", "text": "This should be noted.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||22", "text": "(Though comparison to UMTRA (Khodadadeh et al., 2019) is fair.)", "label": null}
{"identifier": "-aThAo4b1zn|||4|||23", "text": "#### Recommendation\n- I currently recommend rejection (3), as the submission's poor writing severely hampers clarity and thus prevents it from meeting publication standards.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||24", "text": "If the writing were fixed, I would probably rate it around a 6.", "label": null}
{"identifier": "-aThAo4b1zn|||4|||25", "text": "#### References\n- [1]", "label": null}
{"identifier": "-aThAo4b1zn|||4|||26", "text": "Qi et al., Low-Shot Learning with Imprinted Weights, CVPR 2018\n- [2]", "label": null}
{"identifier": "-aThAo4b1zn|||4|||27", "text": "Gidaris et al., Dynamic Few-Shot Visual Learning without Forgetting, CVPR 2018", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||0", "text": "In this work the authors focus on self-supervised representation learning.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||1", "text": "This work proposes a composite loss function which includes contrastive loss, predictive loss, and a novel inverse predictive loss.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||2", "text": "The authors have provided theoretical analysis as well as experimental results on multiple datasets under control environment.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||3", "text": "Pros:", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||4", "text": "This work presents a very detailed theoretical analysis for self-supervised learning objectives.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||5", "text": "The idea of inverse predictive learning for filtering task irrelevant information is interesting.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||6", "text": "Cons:", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||7", "text": "The inverse predictive learning appears to be similar to contrastive learning if we ignore the negative samples in the contrastive learning formulation and use the projection head from contrastive learning for inverse predictive learning.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||8", "text": "How will the authors differentiate between these two formulations?", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||9", "text": "The variation in the performance shown in Figure 3 is very marginal.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||10", "text": "Also, only selective results are shown in Figure 3.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||11", "text": "No results are shown on CIFAR-10 which uses all the three losses at the same time.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||12", "text": "Figure 5 a shows some results on Omniglot, but the improvement shown there is very marginal.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||13", "text": "Also, how was the loss weights determined?", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||14", "text": "Experiment on a large scale dataset, maybe ImageNet will be very useful to demonstrate the effectiveness of the proposed loss.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||15", "text": "The weights required for inverse predictive learning in the loss formulation is not trivial.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||16", "text": "As shown in Figure 3, it varies with different datasets and varies a lot (1 for Omniglot and 0.1 for CIFAR10).", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||17", "text": "Is there a simple way to determine this weights without exhaustive search on target dataset?", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||18", "text": "This work mainly focuses on the theoretical aspect of semi-supervised learning and have also presented an interesting inverse predictive learning task for self-supervision which can dis-regard task irrelevant information.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||19", "text": "However, it is not clear from the experimental results if this is really effective.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||20", "text": "The results shown in Figure 3 and Figure 4 are not sufficient to demonstrate its effectiveness.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||21", "text": "Also, quantitative comparison with existing approaches will also strengthen this work, which is only partially addressed.", "label": null}
{"identifier": "-bdp_8Itjwp|||0|||22", "text": "A more in-depth experimental analysis (apart from the theoretical analysis) is required to show its usefulness.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||0", "text": "This paper explores both theoretical and empirical perspectives on self-supervised learning.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||1", "text": "The authors attempt to prove that self-supervised learning could extract task-relevant information while discard task-irrelevant information.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||2", "text": "They also provide a composite objective that includes contrastive and predictive loss functions along with an additional regularization.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||3", "text": "Controlled experiments are conducted to support the presented theorems.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||4", "text": "There are three major concerns for me.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||5", "text": "1)\tThe experiments are conducted in a controlled way, including visual representation learning and visual-textual representation learning.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||6", "text": "Traditional uncontrolled experiments, such as unsupervised learning on Cifar10 or ImageNet are suggested.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||7", "text": "2)\tThe results in Fig. 3 and Fig. 5 demonstrate that the performance is sensitive to the hyper-parameter of $L_{IP}$.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||8", "text": "How to set the hyper-parameter $\\lambda_{IP}$  in practice?", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||9", "text": "Besides, the best performance achieved by $L_{IP}$ is only marginally better to those without it.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||10", "text": "3)\tSome previous related works, such as InfoMin [Tian et al., 2020], are suggested to discussed and compared with more details.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||11", "text": "As far as I know, InfoMin analyzed influence of different view choices using mutual information theory.", "label": null}
{"identifier": "-bdp_8Itjwp|||1|||12", "text": "Therefore, it would be helpful to compare the similarities and differences between this work with it.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||0", "text": "Summary: \nAuthors give an information-theoretical abstraction to represent various self-supervised learning methods and understand them better.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||1", "text": "Specifically, authors claim that self-supervised learning methods can extract task-relevant information and discard task-irrelevant information.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||2", "text": "Authors also do some controlled experiments and try to support their theoretical analysis.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||3", "text": "Recommendation: I recommend to accept the paper (rating 7).", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||4", "text": "I liked the abstraction proposed by authors and particularly liked the way authors set up the Definition 1 and analysis afterwards.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||5", "text": "Ratings can be improved further if authors can relate experimental setup more to the theory which I find slightly disconnected.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||6", "text": "1) In assumption 1: I(X;T|S) <= \\epslion.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||7", "text": "If task-relevant information lies mostly in the shared information between the input and the self-supervised signals then I(S;T|X) <= \\epslion, should also be true, right?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||8", "text": "2) Can authors give an intuitive proof sketch for Theorem 1?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||9", "text": "3) In Proposition 1 and Theorem 3, it looks like d can be arbitrarily large and even scale with n.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||10", "text": "Is there any upper bound in d?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||11", "text": "Would this invalidate all claims?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||12", "text": "4) In Figure 3 a and 3b, waht happens when you don\u2019t use L_{CL} but use L_{FP} instead?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||13", "text": "5) Figure 3 c is not clear.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||14", "text": "Why L_{CL} overfits and why does adding L_{FP} avoid that over-fitting?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||15", "text": "Why did you use \\lambda_{FP} = 0.005?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||16", "text": "6) From figure 3, It looks like L_{CL} is good enough if early stopping is used properly.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||17", "text": "How does one justify the use of FP and IP?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||18", "text": "One can also use some regularization along with CL (would the regularization act as \u201cdiscarding  task-irrelevant  information\u201d)?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||19", "text": "7) How were hyper-parameters (learning rate, batch size, etc) chosen for all the experiments?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||20", "text": "Was proper hyper-parameter tuning done for all the methods?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||21", "text": "Can one achieve the same best results as in Figure 3a and 3b with proper tuning of method when \\lambda_{IP} is zero?", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||22", "text": "8) It is not very convincing that controlled experiments in section 3 actually support Theorem 1 and 2.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||23", "text": "\u201cTheorem 1 indicates that the self-supervised learned representations can extract almost as much task-relevant information as the supervised on\u201d and \u201cTheorem 2 indicates that a compression gap (i.e.,I(X;S|T)) exists when we discard the task-irrelevant information from the input\u201d.", "label": null}
{"identifier": "-bdp_8Itjwp|||2|||24", "text": "Can authors clearly point out how conclusions from Figure 3 and Figure 4 are exactly supporting the respective statements of theorem 1 and 2 respectively?", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||0", "text": "Strengths:", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||1", "text": "Analyze the self-supervised learning methods from extracting task-relevant information and discarding task-irrelevant information from the input theoretically and clearly.", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||2", "text": "Propose a new objective to discard task-irrelevant information explicitly.", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||3", "text": "And it seems easy to combine with other SOTA self-supervised methods.", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||4", "text": "Conduct experiments on visual setting and multi-modal setting.", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||5", "text": "Weakness:", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||6", "text": "I like the idea of discarding the redundant task-irrelevant information to improve the self-supervised learning.", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||7", "text": "However, the composite objective proposed in this paper seems just like a simple combination of three tasks, which is not strikingly novel.", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||8", "text": "another concern of this paper is the lack of persuasive experiment results to prove the effectiveness of the proposed method.", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||9", "text": "In fig.3, the improvements on two dataset are marginally, which can not convince me.", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||10", "text": "The \\lambda (\u03bb_IP) in proposed objective function seems not robust to different datasets, which makes me doubt about the generalization of this method.", "label": null}
{"identifier": "-bdp_8Itjwp|||3|||11", "text": "I hope the authors provide more explanation about this.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||0", "text": "The authors define (Definition 2) a generalized form of calibration error for any model with probabilistic output and any output space (binary classification, multiclass classification, real-valued regression, ordinal regression, structured prediction, etc.).", "label": null}
{"identifier": "-bxf89v3Nx|||0|||1", "text": "The main novelty of Definition 2 seems to be the introduction of a dummy variable $Z_X$ which is sampled from the predicted distribution $P_X$ over the target space, in order to compute an integral probability metric that compares the joint distribution of $(P_X, Y)$ to the joint distribution of $(P_X, Z_X)$.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||2", "text": "The authors note that in some cases, $Z_X$ can be integrated out of the definition either analytically or using numerical integration.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||3", "text": "It is not clear, from my limited background knowledge in the related work, why it should be helpful to introduce a dummy variable and then integrate it out.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||4", "text": "At first reading, Definition 2 seems too general to be useful in practice, as it requires the choice of a space of functions $\\mathcal{F}$.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||5", "text": "The authors argue (with details in the appendices, which are not provided to me at this time) that it generalizes several previous definitions of calibration error including the maximum mean discrepancy, the total variation distance, the Kantorovich distance, and the Dudley metric.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||6", "text": "Section 3 went beyond my area of expertise and beyond my comprehension; I feel unqualified to provide an informed review of this section.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||7", "text": "I think some reference to kernels or RKHSs should be made in the title or the abstract.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||8", "text": "The experiment in Section 5.2 demonstrates the utility of the proposed SKCE calibration metric, alongside more common metrics like negative log-likelihood (NLL) and mean squared error (MSE).", "label": null}
{"identifier": "-bxf89v3Nx|||0|||9", "text": "It appears that the SKCE curves (for both training and test data) have a very similar shape to the NLL curves, so it's not clear what benefit SKCE provides above and beyond the more common and easily-computed NLL.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||10", "text": "I would have liked to see more convincing experimental evidence of the marginal benefit of this approach beyond common calibration metrics.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||11", "text": "Regarding the significance of the work, I can add that in practice, I find that relatively few ML users are concerned with the calibration of their models, and these are entirely restricted to problems of classification (almost always binary classification) or quantile regression.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||12", "text": "The novelty of this work seems to lie mostly in its applicability beyond these types problems: the authors write, \"A key contribution of this paper is a new framework that is applicable to multivariate regression, as well as... discrete ordinal or more complex (e.g., graph-structured) [output],\" so I venture a guess that the intended audience for this work is relatively small.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||13", "text": "**Minor comments**", "label": null}
{"identifier": "-bxf89v3Nx|||0|||14", "text": "In equation (2), I believe the RHS should be $\\max P_X$ instead of $\\arg \\max P_X$.", "label": null}
{"identifier": "-bxf89v3Nx|||0|||15", "text": "Section 2, paragraph 2, you wrote \"instead of the discrepancy between the conditional distributions $\\mathbb{", "label": null}
{"identifier": "-bxf89v3Nx|||0|||16", "text": "P}(Y | X)$ and $P_X$.\"", "label": null}
{"identifier": "-bxf89v3Nx|||0|||17", "text": "Did you mean to write $\\mathbb{P}(Y | P_X)$ instead of $\\mathbb{", "label": null}
{"identifier": "-bxf89v3Nx|||0|||18", "text": "P}(Y | X)$?", "label": null}
{"identifier": "-bxf89v3Nx|||0|||19", "text": "That would make more sense to me, since the sentence would refer to comparing the LHS and RHS of Equation (1).", "label": null}
{"identifier": "-bxf89v3Nx|||1|||0", "text": "This paper addresses probabilistic data driven model calibration, i.e. aligning predicted target probabilities with actual ones.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||1", "text": "This problem has been extensively studied for classification tasks but solutions for regression tasks have limitations as illustrated by Fig.1.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||2", "text": "The authors intend to fill this gap and introduce a general kernel-based calibration framework that subsumes other ones previously defined for classification.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||3", "text": "The contribution of the authors is thus clearly stated and positioned w.r.t. prior arts.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||4", "text": "The authors start by proposing an alternative definition of calibration (Def.2) in order to cast the problem into integral probability metrics.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||5", "text": "It is this re-definition of the problem that allow them to encompass prior arts as special cases.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||6", "text": "For a number of practical and theoretical reasons, the authors focus on a special case of this framework which involves the computation of the MMD as a metric.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||7", "text": "Based on the MMD literature but also relying on the structure of their problem (where the auxiliary variable can be marginalized out), the authors provide several consistent estimator with known rates in dataset size.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||8", "text": "The validity of the proposed estimates is assessed through convincing numerical experiments that involve a calibration test.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||9", "text": "I honestly do not have much critic to address to this work which seems to have reached a level of maturity perfectly adapted for publication in ICLR.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||10", "text": "The only damper is that the proposed methodology allows to detect miscalibration not yet to cure it.", "label": null}
{"identifier": "-bxf89v3Nx|||1|||11", "text": "However, the authors seem to have some ideas on that too as mentioned in their conclusion.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||0", "text": "Summary:", "label": null}
{"identifier": "-bxf89v3Nx|||2|||1", "text": "The authors present an approach for testing calibration in conditional probability estimation models.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||2", "text": "They build on a line of work in the kernel estimation literature assessing whether the conditional distributions are well calibrated (i.e. P(Y | f(X)) = f(X), where f is some predictive model).", "label": null}
{"identifier": "-bxf89v3Nx|||2|||3", "text": "They develop an MMD kernel estimator and expand on practical choices of kernels that are computationally tractable.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||4", "text": "They then derive an asymptotic null distribution for calibrated models, enabling control over the error rate when labeling a model uncalibrated.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||5", "text": "A few simulation studies are done with neural networks to show the applicability of the method.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||6", "text": "Review:", "label": null}
{"identifier": "-bxf89v3Nx|||2|||7", "text": "This is an excellently written paper.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||8", "text": "The intro and first few chapters are a joy to read and really explain the problem well.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||9", "text": "There is a lot of nuance to calibration, so I really appreciated the precision and clarity in the exposition.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||10", "text": "The idea itself also seems quite elegant.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||11", "text": "Generalizing a previously published kernel approach from only discrete distributions to handle a more general class of problems may seem like a small conceptual step.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||12", "text": "However, I think the authors did a good job explaining the challenges of this extension.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||13", "text": "The resulting estimators are now applicable to many more problems than the existing work.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||14", "text": "Note I am not an expert in kernel learning, so I have not evaluated the proofs for correctness.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||15", "text": "My main issue comes with the lack of empirical studies.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||16", "text": "The toy problem is not terribly interesting and does not reveal any particular insight.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||17", "text": "It leads me to believe that maybe this is not that useful of a method, since the authors did not have anywhere that they could apply it to and derive meaningful insights or uses.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||18", "text": "The comment in the conclusion about the differentiability of their kernels is interesting and I think incorporating this into the training procedure could potentially show some very clear pragmatic use of this method.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||19", "text": "Overall, I like the paper.", "label": null}
{"identifier": "-bxf89v3Nx|||2|||20", "text": "It is clearly written and presents what I think is an interesting and novel idea.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||0", "text": "This paper consider the decentralized adaptive algorithms.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||1", "text": "At the first glance,  I am really happy to that the adaptive methods are used for the decentralized optimization.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||2", "text": "However,  after I read the  main document, I do not think  paper actually    analyzes the decentralized adaptive algorithms.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||3", "text": "In line 9 of Algorithm 1, the   denominator is $\\sqrt{\\hat{v}_{t,i}}$.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||4", "text": "However, in Algorithms 2 and 3, it is changed as $\\sqrt{u_{t,i}}$.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||5", "text": "In  the proofs, the authors proved the convergence based on $u_{t,i}\\geq \\epsilon$.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||6", "text": "This is actually the DSGD.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||7", "text": "The proofs can be quite simple.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||8", "text": "And the restriction $\\alpha=O(\\sqrt{\\epsilon})$ can be easily removed.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||9", "text": "This paper does not present any insights for the decentralized  adaptive methods.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||10", "text": "It only depends on $u_{t,i}\\geq \\epsilon$.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||11", "text": "The numerical results show that the proposed method is similar as DSGD.", "label": null}
{"identifier": "-csYGiUuGlt|||0|||12", "text": "As mentioned before, it is actually DSGD but with slightly modification.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||0", "text": "This paper discusses the problem of adaptive acceleration in the decentralized setting.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||1", "text": "The premise is that decentralized versions, while successful in the simple SGD setting, do not extend well in the acceleration settings, e.g. Adam and Adagrad.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||2", "text": "They first present a counterexample where a simple decentralized scheme, applying Adam, converges to a nonstationary point.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||3", "text": "(Suggestion: I would maybe add a cleaner, more flushed out version of this proof in the appendix, maybe with illustrations.)", "label": null}
{"identifier": "-csYGiUuGlt|||1|||4", "text": "Overall, everything the paper presented seemed reasonable.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||5", "text": "The motivation and counterexample in DADAM case are solid, and the following theorems seem to suggest gradient error norm $\\to 0$ at rate $O(1/\\sqrt{T})$ which is reasonable in nonconvex optimization.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||6", "text": "The intuition in the adjusted merging scheme is also reasonable, and makes sense that it would work better than vanilla merging schemes.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||7", "text": "However, I did not have a chance to carefully check the proofs, which are clearly the main contribution of the paper.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||8", "text": "One thing I would suggest is a more thorough set of numerical experiments.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||9", "text": "The two examples shown, in fact all the methods converge, and while the proposed method converges faster, it isn't really verifying the paper's main point, which is that the standard distributed methods diverge and the proposed method converges.", "label": null}
{"identifier": "-csYGiUuGlt|||1|||10", "text": "Showing this on a standard machine learning task would improve motivation.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||0", "text": "The paper introduces a decentralized framework for adaptive momentum-based gradient descent optimizers, such as ADAM.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||1", "text": "The proposed method is novel and is among the first works to consider a decentralized communication graph without a master node.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||2", "text": "The author discovers the divergent properties of the recent work of DADAM (Nazari, 2019) and proposes a way to fix it by adding a similar consensus step for the adaptive learning rates of agents.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||3", "text": "The mathematical derivation seems to be correct to the best of my knowledge.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||4", "text": "Finally, the author tests their method on a simple CNN and show their superiority compared to DADAM to achieve a close to the centralized performance.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||5", "text": "However, I have some minor comments to improve the manuscript.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||6", "text": "1. The experimental evaluation of the work is quite limited.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||7", "text": "I understand the space limit but it would have been nice to see more experiments instead of showcasing Algorithm 2 with an extra example in Algorithm 3.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||8", "text": "It is important to see the convergence behavior of the method (on the training data) with respect to the DGD on various datasets/networks in practice, rather than observing how the testing accuracy behaves.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||9", "text": "Note that your method does not guarantee any specific generalization behavior and therefore I believe it is more suited to report the experiments only in terms of training performance when you are out of space.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||10", "text": "2. What are the drawbacks of this method?", "label": null}
{"identifier": "-csYGiUuGlt|||2|||11", "text": "I can see more memory requirements for the agents due to the new variable \\tilde{u} for instance.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||12", "text": "Do you have any quantified evaluation in this respect?", "label": null}
{"identifier": "-csYGiUuGlt|||2|||13", "text": "I suspect it can be significant especially if the trained model is large and the agents have limited memory/computational resources", "label": null}
{"identifier": "-csYGiUuGlt|||2|||14", "text": "3. In Section 3.2, the author says \"Algorithm 2 can become different adaptive gradient methods by specifying r_t as different functions.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||15", "text": "E.g., when we choose ..., Algorithm 2 becomes a decentralized version of AdaGrad.\"", "label": null}
{"identifier": "-csYGiUuGlt|||2|||16", "text": "This sentence is not accurate as algorithms like AdaGrad and Adadelta do not use momentum on the past gradients.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||17", "text": "They only use the squared values of the past gradients.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||18", "text": "I believe your method, as I mentioned above, is a general framework for momentum-based techniques including ADAM, AdaMax, NADAM, etc, which brings me to the next question.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||19", "text": "4. Is it possible to generalized your method for an adaptive gradient descent algorithm that does not use the momentum of the gradients?", "label": null}
{"identifier": "-csYGiUuGlt|||2|||20", "text": "For example, take AdaGrad with a fixed learning rate of \\eta instead of m_t.", "label": null}
{"identifier": "-csYGiUuGlt|||2|||21", "text": "How does your convergence behavior change?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||0", "text": "In this paper, the authors attempt to use adaptive gradient methods in decentralized training paradigm.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||1", "text": "They develop a general framework to convert an adaptive gradient method from a centralized one to its decentralized variant.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||2", "text": "Specifically, they propose a decentralized AMSGrad algorithm.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||3", "text": "They also point out a potential divergent problem of an existing method and investigate the conditions to ensure convergence.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||4", "text": "Finally, they conduct some experiments to verify the performance of their algorithm.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||5", "text": "Pros:\n1. This paper is the first one to use adaptive gradient method to decentralized training paradigm and the authors also provide a completed convergence analysis for their algorithm.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||6", "text": "2. This paper reveals a divergent problem of DADAM in offline situation by taking some intuitive examples.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||7", "text": "Based on this issue, they find a solution that makes consensus on the adaptive learning rates.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||8", "text": "3. This paper proposes a general framework to apply adaptive gradient methods to decentralized optimization.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||9", "text": "The authors also investigate the conditions to make sure these decentralized variants will converge.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||10", "text": "Cons:\n1. In section 3.2, the paper claims AdaGrad and AMSGrad satisfy the condition to guarantee the convergence of Algorithm 2 while Adam does not.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||11", "text": "It seems not to be an obvious conclusion from the reference Chen et al. (2019).", "label": null}
{"identifier": "-csYGiUuGlt|||3|||12", "text": "Is there more explanation or proof about why AdaGrad and AMSGrad satisfy the condition?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||13", "text": "And does it mean Adam still diverges even after using the algorithmic approach proposed in this paper?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||14", "text": "2. In Theorem 2, the convergence analysis result of Algorithm 2 is given.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||15", "text": "However, the convergence of common adaptive gradient methods such as AdaGrad and Adam is still not clear.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||16", "text": "Therefore, the \u201cconvergent adaptive gradient method\u201d in the title is very misleading.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||17", "text": "Do most of the adaptive gradient methods have the same theoretical guarantees as AMSGrad?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||18", "text": "3. \\mathbb{E}[\\sum_{t=1}^T", "label": null}
{"identifier": "-csYGiUuGlt|||3|||19", "text": "\\lVert (-\\hat{V}_{t-2} + \\hat{V}_{t-1}) \\rVert_{abs}] = o(T) is the key condition to ensure the convergence.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||20", "text": "But when the above equation is O(\\sqrt{T}), the convergence rate is worse than the centralized counterpart.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||21", "text": "Is that case possible?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||22", "text": "4. In section 3.4, the experiment is divided into homogeneous and heterogeneous data, which is very confusing.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||23", "text": "What is the reason for doing this and what will happen if we just deal with the dataset normally?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||24", "text": "The heterogeneous data is treated very intentionally.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||25", "text": "Is there any discussion about when the treatment of heterogeneous data is important?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||26", "text": "5. In the homogeneous data experiment, the performance of DADAM and decentralized AMSGrad are similar.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||27", "text": "What is the reason that the learning rates on different node tend to be similar?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||28", "text": "Is that a common case?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||29", "text": "Maybe the experiment on more dataset is needed to address this concern.", "label": null}
{"identifier": "-csYGiUuGlt|||3|||30", "text": "Besides, how will such similarity among data impact the theoretical convergence?", "label": null}
{"identifier": "-csYGiUuGlt|||3|||31", "text": "Minors:", "label": null}
{"identifier": "-csYGiUuGlt|||3|||32", "text": "The algorithm proposed in Lian et al. (2017) is called Decentralized Parallel Stochastic Gradient Descent (D-PSGD).", "label": null}
{"identifier": "-csYGiUuGlt|||4|||0", "text": "This paper studied the decentralized adaptive gradient methods and provided convergence guarantees.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||1", "text": "Experiment on MNIST is conducted to show the effectiveness of the proposed approach.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||2", "text": "1. The theoretical result is weak.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||3", "text": "The linear speedup result is not proved as in (Lian et al. 2017), the benefits of adaptive gradient methods are also not illustrated in the bound in Theorem 2 and Theorem 3.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||4", "text": "2. The learning rate scheme is not practical and does not hold in practice.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||5", "text": "As illustrated in Theorem 2 and 3, the learning rate $\\alpha$ is set to be less than $\\epsilon^{0.5}/16L$.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||6", "text": "3. The LHS of Theorem 2 and Theorem 3 are not the standard gradient squared norm but the scaled version.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||7", "text": "It is unclear what is the bound if the LHS is the standard gradient squared norm as in (Lian et al. 2017).", "label": null}
{"identifier": "-csYGiUuGlt|||4|||8", "text": "It is important to use the same measure as in the previous literature for fair comparison.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||9", "text": "3. The experiment is weak.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||10", "text": "Doing distributed training only on a tiny dataset on MNIST is not sufficient.", "label": null}
{"identifier": "-csYGiUuGlt|||4|||11", "text": "I would like to see results on larger datasets such as CIFAR and ImageNet.", "label": null}
